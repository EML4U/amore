{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 contain only a few reviews and are not included in Doc2Vec embeddings.  \n",
    "  (The years 2000 to 2012 are included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* **opinion_lexicon**: Instance of OpinionLexicon to access negative and positive words. Not used for now.\n",
    "* **year_star_ids**: Collection of IDs sorted by years and stars. Used afterwards to filter by stars and years in method **get_review_ids**.\n",
    "* **reader**: Instance of AmazonReviewsReader to access review data. Used afterwards to create revno_to_text.\n",
    "* **revno_to_text**: Dictionary review-number to full text. Used afterwards to access review-texts in method **get_text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 291.32208580401493\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files\n",
    "\n",
    "Data required to create **Matrix** instances afterwards.\n",
    "\n",
    "* **doc_term_matrix**: Matrix containing document-IDs and term-IDs. Used afterwards for building Matrix.\n",
    "* **vocabulary**: Dictionary containing tuples of terms and term-IDs. Used afterwards to create inv_vocabulary.\n",
    "* **inv_vocabulary**: Dictionary containing tuples of term-IDs and terms.Used afterwards to create tokens.\n",
    "* **tokens**: Ordered list of terms, index represents term-ID. Used afterwards as token-indices for building Matrix.\n",
    "* **vecid_revno**: Values used afterwards as review-indices for building Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 37.48579945595702\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access methods and Matrix class\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used afterwards in Matrix class.\n",
    "\n",
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None # list of tokens\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1  \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences\n",
    "    \n",
    "    def count_token_occurences(self, review_ids, tokens):\n",
    "        \"\"\"\n",
    "        Sums up the occurencies of given tokens in given reviews.\n",
    "        \"\"\"\n",
    "        review_to_tokenoccurences = {}\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "        for review_id in review_ids:\n",
    "            if not review_id in review_ids:\n",
    "                continue\n",
    "            review_to_tokenoccurences[review_id] = 0\n",
    "            for counts in self.get_token_data(review_id=review_id):\n",
    "                review_to_tokenoccurences[review_id] += 1\n",
    "        return review_to_tokenoccurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "#### Get tokens which occur predominantly in only one dataset: Get ratio for each token.\n",
    "\n",
    "* **get_ratio**\n",
    "  * In: matrix-A, matrix-B\n",
    "  * In: min_token_occurences (token has to occure at least x times in each of both matrixes)\n",
    "  * Out: tokens sorted by ratio\n",
    "  * Out: tokens sorted by inverted ratio\n",
    "  * Out: tokens not reached minimum\n",
    "  * Out: tokens only in A\n",
    "  * Out: tokens only in B\n",
    "* **filter_tokens_by_ratio**\n",
    "  * In: ratio (generated by method get_ratio)\n",
    "  * In: min_ratio (minimum threshold)\n",
    "  * In: max_tokens (maximum number of tokens to return)\n",
    "  * In: exclude_tokens (tokens not to return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    # Pairs of tokens and occurences\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter/remove rare:\n",
    "        # minimum is set AND\n",
    "        # token has to be in both matrixes AND\n",
    "        # threshold reached in A AND\n",
    "        # threshod reached in B AND\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare[tok_occ[0]] = str(tok_occ[1]) + \" / \" + str(b_token_occurences[tok_occ[0]])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data / create base matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Main matrix containing all data\n",
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews to use\n",
    "ids_neg_a = get_review_ids(years=[2000, 2004], stars=[1,2])\n",
    "ids_neg_b = get_review_ids(years=[2005], stars=[1,2])\n",
    "ids_neg = ids_neg_a + ids_neg_b\n",
    "\n",
    "ids_pos_a = []\n",
    "ids_pos_b = get_review_ids(years=[2005], stars=[4,5])\n",
    "ids_pos = ids_pos_a + ids_pos_b\n",
    "\n",
    "ids_all = ids_neg + ids_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 33663 review IDs\n",
      "Created: Matrix (33663, 607181), 33663 reviews, 607181 tokens\n",
      "\n",
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 607181), 90403 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Sub-matrixes\n",
    "m_neg = matrix.filter_keep_reviews(ids_neg)\n",
    "print()\n",
    "m_pos = matrix.filter_keep_reviews(ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens to use\n",
    "\n",
    "Note: During exploration of the data it turned out that there are more positive words that have a large ratio and therefore can be used to determine (unique) drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 153373 / 85218\n",
      "Output sizes: ratio 40 ; rare 61789 ; only a 91544 ; only b 23389\n",
      "\n",
      "ratio_a [('excellent', 5.64), ('best', 3.2), ('great', 3.17), ('love', 2.9), ('set', 2.44), ('years', 2.43), ('world', 2.36), ('dvd', 2.32), ('series', 2.32), ('music', 2.29), ('life', 2.27), ('new', 2.16), ('man', 1.84), ('makes', 1.82), ('old', 1.8), ('work', 1.71), ('little', 1.64), ('long', 1.64), ('story', 1.6), ('watch', 1.59), ('real', 1.55), ('time', 1.54), ('good', 1.51), ('way', 1.51), ('seen', 1.5), ('films', 1.46), ('think', 1.4), ('watching', 1.39), ('people', 1.38), ('characters', 1.37), ('know', 1.35), ('like', 1.34), ('end', 1.33), ('character', 1.3), ('scenes', 1.27), ('film', 1.26), ('movies', 1.22), ('want', 1.21), ('better', 1.13), ('movie', 1.07)]\n",
      "\n",
      "ratio_b [('movie', 0.93), ('better', 0.88), ('want', 0.83), ('movies', 0.82), ('film', 0.79), ('scenes', 0.79), ('character', 0.77), ('like', 0.75), ('end', 0.75), ('know', 0.74), ('characters', 0.73), ('people', 0.72), ('watching', 0.72), ('think', 0.71), ('films', 0.68), ('seen', 0.67), ('good', 0.66), ('way', 0.66), ('time', 0.65), ('real', 0.65), ('watch', 0.63), ('story', 0.62), ('little', 0.61), ('long', 0.61), ('work', 0.58), ('old', 0.56), ('makes', 0.55), ('man', 0.54), ('new', 0.46), ('life', 0.44), ('music', 0.44), ('dvd', 0.43), ('series', 0.43), ('world', 0.42), ('years', 0.41), ('set', 0.41), ('love', 0.34), ('great', 0.32), ('best', 0.31), ('excellent', 0.18)]\n",
      "\n",
      "rare [('worth', '9600 / 2749'), ('look', '9515 / 3673'), ('shows', '9403 / 1668'), ('fan', '9377 / 2901'), ('times', '9358 / 2688'), ('come', '9310 / 3136'), ('going', '9273 / 4295'), ('fun', '9186 / 1729'), ('lot', '9170 / 3070'), ('right', '8943 / 3158')]\n",
      "\n",
      "only_a [('emmys', 112), ('dreamer', 102), ('transfere', 100), ('hatton', 82), ('greys', 80), ('rusesabagina', 75), ('keye', 73), ('hutu', 71), ('tutsi', 70), ('kolk', 68)]\n",
      "\n",
      "only_b [('dumberer', 15), ('snoozefest', 14), ('yawned', 12), ('djmon', 12), ('compost', 11), ('uncompelling', 10), ('cra', 9), ('yawnfest', 9), ('mi2', 8), ('stinko', 8)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_pos,\n",
    "    m_neg,\n",
    "    min_token_occurences=10*1000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'best', 'great', 'love']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 4,\n",
    "['highly', 'season', 'collection', 'episodes', 'features', 'classic', 'episode', 'definitely', 'shows', 'family', 'performances', 'gives', 'young', 'series',\n",
    " 'music', 'performance', 'live', 'years', 'job', 'john', 'set', 'true', 'finally', 'truly', 'world', 'different', 'dvd', 'especially', 'history'\n",
    "])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.25, 10, [])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 4), 1584098 reviews, 4 tokens\n",
      "Filtering. Based on 16197 review IDs\n",
      "Created: Matrix (16197, 4), 16197 reviews, 4 tokens\n",
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 4), 17466 reviews, 4 tokens\n",
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 4), 90403 reviews, 4 tokens\n"
     ]
    }
   ],
   "source": [
    "# Remove everything but filtered tokens\n",
    "matrix_filtered       = matrix.filter_tokens(filter_tokens_a)\n",
    "matrix_filtered_neg_a = matrix_filtered.filter_keep_reviews(ids_neg_a)\n",
    "matrix_filtered_neg_b = matrix_filtered.filter_keep_reviews(ids_neg_b)\n",
    "matrix_filtered_pos_b = matrix_filtered.filter_keep_reviews(ids_pos_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10238\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_a = {}\n",
    "for review_item in matrix_filtered_neg_a.count_token_occurences(ids_neg_a, filter_tokens_a).items():\n",
    "    if review_item[1] == 0:\n",
    "        reviews_neg_a[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11231\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_b = {}\n",
    "for review_item in matrix_filtered_neg_b.count_token_occurences(ids_neg_b, filter_tokens_a).items():\n",
    "    if review_item[1] == 0:\n",
    "        reviews_neg_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61149\n"
     ]
    }
   ],
   "source": [
    "reviews_pos_b = {}\n",
    "for review_item in matrix_filtered_pos_b.count_token_occurences(ids_pos_b, filter_tokens_a).items():\n",
    "    if review_item[1] > 0:\n",
    "        reviews_pos_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_pos_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "reviews_pos_b_valuesorted = {k: v for k, v in sorted(reviews_pos_b.items(), key=lambda item: item[1], reverse=True)}\n",
    "end_id = 1005\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 9000 1000\n"
     ]
    }
   ],
   "source": [
    "review_ids_neg_a = list(reviews_neg_a.keys())[:10000]\n",
    "review_ids_neg_b = list(reviews_neg_b.keys())[:9000]\n",
    "review_ids_pos_b = list(reviews_pos_b_valuesorted.keys())[:1000]\n",
    "review_ids_neg_a.sort()\n",
    "review_ids_neg_b.sort()\n",
    "review_ids_pos_b.sort()\n",
    "print(len(review_ids_neg_a), len(review_ids_neg_b), len(review_ids_pos_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-a-neg.txt\n",
      "Writing 9000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-b-neg.txt\n",
      "Writing 1000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-a-pos.txt\n",
      "Writing 4 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-tokens.txt\n"
     ]
    }
   ],
   "source": [
    "def write_ids(file_path, ids_list):\n",
    "    print('Writing ' + str(len(ids_list)) + ' items to ' + file_path)\n",
    "    with open(file_path, 'w') as fp:\n",
    "        fp.write(\"\\n\".join(str(item) for item in ids_list))\n",
    "\n",
    "if True:\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-1-a-neg.txt', review_ids_neg_a)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-1-b-neg.txt', review_ids_neg_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-1-a-pos.txt', review_ids_pos_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-1-tokens.txt', filter_tokens_a)\n",
    "\n",
    "# Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-a-neg.txt\n",
    "# Writing 9000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-b-neg.txt\n",
    "# Writing 1000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-a-pos.txt\n",
    "# Writing 4 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-1-tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1281331, 1941102, 2107325, 2224334, 2458186, 2765240, 3099573, 3128914, 3617118]\n",
      "b [(1281331, 4), (1941102, 4), (2107325, 4), (2224334, 4), (2458186, 4), (2765240, 4), (3099573, 4), (3128914, 4), (3617118, 4)]\n",
      "Great! Gilad is still awsome! I used to do Gilad videos in the early 90s when I was in high school and I always liked him. I thought his videos were challenging. I decided to purchase this dvd to see what the deal was with Gilad today, but this DVD is from like 1993. So it's pretty dated as far as their clothes go, but I still love it! If you like Gilad, you'll like this. You either love him or hate him. I love him! I think he's GREAT!  Its a very challenging, fun video and it gives you the option to intensify the movements to make it even more challenging. The setting is at a beach resort in Hawaii and although some people might find this a little hokey, I was totally into it. I could almost imagine myself at this beach (yes wishfull thinking..) And even the best excercise dvds are pretty corny, this one is really good. I only gave it 4 stars because I would have like a little more difficult combinations, but they were still pretty challenging none the less. If you need to lose weight, this is the way! The ab workout is great too! Gilad defiantely stands the test of time! Definately purchase this workout if you want to burn calories (which burns fat!!) and also step aerobics certainly tones your legs too. If you don't have a gym membership and work out at home like I do, you can be pretty limited on what your equipment is. But step aerobics really tones up your calves and thighs. This is an excellent DVD and I definately recommend it for all fitness levels. If you are a beginner, don't do the arms and don't jump the movements and walk in place if you need to catch your breathe. If you are advanced then you can definately power it up and get a great workout!\n"
     ]
    }
   ],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if True:\n",
    "    print('a', list(reviews_pos_b_valuesorted.keys())[0:9])\n",
    "    print('b', list(review_ids_pos_b)[0:9])\n",
    "    print(get_text(1281331))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print('1', m.get_token_data(revs[1]))\n",
    "    print('6', m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print('0', m.get_token_data(revs[0]))\n",
    "    print('4', m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "print_matrix_info = None\n",
    "get_test_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
