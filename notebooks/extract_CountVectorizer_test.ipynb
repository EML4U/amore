{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results: extract_CountVectorizer notebook\n",
    "\n",
    "[docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: dict_keys([2007, 2006, 2008, 2003, 2002, 2004, 2000, 2009, 2011, 2010, 2001, 2005, 2012, 1999, 1998, 1997])\n",
      "Example stars:   dict_keys([3, 5, 4, 1, 2])\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', year_star_ids.keys())\n",
    "print('Example stars:  ', year_star_ids[2007].keys())\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: dict_keys([2007, 2006, 2008, 2003, 2002, 2004, 2000, 2009, 2011, 2010, 2001, 2005, 2012, 1999, 1998, 1997])\n",
    "# Example stars:   dict_keys([3, 5, 4, 1, 2])\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 4662381\n",
      "Runtime: 281.05807309411466\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "max_docs = -1\n",
    "min_year = 2007\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 26.182680692523718\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 486546 <class 'dict'>\n",
      "example: ('movie', 299799)\n",
      "inv_vocabulary: 486546 <class 'dict'>\n",
      "example: (299799, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1203682 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    ids += year_star_ids[year][star]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id: dict\n",
    "    docindex_to_reviewid = None\n",
    "    reviewid_to_docindex = None\n",
    "    \n",
    "    # matrix-term-index to token: dict\n",
    "    tokenindex_to_token = None\n",
    "    token_to_tokenindex = None\n",
    "        \n",
    "    def __init__(self, doc_term_matrix, docindex_to_reviewid, tokenindex_to_token):\n",
    "        print('Document-term matrix:       ', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "        print('Document-index to review-id:', len(docindex_to_reviewid), type(docindex_to_reviewid))\n",
    "        print('Term-index to token:        ', len(tokenindex_to_token), type(tokenindex_to_token))\n",
    "        self.doc_term_matrix      = doc_term_matrix\n",
    "        self.docindex_to_reviewid = docindex_to_reviewid\n",
    "        self.tokenindex_to_token  = tokenindex_to_token\n",
    "    \n",
    "    def get_reviewid_to_docindex(self, review_id):\n",
    "        if not self.reviewid_to_docindex:\n",
    "            self.reviewid_to_docindex = {v: k for k, v in self.docindex_to_reviewid.items()}\n",
    "        return self.reviewid_to_docindex[review_id]\n",
    "    \n",
    "    def get_token_to_tokenindex(self, token):\n",
    "        if not self.token_to_tokenindex:\n",
    "            self.token_to_tokenindex = {v: k for k, v in self.tokenindex_to_token.items()}\n",
    "        if token in self.token_to_tokenindex:\n",
    "            return self.token_to_tokenindex[token]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_token_indices(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].indices\n",
    "        \n",
    "    def get_token_data(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].data\n",
    "    \n",
    "    def get_token_counts(self, review_id):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count).\n",
    "        \"\"\"\n",
    "        token_counts = {}\n",
    "        doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        token_data = self.get_token_data(doc_index=doc_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(doc_index=doc_index)):\n",
    "            token = self.get_token(token_index)\n",
    "            if token:\n",
    "                token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def get_token(self, token_index):\n",
    "        if token_index in self.tokenindex_to_token:\n",
    "            return self.tokenindex_to_token[token_index]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def filter_min_count(self, min_count):  \n",
    "        \"\"\"\n",
    "        Generates smaller matrix based on token counts (overall word usage)\n",
    "        \"\"\"\n",
    "        # Sum up token occurences in docs\n",
    "        token_sums = self.doc_term_matrix.sum(0)\n",
    "        print('Filtering. Based on', token_sums.shape[1], 'summed up tokens')\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        token_indices_extract = []\n",
    "        new_inv_vocabulary = {}\n",
    "        new_token_index = 0\n",
    "        for token_index in range(0, token_sums.shape[1]):\n",
    "            if token_sums.item(token_index) >= min_count:\n",
    "                token_indices_extract.append(token_index)\n",
    "                new_inv_vocabulary[new_token_index] = self.get_token(token_index)\n",
    "                new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,token_indices_extract]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)\n",
    "        \n",
    "    def filter_tokens(self, tokens):\n",
    "        \n",
    "        # Collect available token-indices\n",
    "        tokenindex_to_token = {}\n",
    "        for token in tokens:\n",
    "            token_index = self.get_token_to_tokenindex(token)\n",
    "            if token_index:\n",
    "                tokenindex_to_token[token_index] = token\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        new_inv_vocabulary = {}\n",
    "        for new_token_index, token_index in enumerate(tokenindex_to_token.keys()):\n",
    "            new_inv_vocabulary[new_token_index] = tokenindex_to_token[token_index]\n",
    "            new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,list(tokenindex_to_token.keys())]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an example review to test code.\n",
    "\n",
    "(e.g. review ID 6590 contains pos and neg words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of review IDs: 108952\n",
      "Review ID: 6590\n",
      "Review text: A Priceless Treasure Bobby Short was a favorite performer of mine since I moved to New York in 1955. A friend had his LP with \"At the Moving Picture Ball\" on it and I insisted she play it whenever I dropped by for coffee, which I did every morning when I was between jobs. I always intended to splurge and go see him at the Carlyle but I was too poor and then too populist for a night club and then too stingy. And there were always the great, great records. And suddenly, after only half a century, he was gone. Thank god for this DVD of a wonderful performance at the club. The ebullience, the superb artistry and the glow of his personal niceness make it a marvelous experience to treasure over and over. And he does \"Moving Picture Ball\" and other personal favorites, \"On the Amazon\" and \"Why Shouldn't I?\"  Bobby fans won't need prompting but this great treat should also be a key discovery for anyone interested in popular song styling of the civilized pre-wail-and-whine era, in Manhattan high life, in the triumph of individual striving and dedication, in... Oh hell, in anything that's fun. -- Paul Rawlings, Bayport, N.Y..\n"
     ]
    }
   ],
   "source": [
    "review_ids = get_review_ids([2007], [5])\n",
    "print(\"Number of review IDs:\", len(review_ids))\n",
    "KEY_ID = 0\n",
    "review_id = review_ids[11][KEY_ID]\n",
    "print(\"Review ID:\", review_id)\n",
    "print(\"Review text:\", get_text(review_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit all tokens (486,546) to those appearing e.g. at least 1,000 times (8,444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data\n",
      "Document-term matrix:        (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1203682 <class 'dict'>\n",
      "Term-index to token:         486546 <class 'dict'>\n",
      "{'great': 3, 'bobby': 2, 'moving': 2, 'picture': 2, 'treasure': 2, 'personal': 2, 'club': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'wonderful': 1, 'song': 1, 'performance': 1, 'dvd': 1, 'night': 1, 'amazon': 1, 'morning': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'hell': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'play': 1, 'god': 1, 'gone': 1, 'need': 1, 'interested': 1, 'experience': 1, 'superb': 1, 'york': 1, 'era': 1, 'marvelous': 1, 'fun': 1, 'thank': 1, 'century': 1, 'short': 1, 'poor': 1, 'half': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'popular': 1, 'insisted': 1, 'pre': 1, 'discovery': 1, 'intended': 1, 'priceless': 1, 'paul': 1, 'civilized': 1, 'key': 1, 'dedication': 1, 'jobs': 1, 'dropped': 1, 'records': 1, 'striving': 1, 'triumph': 1, 'prompting': 1, 'performer': 1, 'manhattan': 1, 'whine': 1, '1955': 1, 'stingy': 1, 'splurge': 1, 'carlyle': 1, 'populist': 1, 'ebullience': 1, 'glow': 1, 'niceness': 1, 'styling': 1, 'wail': 1, 'rawlings': 1, 'bayport': 1}\n",
      "\n",
      "Min word occurences: 1000 (from all)\n",
      "Filtering. Based on 486546 summed up tokens\n",
      "Document-term matrix:        (1203682, 8444) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1203682 <class 'dict'>\n",
      "Term-index to token:         8444 <class 'dict'>\n",
      "{'great': 3, 'bobby': 2, 'moving': 2, 'picture': 2, 'treasure': 2, 'personal': 2, 'club': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'wonderful': 1, 'song': 1, 'performance': 1, 'dvd': 1, 'night': 1, 'amazon': 1, 'morning': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'hell': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'play': 1, 'god': 1, 'gone': 1, 'need': 1, 'interested': 1, 'experience': 1, 'superb': 1, 'york': 1, 'era': 1, 'marvelous': 1, 'fun': 1, 'thank': 1, 'century': 1, 'short': 1, 'poor': 1, 'half': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'popular': 1, 'pre': 1, 'discovery': 1, 'intended': 1, 'priceless': 1, 'paul': 1, 'key': 1, 'dedication': 1, 'jobs': 1, 'dropped': 1, 'records': 1, 'triumph': 1, 'performer': 1, 'manhattan': 1, '1955': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('All data')\n",
    "m = Matrix(doc_term_matrix, vecid_revno, inv_vocabulary)\n",
    "print(m.get_token_counts(review_id))\n",
    "print()\n",
    "\n",
    "if False:\n",
    "    print('Min word occurences: 100 (from all)')\n",
    "    m100 = m.filter_min_count(100)\n",
    "    print(m100.get_token_counts(review_id))\n",
    "    print()\n",
    "\n",
    "print('Min word occurences: 1000 (from all)')\n",
    "m1k = m.filter_min_count(1000)\n",
    "print(m1k.get_token_counts(review_id))\n",
    "print()\n",
    "\n",
    "if False:\n",
    "    print('Min word occurences: 1000 (from 100)')\n",
    "    m1000b = m100.filter_min_count(1000)\n",
    "    print(m1000b.get_token_counts(review_id))\n",
    "    print()\n",
    "\n",
    "    print('Min word occurences: 100k (from all)')\n",
    "    m100k = m.filter_min_count(100000)\n",
    "    print(m100k.get_token_counts(review_id))\n",
    "    print()\n",
    "\n",
    "    print('Min word occurences: 100k (from 1000, from 100)')\n",
    "    m100kb = m1000.filter_min_count(100000)\n",
    "    print(m100kb.get_token_counts(review_id))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit matrix tokens to pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1203682, 873) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1203682 <class 'dict'>\n",
      "Term-index to token:         873 <class 'dict'>\n",
      "some negetive words: ['sue', 'helpless', 'trapped', 'distract', 'stupidity', 'slave', 'drags', 'unbearable', 'smash', 'murderous', 'ignore', 'distress', 'warned', 'creeps', 'injury', 'melodramatic', 'obscure', 'suspicious', 'blatant', 'hostage']\n",
      "{'hell': 1, 'poor': 1}\n"
     ]
    }
   ],
   "source": [
    "m_neg = m1k.filter_tokens(opinion_lexicon.get_negative_set())\n",
    "#print('some negetive words:', list(m_neg.tokenindex_to_token.values())[:20])\n",
    "\n",
    "print(m_neg.get_token_counts(review_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1203682, 687) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1203682 <class 'dict'>\n",
      "Term-index to token:         687 <class 'dict'>\n",
      "{'great': 3, 'treasure': 2, 'wonderful': 1, 'favorite': 1, 'fans': 1, 'superb': 1, 'marvelous': 1, 'fun': 1, 'thank': 1, 'popular': 1, 'priceless': 1, 'triumph': 1}\n"
     ]
    }
   ],
   "source": [
    "m_pos = m1k.filter_tokens(opinion_lexicon.get_positive_set())\n",
    "#print('some positive words:', list(m_pos.tokenindex_to_token.values())[:20])\n",
    "\n",
    "print(m_pos.get_token_counts(review_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
