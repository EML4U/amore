{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 are not included in Doc2Vec embeddings.\n",
    "- Ideas:\n",
    "    - 100/0 to 0/100 neg/pos\n",
    "    - 50/50 to 40/60 neg/pos\n",
    "    - build on results on that: other distributions, e.g. 45/55\n",
    "    - 50/50 to 40/30/30 neg/posCluster1/posCluster2\n",
    "    - for token-level and document-level evaluation\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: dict_keys([2007, 2006, 2008, 2003, 2002, 2004, 2000, 2009, 2011, 2010, 2001, 2005, 2012, 1999, 1998, 1997])\n",
      "Example stars:   dict_keys([3, 5, 4, 1, 2])\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', year_star_ids.keys())\n",
    "print('Example stars:  ', year_star_ids[2007].keys())\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: dict_keys([2007, 2006, 2008, 2003, 2002, 2004, 2000, 2009, 2011, 2010, 2001, 2005, 2012, 1999, 1998, 1997])\n",
    "# Example stars:   dict_keys([3, 5, 4, 1, 2])\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 312.4320105519146\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 54.29142002761364\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    ids += year_star_ids[year][star]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    # Note (TODO): Maybe a simplified version could become to replace the indices dicts by lists,\n",
    "    # as indices probably simply increment starting at 0.\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id: dict\n",
    "    docindex_to_reviewid = None\n",
    "    reviewid_to_docindex = None\n",
    "    \n",
    "    # matrix-term-index to token: dict\n",
    "    tokenindex_to_token = None\n",
    "    token_to_tokenindex = None\n",
    "        \n",
    "    def __init__(self, doc_term_matrix, docindex_to_reviewid, tokenindex_to_token):\n",
    "        print('Document-term matrix:       ', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "        print('Document-index to review-id:', len(docindex_to_reviewid), type(docindex_to_reviewid))\n",
    "        print('Term-index to token:        ', len(tokenindex_to_token), type(tokenindex_to_token))\n",
    "        self.doc_term_matrix      = doc_term_matrix\n",
    "        self.docindex_to_reviewid = docindex_to_reviewid\n",
    "        self.tokenindex_to_token  = tokenindex_to_token\n",
    "    \n",
    "    def get_reviewid_to_docindex(self, review_id):\n",
    "        if not self.reviewid_to_docindex:\n",
    "            self.reviewid_to_docindex = {v: k for k, v in self.docindex_to_reviewid.items()}\n",
    "        if review_id in self.reviewid_to_docindex.keys():\n",
    "            return self.reviewid_to_docindex[review_id]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_token_to_tokenindex(self, token):\n",
    "        if not self.token_to_tokenindex:\n",
    "            self.token_to_tokenindex = {v: k for k, v in self.tokenindex_to_token.items()}\n",
    "        if token in self.token_to_tokenindex:\n",
    "            return self.token_to_tokenindex[token]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_token_indices(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].indices\n",
    "        \n",
    "    def get_token_data(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].data\n",
    "    \n",
    "    def get_token_counts(self, review_id):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count).\n",
    "        \"\"\"\n",
    "        token_counts = {}\n",
    "        doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        token_data = self.get_token_data(doc_index=doc_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(doc_index=doc_index)):\n",
    "            token = self.get_token(token_index)\n",
    "            if token:\n",
    "                token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def get_token(self, token_index):\n",
    "        if token_index in self.tokenindex_to_token:\n",
    "            return self.tokenindex_to_token[token_index]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_overall_token_counts(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their sums of occurences in all documents.\n",
    "        \"\"\"\n",
    "        token_sums = self.doc_term_matrix.sum(0)\n",
    "        token_counts = {}\n",
    "        for token_index in range(0, token_sums.shape[1]):\n",
    "            token_counts[self.get_token(token_index)] = token_sums.item(token_index)\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could be improved by using matrix instead of dict.\n",
    "        \n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.get_token(item[0])] = item[1]\n",
    "        return token_occurences\n",
    "        \n",
    "    def filter_min_count(self, min_count):  \n",
    "        \"\"\"\n",
    "        Generates smaller matrix based on token minimum counts (overall word usage)\n",
    "        \"\"\"\n",
    "        # Sum up token occurences in docs\n",
    "        token_sums = self.doc_term_matrix.sum(0)\n",
    "        print('Filtering. Based on', token_sums.shape[1], 'summed up tokens')\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        token_indices_extract = []\n",
    "        new_inv_vocabulary = {}\n",
    "        new_token_index = 0\n",
    "        for token_index in range(0, token_sums.shape[1]):\n",
    "            if token_sums.item(token_index) >= min_count:\n",
    "                token_indices_extract.append(token_index)\n",
    "                new_inv_vocabulary[new_token_index] = self.get_token(token_index)\n",
    "                new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,token_indices_extract]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)\n",
    "    \n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Collect available token-indices\n",
    "        tokenindex_to_token = {}\n",
    "        for token in tokens:\n",
    "            token_index = self.get_token_to_tokenindex(token)\n",
    "            if token_index:\n",
    "                tokenindex_to_token[token_index] = token\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        new_inv_vocabulary = {}\n",
    "        for new_token_index, token_index in enumerate(tokenindex_to_token.keys()):\n",
    "            new_inv_vocabulary[new_token_index] = tokenindex_to_token[token_index]\n",
    "            new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,list(tokenindex_to_token.keys())]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)\n",
    "    \n",
    "    def filter_reviews(self, years=None, stars=None):\n",
    "        \"\"\"\n",
    "        Filters matrix by years and stars of reviews.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get review-ids\n",
    "        review_ids = []\n",
    "        for review_tup in get_review_ids(years, stars):\n",
    "            review_ids.append(review_tup[0])\n",
    "        review_ids = sorted(review_ids)\n",
    "        \n",
    "        # Collect matrix-doc-indices from review-ids\n",
    "        doc_indices_extract = []\n",
    "        new_docindex_to_reviewid = {}\n",
    "        new_docindex = 0\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.get_reviewid_to_docindex(review_id))\n",
    "            new_docindex_to_reviewid[new_docindex] = review_id\n",
    "            new_docindex += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = m.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_docindex_to_reviewid, self.tokenindex_to_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit all tokens (486,546) to those appearing e.g. at least 1,000 times (8,444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 607181 summed up tokens\n",
      "Document-term matrix:        (1584098, 299517) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         299517 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 607181 summed up tokens\n",
      "Document-term matrix:        (1584098, 10842) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         10842 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m = Matrix(doc_term_matrix, vecid_revno, inv_vocabulary)\n",
    "print()\n",
    "m2 = m.filter_min_count(2)\n",
    "print()\n",
    "m1k = m.filter_min_count(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit matrix tokens to pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1584098, 1131) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         1131 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m_neg = m1k.filter_tokens(opinion_lexicon.get_negative_set())\n",
    "#print('some negetive words:', list(m_neg.tokenindex_to_token.values())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1584098, 841) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         841 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m_pos = m1k.filter_tokens(opinion_lexicon.get_positive_set())\n",
    "#print('some positive words:', list(m_pos.tokenindex_to_token.values())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit matrix to subset of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 146616 review IDs\n",
      "Document-term matrix:        (146616, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 146616 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m_2007_pos = m.filter_reviews(years=[2007], stars=[4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example review\n",
    "\n",
    "(e.g. review ID 6590 contains pos and neg words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of review IDs: 108952\n",
      "Review ID: 6590\n"
     ]
    }
   ],
   "source": [
    "review_ids = get_review_ids([2007], [5])\n",
    "print(\"Number of review IDs:\", len(review_ids))\n",
    "KEY_ID = 0\n",
    "review_id = review_ids[11][KEY_ID]\n",
    "print(\"Review ID:\", review_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review text:\n",
      "A Priceless Treasure Bobby Short was a favorite performer of mine since I moved to New York in 1955. A friend had his LP with \"At the Moving Picture Ball\" on it and I insisted she play it whenever I dropped by for coffee, which I did every morning when I was between jobs. I always intended to splurge and go see him at the Carlyle but I was too poor and then too populist for a night club and then too stingy. And there were always the great, great records. And suddenly, after only half a century, he was gone. Thank god for this DVD of a wonderful performance at the club. The ebullience, the superb artistry and the glow of his personal niceness make it a marvelous experience to treasure over and over. And he does \"Moving Picture Ball\" and other personal favorites, \"On the Amazon\" and \"Why Shouldn't I?\"  Bobby fans won't need prompting but this great treat should also be a key discovery for anyone interested in popular song styling of the civilized pre-wail-and-whine era, in Manhattan high life, in the triumph of individual striving and dedication, in... Oh hell, in anything that's fun. -- Paul Rawlings, Bayport, N.Y..\n",
      "\n",
      "All data\n",
      "{'great': 3, 'bobby': 2, 'club': 2, 'moving': 2, 'personal': 2, 'treasure': 2, 'picture': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'dvd': 1, 'era': 1, 'performance': 1, 'song': 1, 'morning': 1, 'amazon': 1, 'pre': 1, 'need': 1, 'wonderful': 1, 'experience': 1, 'marvelous': 1, 'night': 1, 'gone': 1, 'half': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'thank': 1, 'hell': 1, 'short': 1, 'fun': 1, 'play': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'god': 1, 'interested': 1, 'superb': 1, 'york': 1, 'century': 1, 'key': 1, 'poor': 1, 'priceless': 1, 'intended': 1, 'popular': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'wail': 1, 'insisted': 1, 'discovery': 1, 'civilized': 1, 'paul': 1, 'striving': 1, 'styling': 1, 'triumph': 1, 'dedication': 1, 'jobs': 1, 'performer': 1, 'glow': 1, 'dropped': 1, 'records': 1, 'prompting': 1, 'manhattan': 1, 'whine': 1, '1955': 1, 'stingy': 1, 'splurge': 1, 'carlyle': 1, 'populist': 1, 'ebullience': 1, 'niceness': 1, 'rawlings': 1, 'bayport': 1}\n",
      "\n",
      "Min word occurences: 1000\n",
      "{'great': 3, 'bobby': 2, 'club': 2, 'moving': 2, 'personal': 2, 'treasure': 2, 'picture': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'dvd': 1, 'era': 1, 'performance': 1, 'song': 1, 'morning': 1, 'amazon': 1, 'pre': 1, 'need': 1, 'wonderful': 1, 'experience': 1, 'marvelous': 1, 'night': 1, 'gone': 1, 'half': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'thank': 1, 'hell': 1, 'short': 1, 'fun': 1, 'play': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'god': 1, 'interested': 1, 'superb': 1, 'york': 1, 'century': 1, 'key': 1, 'poor': 1, 'priceless': 1, 'intended': 1, 'popular': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'insisted': 1, 'discovery': 1, 'civilized': 1, 'paul': 1, 'triumph': 1, 'dedication': 1, 'jobs': 1, 'performer': 1, 'glow': 1, 'dropped': 1, 'records': 1, 'manhattan': 1, '1955': 1}\n",
      "\n",
      "Min word occurences: 1000, only negative\n",
      "{'hell': 1, 'poor': 1}\n",
      "\n",
      "Min word occurences: 1000, only positive\n",
      "{'great': 3, 'treasure': 2, 'wonderful': 1, 'marvelous': 1, 'favorite': 1, 'thank': 1, 'fun': 1, 'fans': 1, 'superb': 1, 'priceless': 1, 'popular': 1, 'triumph': 1, 'glow': 1}\n",
      "\n",
      "2007 positive reviews\n",
      "{'great': 3, 'bobby': 2, 'club': 2, 'moving': 2, 'personal': 2, 'treasure': 2, 'picture': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'dvd': 1, 'era': 1, 'performance': 1, 'song': 1, 'morning': 1, 'amazon': 1, 'pre': 1, 'need': 1, 'wonderful': 1, 'experience': 1, 'marvelous': 1, 'night': 1, 'gone': 1, 'half': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'thank': 1, 'hell': 1, 'short': 1, 'fun': 1, 'play': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'god': 1, 'interested': 1, 'superb': 1, 'york': 1, 'century': 1, 'key': 1, 'poor': 1, 'priceless': 1, 'intended': 1, 'popular': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'wail': 1, 'insisted': 1, 'discovery': 1, 'civilized': 1, 'paul': 1, 'striving': 1, 'styling': 1, 'triumph': 1, 'dedication': 1, 'jobs': 1, 'performer': 1, 'glow': 1, 'dropped': 1, 'records': 1, 'prompting': 1, 'manhattan': 1, 'whine': 1, '1955': 1, 'stingy': 1, 'splurge': 1, 'carlyle': 1, 'populist': 1, 'ebullience': 1, 'niceness': 1, 'rawlings': 1, 'bayport': 1}\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Review text:')\n",
    "print(get_text(review_id))\n",
    "\n",
    "print()\n",
    "print('All data')\n",
    "print(m.get_token_counts(review_id))\n",
    "\n",
    "print()\n",
    "print('Min word occurences: 1000')\n",
    "print(m1k.get_token_counts(review_id))\n",
    "\n",
    "print()\n",
    "print('Min word occurences: 1000, only negative')\n",
    "print(m_neg.get_token_counts(review_id))\n",
    "\n",
    "print()\n",
    "print('Min word occurences: 1000, only positive')\n",
    "print(m_pos.get_token_counts(review_id))\n",
    "\n",
    "print()\n",
    "print('2007 positive reviews')\n",
    "print(m_2007_pos.get_token_counts(review_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare years\n",
    "\n",
    "- Assume 50/50 to 40/60 (neg/pos)\n",
    "- Goal: Get and explain drift. Should be based on tokens (words) and also documents.  \n",
    "  Drift in docs: Both, Bow and also BERT (focus on semantics, not words), should have advantages\n",
    "- Explaination on docs: Set of docs (prototypes) would be returned.  \n",
    "  -> Each of the 60% positive docs should be valid. And none of the 40% negative docs.  \n",
    "  -> Goal of algo will be to have exact numbers. Benchmark data should also be exact.\n",
    "\n",
    "Same negative words should be be included in neg reviews of both years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 19192 review IDs\n",
      "Document-term matrix:        (19192, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 19192 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n",
      "Filtering. Based on 21078 review IDs\n",
      "Document-term matrix:        (21078, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 21078 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n",
      "Filtering. Based on 141293 review IDs\n",
      "Document-term matrix:        (141293, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 141293 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m_2007_neg = m.filter_reviews(years=[2007], stars=[1,2])\n",
    "m_2008_neg = m.filter_reviews(years=[2008], stars=[1,2])\n",
    "m_2008_pos = m.filter_reviews(years=[2008], stars=[4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 29057), ('film', 17089), ('like', 11750), ('dvd', 8174), ('good', 8080), ('time', 7281), ('bad', 6899), ('story', 6160), ('people', 4608), ('movies', 4533), ('way', 4327), ('watch', 4200), ('better', 4194), ('great', 4130), ('know', 3804), ('think', 3783), ('plot', 3602), ('characters', 3559), ('character', 3515), ('money', 3466), ('little', 3343), ('acting', 3339), ('seen', 3062), ('end', 3022), ('watching', 2939), ('want', 2867), ('films', 2860), ('scenes', 2817), ('thing', 2747), ('love', 2694)]\n",
      "\n",
      "[('movie', 32201), ('film', 17625), ('like', 12847), ('good', 8949), ('dvd', 8424), ('time', 8088), ('bad', 7954), ('story', 6729), ('people', 5127), ('movies', 5110), ('better', 4675), ('way', 4631), ('watch', 4476), ('great', 4275), ('know', 4166), ('think', 4043), ('plot', 3959), ('characters', 3925), ('money', 3899), ('acting', 3853), ('character', 3829), ('little', 3446), ('seen', 3443), ('watching', 3372), ('end', 3246), ('love', 3138), ('waste', 3091), ('films', 3072), ('going', 3061), ('want', 3021)]\n",
      "\n",
      "[('movie', 121560), ('film', 88308), ('great', 82215), ('dvd', 68679), ('good', 60615), ('like', 58854), ('time', 45871), ('love', 42648), ('story', 38209), ('best', 35818), ('series', 32793), ('watch', 28519), ('season', 25229), ('movies', 24729), ('way', 24406), ('life', 23648), ('people', 23186), ('new', 21681), ('little', 21166), ('set', 20776), ('think', 20314), ('better', 20223), ('years', 19688), ('seen', 19671), ('old', 18972), ('music', 18657), ('know', 18484), ('video', 18335), ('fun', 18313), ('films', 18275)]\n"
     ]
    }
   ],
   "source": [
    "print(list(m_2007_neg.get_overall_token_counts().items())[:30])\n",
    "print()\n",
    "print(list(m_2008_neg.get_overall_token_counts().items())[:30])\n",
    "print()\n",
    "print(list(m_2008_pos.get_overall_token_counts().items())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_2007_neg = m_2007_neg.get_overall_token_occurences()\n",
    "occur_2007_pos = m_2007_pos.get_overall_token_occurences()\n",
    "occur_2008_neg = m_2008_neg.get_overall_token_occurences()\n",
    "occur_2008_pos = m_2008_pos.get_overall_token_occurences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp tests \n",
    "# set(['a', 'b']).union({'b', 'c'})\n",
    "# set(['a', 'b']).union({'b':2, 'c'=3})\n",
    "#print(list(m_2007_neg.get_overall_token_occurences().items())[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
