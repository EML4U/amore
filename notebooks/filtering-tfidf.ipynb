{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering: TFIDF\n",
    "\n",
    "Computes TFIDF matrix and reduces results to 2 dimensions.\n",
    "\n",
    "Reads `deduplicated.pickle.bz2`, writes `tsvd.pickle.bz2` and `umap.pickle.bz2`.\n",
    "\n",
    "Data format: `{year {star [(number, year, star)] } }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.printer import Printer\n",
    "\n",
    "KEY_NUMBER = 0\n",
    "#KEY_YEAR   = 1\n",
    "#KEY_STAR   = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_storage = FileStorage()\n",
    "printer = Printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ysl(ysl):\n",
    "    c = 0\n",
    "    for year in ysl.keys():\n",
    "        for star in ysl[year].keys():\n",
    "            c += len(ysl[year][star])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read deduplicated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews in ys_lists: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read cache\n",
    "ys_lists = InterimStorage('deduplicated').read()\n",
    "print('Reviews in ys_lists:', count_ysl(ys_lists))\n",
    "# Reviews in ys_lists: 1,727,821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews in ys_lists: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Print table\n",
    "if False:\n",
    "    printer.ipython_display(printer.get_dataframe_with_sums(ys_lists))\n",
    "if False:\n",
    "    print(printer.get_dataframe_markdown(printer.get_dataframe_with_sums(ys_lists), float_as_integer=True, tablefmt=\"pipe\"))\n",
    "print('Reviews in ys_lists:', count_ysl(ys_lists))\n",
    "# Reviews in ys_lists: 1,727,821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicated\n",
    "\n",
    "|     |   1997 |   1998 |   1999 |   2000 |   2001 |   2002 |   2003 |   2004 |   2005 |   2006 |   2007 |   2008 |   2009 |   2010 |   2011 |   2012 |     Sum |\n",
    "|:----|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|\n",
    "| 1   |      2 |     26 |    597 |   2512 |   3015 |   3597 |   3689 |   6643 |  10413 |   9943 |  11125 |  12661 |  14150 |  15822 |  19132 |  21570 |  134897 |\n",
    "| 2   |    nan |     30 |    437 |   2162 |   2541 |   3048 |   3364 |   4880 |   7053 |   7050 |   8067 |   8417 |   8846 |   9536 |  11363 |  12041 |   88835 |\n",
    "| 3   |      1 |     65 |    880 |   3932 |   4562 |   5064 |   5860 |   8592 |  11420 |  11322 |  13932 |  13944 |  14835 |  14925 |  16796 |  17593 |  143723 |\n",
    "| 4   |      4 |    146 |   2166 |   9832 |  11216 |  12257 |  13466 |  19364 |  25958 |  27917 |  37664 |  36838 |  37089 |  36408 |  40392 |  40528 |  351245 |\n",
    "| 5   |     14 |    561 |   7266 |  25204 |  26294 |  29576 |  32416 |  46222 |  64445 |  71619 | 108952 | 104455 | 112998 | 113957 | 130571 | 134571 | 1009121 |\n",
    "| Sum |     21 |    828 |  11346 |  43642 |  47628 |  53542 |  58795 |  85701 | 119289 | 127851 | 179740 | 176315 | 187918 | 190648 | 218254 | 226303 | 1727821 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ID-numbers\n",
    "\n",
    "- years: 2006 - 2012\n",
    "- stars: 1, 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203682 69.66473957661124\n",
      "524139 30.335260423388764\n"
     ]
    }
   ],
   "source": [
    "review_numbers = set()\n",
    "for year in ys_lists.keys():\n",
    "    if(year < 2006):\n",
    "        continue\n",
    "    for star in ys_lists[year].keys():\n",
    "        if(star == 3):\n",
    "            continue\n",
    "        for tup in ys_lists[year][star]:\n",
    "            review_numbers.add(tup[KEY_NUMBER])\n",
    "\n",
    "total_deduplicated = count_ysl(ys_lists)\n",
    "print(len(review_numbers), len(review_numbers)*100/total_deduplicated)\n",
    "print(total_deduplicated-len(review_numbers), (total_deduplicated-len(review_numbers))*100/total_deduplicated)\n",
    "# 1,203,682 69.66473957661124\n",
    "#   524,139 30.335260423388764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Was used for external embeddings. Not required at the moment.\n",
    "    print(InterimStorage('reviewnumbers_2006-2012_posneg').write(review_numbers).get_filepath())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203682\n"
     ]
    }
   ],
   "source": [
    "revno_to_text = {}\n",
    "\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, max_docs=-1)\n",
    "for item in reader:\n",
    "    if item[AmazonReviewsReader.KEY_NUMBER] in review_numbers:\n",
    "        revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print(len(revno_to_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from gensim.parsing.preprocessing import STOPWORDS as stopwords_gensim\n",
    "    print('stopwords_gensim', len(stopwords_gensim))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    #import nltk\n",
    "    #nltk.download('stopwords')\n",
    "    stopwords_nltk = set(stopwords.words('english'))\n",
    "    print('stopwords_nltk', len(stopwords_nltk))\n",
    "\n",
    "    from sklearn.feature_extraction import _stop_words\n",
    "    stopwords_sklearn = _stop_words.ENGLISH_STOP_WORDS\n",
    "    print('stopwords_sklearn', len(stopwords_sklearn))\n",
    "\n",
    "    import spacy\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    stopwords_spacy = spacy.load(\"en_core_web_sm\").Defaults.stop_words\n",
    "    print('stopwords_spacy', len(stopwords_spacy))\n",
    "\n",
    "    stopwords_all = stopwords_nltk.union(stopwords_sklearn).union(stopwords_spacy).union(stopwords_gensim)\n",
    "    # stopwords_gensim 337\n",
    "    # stopwords_nltk 179\n",
    "    # stopwords_sklearn 318\n",
    "    # stopwords_spacy 326\n",
    "    # stopwords_all 412\n",
    "\n",
    "    print(InterimStorage('stopwords').write(stopwords_all).get_filepath())\n",
    "else:\n",
    "    stopwords_all = InterimStorage('stopwords').read()\n",
    "print('stopwords_all', len(stopwords_all))\n",
    "# stopwords_all 412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'other', 'all', 'did', 'whereupon', 'whole', 'its', 'con', 'wasn', 'four', 'wherein', 'o', 'be', \"weren't\", 'because', \"you're\", 'again', 'nine', 'am', 'twenty', 'de', 'however', 'doesn', 'could', 'whose', 'un', 'may', 'toward', 'hundred', 'would', 'out', '‘ve', 'if', 'some', 'anywhere', 'perhaps', '‘m', \"didn't\", 'becoming', 'five', 'system', 'on', 'me', 'mine', 'least', 'meanwhile', \"mustn't\", 'two', 'mostly', '’d', 'fifteen', 'having', 'per', 'further', 'whatever', 'top', 'shan', 'himself', 'several', 'thereafter', 'part', 'about', 'cannot', 'whoever', '’s', 'seem', 'too', 'rather', 'latterly', 'sometime', 'where', 'off', 'anyway', 'really', 'last', 'many', 'when', 'whither', 'unless', 'hereby', 'though', 'become', 'will', 'own', 'front', \"should've\", 'above', 'along', 'another', 'an', 'what', 'cant', \"haven't\", 'm', 'for', 'doing', \"wouldn't\", 'using', 'hereafter', 'eg', 'mightn', 'beforehand', 'anyone', 'none', 'are', 'why', 'empty', 'which', '’ll', 'everywhere', 'co', 'same', 'cry', 'seems', 'afterwards', 'that', 'via', 'something', 'quite', 'find', '’re', 'upon', 'everything', 'around', 've', 'under', 'the', 'km', 'very', \"you'd\", 'take', 'my', 'we', 'otherwise', 'get', 'former', 'once', 'kg', 'then', 'should', 'various', 'name', 'became', 'ours', 'everyone', 'yourself', \"isn't\", 'yours', 'first', 'either', 'bill', 'it', 'haven', 'these', 'call', 'until', 'couldn', 'whereas', 'while', 'hence', 'those', 'hadn', \"n't\", 'whereby', 'put', 'seemed', 'found', 'thru', 'amount', 'whether', 'without', 'won', 'six', '‘ll', 'besides', 'there', 'within', '’m', 'ourselves', 'were', 'throughout', 'except', 'hasnt', \"doesn't\", 'beside', 'down', 'than', 'sixty', 'beyond', '‘s', 'also', 'in', 'formerly', 'fill', 'among', 'nowhere', \"won't\", 'sometimes', 'ie', 'being', 'make', 'else', 'anything', \"needn't\", 'her', 'therefore', 'seeming', 'computer', \"couldn't\", 'herein', 'enough', 'therein', 'but', 'd', 'becomes', 'ma', \"you'll\", 'twelve', 'three', 'shouldn', 'he', 'wherever', 'our', 'eleven', 'have', 'already', \"hasn't\", 'show', 'yet', 'wouldn', 'few', 'herself', 'every', 'whenever', 'isn', 'give', \"don't\", 'even', 'amongst', 'always', \"'s\", \"hadn't\", 'before', 'weren', 'now', 'do', 'nevertheless', 'much', 'most', 'might', 'how', 'needn', 'anyhow', 'your', 'any', 'itself', 'others', \"aren't\", 'whence', 'hereupon', 'interest', 'one', 'thus', 'side', 'this', 'used', 'next', 'since', 'you', 'n’t', 'serious', 'his', 'behind', 'must', 'somewhere', 'at', 'fifty', 'go', 'and', 'mustn', 'she', 'ever', 'nobody', 'still', 'neither', 'no', 'moreover', \"it's\", 'nor', 'during', 'across', \"'m\", \"wasn't\", 'bottom', '‘d', 'more', 'back', \"she's\", 'thick', 'theirs', '’ve', \"shouldn't\", 'a', 'eight', 'couldnt', 'yourselves', 'noone', 'thin', \"'d\", 'hers', 'of', \"'ll\", 'thereby', 'is', 'due', 'over', 'has', \"you've\", 'whereafter', 'keep', 'so', 'to', 'with', 'through', 'amoungst', 'done', 'together', 'never', 'well', 's', 'him', 'someone', 'don', 'third', 'onto', 'mill', 'thereupon', 'move', 'somehow', 'n‘t', 't', 'such', 'i', 'into', 'each', 'ca', 'sincere', 'often', 'fire', 'made', 'them', \"shan't\", 're', 'below', 'here', 'between', 'only', 'ten', \"'ve\", 'from', 'myself', 'almost', 'by', 'as', 'describe', 'alone', 'against', \"mightn't\", 'elsewhere', 'been', 'aren', 'inc', 'had', 'latter', 'full', 'please', 'forty', 'thence', 'ltd', 'does', 'etc', 'after', 'say', 'can', 'detail', 'was', 'towards', 'll', 'whom', 'themselves', 'or', \"that'll\", 'both', 'although', 'up', \"'re\", 'who', 'nothing', 'namely', 'not', '‘re', 'ain', 'us', 'less', 'they', 'see', 'their', 'didn', 'just', 'indeed', 'hasn', 'y', 'regarding'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Alternatives (e.g. CountVectorizer): https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203682, 1000) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "index_to_num = {}\n",
    "for i, item in enumerate(revno_to_text.items()):\n",
    "    corpus.append(item[1])\n",
    "    index_to_num[i] = item[0]\n",
    "\n",
    "if False:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    # https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "    \n",
    "    # Installation:    \n",
    "    #import nltk\n",
    "    #nltk.download('punkt')\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk import word_tokenize\n",
    "    import string\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    \n",
    "    def stem_tokens(tokens):\n",
    "        return [stemmer.stem(item) for item in tokens if len(item)>=3]\n",
    "\n",
    "    def normalize(text):\n",
    "        return stem_tokens(word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "    #stop_words = \"english\"\n",
    "    stop_words = stopwords_all\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    corpus = list(revno_to_text.values())\n",
    "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words=stop_words, max_features=1000)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    print(InterimStorage('tfidf-object').write(tfidf).get_filepath())\n",
    "    print(InterimStorage('tfidf').write(results).get_filepath())\n",
    "    print(InterimStorage('tfidf-vectorizer').write(vectorizer.vocabulary_).get_filepath())\n",
    "    \n",
    "    print(len(vectorizer.get_feature_names()))\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(len(vectorizer.get_stop_words()))\n",
    "    \n",
    "else:\n",
    "    tfidf = InterimStorage('tfidf-object').read()\n",
    "    \n",
    "print(tfidf.shape, type(tfidf))\n",
    "# (1203682, 1000) <class 'scipy.sparse.csr.csr_matrix'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "- https://neptune.ai/blog/dimensionality-reduction\n",
    "\n",
    "### TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=2)\n",
    "tsvd_results = tsvd.fit(tfidf).transform(tfidf)\n",
    "print(tsvd_results.shape, type(tsvd_results))\n",
    "# (1203682, 2) <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map revnum to results\n",
    "results = {}\n",
    "for i_to_num in index_to_num.items():\n",
    "    results[i_to_num[1]] = tsvd_results[i_to_num[0]].tolist()\n",
    "print(len(results))\n",
    "# 1203682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(InterimStorage('tsvd').write(results).get_filepath())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "- https://umap-learn.readthedocs.io/en/latest/sparse.html\n",
    "- [dimension-reduction.py](../scripts/dimension-reduction.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running UMAP with input scipy.sparse.csr.csr_matrix:\n",
    "# \"/home/eml4u/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:125:\n",
    "# SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executed in external script\n",
    "#time python3 dimension-reduction.py\n",
    "#Reading data\n",
    "#Transforming to lil_matrix\n",
    "#UMAP\n",
    "#/home/eml4u/.local/lib/python3.8/site-packages/umap/umap_.py:125: UserWarning: A few of your vertices were disconnected from the manifold.  This shouldn't cause problems.\n",
    "#Disconnection_distance = 1 has removed 1040 edges.\n",
    "#It has only fully disconnected 47 vertices.\n",
    "#Use umap.utils.disconnected_vertices() to identify them.\n",
    "#  warn(\n",
    "#/tmp/InterimStorage/tfidf-umap.pickle.bz2\n",
    "#<class 'numpy.ndarray'>\n",
    "#(1203682, 2)\n",
    "#<class 'numpy.ndarray'>\n",
    "#\n",
    "#real    69m44.076s\n",
    "#user    220m56.252s\n",
    "#sys     12m44.093s\n",
    "if False:\n",
    "    import sys; sys.path.insert(0, '../')\n",
    "    from access.interim_storage import InterimStorage\n",
    "\n",
    "    print('Reading data')\n",
    "    results = InterimStorage('tfidf-object').read()\n",
    "\n",
    "    print('Transforming to lil_matrix')\n",
    "    from scipy.sparse import lil_matrix\n",
    "    lil_tfidf = results.tolil()\n",
    "    results = None\n",
    "\n",
    "    print('UMAP')\n",
    "    from umap.umap_ import UMAP\n",
    "    umap_results = UMAP(metric='cosine', low_memory=True, n_epochs=50).fit_transform(lil_tfidf)\n",
    "    print(InterimStorage('tfidf-umap').write(umap_results).get_filepath())\n",
    "\n",
    "    print(type(umap_results))\n",
    "    print(umap_results.shape)\n",
    "    print(type(umap_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_results = InterimStorage('tfidf-umap').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203682\n"
     ]
    }
   ],
   "source": [
    "umap_results_mapped = {}\n",
    "for i_to_num in index_to_num.items():\n",
    "    umap_results_mapped[i_to_num[1]] = umap_results[i_to_num[0]].tolist()\n",
    "print(len(umap_results_mapped))\n",
    "# 1,203,682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/InterimStorage/umap.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "print(InterimStorage('umap').write(umap_results_mapped).get_filepath())\n",
    "# /tmp/InterimStorage/umap.pickle.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
