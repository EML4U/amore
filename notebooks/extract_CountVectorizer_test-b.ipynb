{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 are not included in Doc2Vec embeddings. (2000 to 2012 included.)\n",
    "- Ideas:\n",
    "    - 100/0 to 0/100 neg/pos\n",
    "    - 50/50 to 40/60 neg/pos\n",
    "    - build on results on that: other distributions, e.g. 45/55\n",
    "    - 50/50 to 40/30/30 neg/posCluster1/posCluster2\n",
    "    - for token-level and document-level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 307.6331848814152\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 39.86092622112483\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could be improved by using matrix instead of dict.\n",
    "        \n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could maybe be improved by using matrix instead of dict.\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1\n",
    "                \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 607181), 17466 reviews, 607181 tokens\n",
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 607181), 90403 reviews, 607181 tokens\n",
      "Filtering. Based on 16993 review IDs\n",
      "Created: Matrix (16993, 607181), 16993 reviews, 607181 tokens\n",
      "Filtering. Based on 99536 review IDs\n",
      "Created: Matrix (99536, 607181), 99536 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "m_2000_neg = matrix.filter_keep_reviews(get_review_ids(years=[2005], stars=[1,2]))\n",
    "m_2000_pos = matrix.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))\n",
    "m_2001_neg = matrix.filter_keep_reviews(get_review_ids(years=[2006], stars=[1,2]))\n",
    "m_2001_pos = matrix.filter_keep_reviews(get_review_ids(years=[2006], stars=[4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens which occur predominantly in only one dataset: 1.) Get ratio for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter rare\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            #rare.append(tok_occ[0])\n",
    "            rare[tok_occ[0]] = tok_occ[1] + b_token_occurences[tok_occ[0]]\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 153373 / 63005\n",
      "Output sizes: ratio 228 ; rare 50075 ; only a 103070 ; only b 12702\n",
      "\n",
      "ratio_a [('excellent', 8.86), ('awesome', 8.71), ('wonderful', 8.41), ('amazing', 8.15), ('highly', 7.59), ('perfect', 7.34), ('favorite', 7.01), ('brilliant', 6.49), ('season', 5.89), ('collection', 5.38), ('beautiful', 5.22), ('enjoyed', 5.13), ('heart', 4.96), ('extras', 4.95), ('loved', 4.85), ('great', 4.62), ('episodes', 4.58), ('features', 4.57), ('best', 4.54), ('classic', 4.52), ('today', 4.49), ('episode', 4.38), ('definitely', 4.27), ('performances', 4.26), ('song', 4.24), ('family', 4.22), ('works', 4.22), ('songs', 4.19), ('shows', 4.17), ('fun', 4.08), ('love', 4.06), ('enjoy', 4.05), ('gives', 3.97), ('including', 3.85), ('drama', 3.68), ('young', 3.66), ('lives', 3.64), ('recommend', 3.58), ('father', 3.46), ('days', 3.44), ('early', 3.43), ('years', 3.36), ('job', 3.34), ('finally', 3.33), ('entertaining', 3.32), ('music', 3.3), ('performance', 3.28), ('human', 3.27), ('nice', 3.26), ('world', 3.24)]\n",
      "\n",
      "ratio_b [('bad', 1.45), ('money', 1.43), ('minutes', 1.03), ('plot', 0.9), ('horror', 0.83), ('maybe', 0.82), ('acting', 0.79), ('reason', 0.77), ('thing', 0.74), ('trying', 0.72), ('looks', 0.72), ('guy', 0.7), ('let', 0.69), ('effects', 0.69), ('star', 0.68), ('actually', 0.67), ('read', 0.67), ('believe', 0.66), ('sense', 0.66), ('point', 0.65), ('left', 0.65), ('movie', 0.64), ('going', 0.63), ('rest', 0.63), ('better', 0.62), ('book', 0.62), ('fact', 0.61), ('hard', 0.61), ('want', 0.6), ('actors', 0.6), ('pretty', 0.6), ('scene', 0.59), ('thought', 0.59), ('looking', 0.59), ('review', 0.59), ('tell', 0.59), ('stars', 0.58), ('director', 0.58), ('said', 0.58), ('line', 0.58), ('buy', 0.57), ('big', 0.57), ('far', 0.57), ('kind', 0.57), ('movies', 0.56), ('got', 0.56), ('original', 0.56), ('version', 0.56), ('entire', 0.56), ('simply', 0.55)]\n",
      "\n",
      "rare [('instead', 5790), ('past', 4558), ('mother', 4518), ('reviews', 5075), ('try', 5034), ('fantastic', 4109), ('soon', 4410), ('coming', 4425), ('son', 4374), ('bought', 4725)]\n",
      "\n",
      "only_a [('emmys', 112), ('dreamer', 102), ('transfere', 100), ('withers', 88), ('byrd', 86), ('gifford', 84), ('excelent', 84), ('hatton', 82), ('greys', 80), ('rusesabagina', 75)]\n",
      "\n",
      "only_b [('snoozefest', 12), ('djmon', 12), ('dumberer', 9), ('yawned', 8), ('guterman', 7), ('cra', 6), ('alejate', 6), ('cters', 6), ('timbo', 6), ('stinko', 6)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_2000_pos,\n",
    "    m_2000_neg,\n",
    "    min_token_occurences=4000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'awesome', 'wonderful', 'amazing', 'perfect', 'favorite', 'brilliant', 'beautiful', 'enjoyed', 'heart', 'loved', 'great', 'best', 'fun', 'love', 'enjoy']\n",
      "['bad']\n"
     ]
    }
   ],
   "source": [
    "# filtering top ratio words (found 16 with only some blacklisted)\n",
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 16, ['highly',\n",
    "                                                          'season', 'collection',\n",
    "                                                          'extras', 'episodes', 'features',\n",
    "                                                          'classic', 'today', 'episode', 'definitely', 'performances', 'song', 'family', 'works', 'songs', 'shows'])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "# min ratio had to set down to 1.4 to get at least one word\n",
    "# money seems to be a thing\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.4, 10, ['money'])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 152728 / 61592\n",
      "Output sizes: ratio 227 ; rare 49486 ; only a 103015 ; only b 11879\n",
      "\n",
      "ratio_a [('awesome', 10.43), ('wonderful', 9.68), ('excellent', 9.32), ('amazing', 9.26), ('highly', 9.03), ('perfect', 7.76), ('fantastic', 7.75), ('favorite', 7.61), ('season', 7.14), ('collection', 6.17), ('loved', 6.09), ('enjoyed', 5.87), ('heart', 5.49), ('episodes', 5.44), ('great', 5.17), ('fun', 5.0), ('features', 4.96), ('classic', 4.94), ('best', 4.92), ('beautiful', 4.89), ('shows', 4.64), ('enjoy', 4.64), ('definitely', 4.56), ('episode', 4.46), ('love', 4.39), ('today', 4.38), ('songs', 4.28), ('recommend', 4.26), ('lives', 4.18), ('family', 4.14), ('works', 4.05), ('performances', 4.02), ('drama', 3.99), ('happy', 3.96), ('gives', 3.94), ('series', 3.8), ('entertaining', 3.75), ('including', 3.65), ('music', 3.64), ('performance', 3.61), ('live', 3.59), ('father', 3.59), ('early', 3.58), ('young', 3.53), ('john', 3.53), ('job', 3.46), ('nice', 3.46), ('history', 3.45), ('years', 3.43), ('liked', 3.41)]\n",
      "ratio_a ['awesome', 'wonderful', 'excellent', 'amazing', 'highly', 'perfect', 'fantastic', 'favorite', 'season', 'collection', 'loved', 'enjoyed', 'heart', 'episodes', 'great', 'fun', 'features', 'classic', 'best', 'beautiful', 'shows', 'enjoy', 'definitely', 'episode', 'love', 'today', 'songs', 'recommend', 'lives', 'family', 'works', 'performances', 'drama', 'happy', 'gives', 'series', 'entertaining', 'including', 'music', 'performance', 'live', 'father', 'early', 'young', 'john', 'job', 'nice', 'history', 'years', 'liked']\n",
      "\n",
      "ratio_b [('money', 1.52), ('bad', 1.37), ('instead', 1.01), ('minutes', 0.98), ('plot', 0.88), ('acting', 0.76), ('reason', 0.76), ('guy', 0.72), ('star', 0.7), ('let', 0.68), ('trying', 0.67), ('thing', 0.66), ('looks', 0.66), ('point', 0.65), ('effects', 0.62), ('actually', 0.61), ('sense', 0.61), ('try', 0.61), ('movie', 0.6), ('pretty', 0.6), ('believe', 0.6), ('read', 0.6), ('left', 0.6), ('scene', 0.59), ('stars', 0.58), ('better', 0.57), ('movies', 0.57), ('going', 0.57), ('actors', 0.57), ('scenes', 0.56), ('original', 0.56), ('hard', 0.56), ('simply', 0.56), ('ending', 0.56), ('tell', 0.56), ('thought', 0.55), ('given', 0.55), ('want', 0.54), ('buy', 0.54), ('review', 0.54), ('far', 0.53), ('fact', 0.53), ('kind', 0.53), ('line', 0.53), ('rest', 0.53), ('lost', 0.53), ('know', 0.52), ('character', 0.52), ('big', 0.52), ('director', 0.52)]\n",
      "\n",
      "rare [('audience', 4841), ('maybe', 5322), ('beginning', 4615), ('documentary', 4457), ('friend', 4548), ('experience', 4423), ('easy', 4287), ('horror', 5259), ('extras', 4277), ('person', 4632)]\n",
      "\n",
      "only_a [('transfere', 238), ('squats', 134), ('chrissy', 123), ('mcvay', 105), ('monogram', 99), ('magers', 93), ('nareau', 93), ('1907', 92), ('hopalong', 91), ('stealer', 88)]\n",
      "\n",
      "only_b [('unentertaining', 15), ('crapfest', 12), ('uncompelling', 10), ('pos', 10), ('contagium', 7), ('unsuspenseful', 7), ('lommel', 6), ('awfull', 6), ('impale', 5), ('stinko', 5)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_2001_pos,\n",
    "    m_2001_neg,\n",
    "    min_token_occurences=4000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print('ratio_a', list(ratio_a.keys())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens which occur predominantly in only one dataset: 2.) Get top tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome', 'wonderful', 'excellent', 'amazing', 'perfect', 'fantastic', 'favorite', 'loved', 'enjoyed', 'heart', 'great', 'fun', 'features', 'classic', 'best', 'beautiful']\n",
      "['money', 'bad']\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 16, ['highly',\n",
    "                                                          'season', 'collection',\n",
    "                                                          'episodes'])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.3, 10, ['money'])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 14), 1584098 reviews, 14 tokens\n",
      "['amazing', 'awesome', 'bad', 'beautiful', 'best', 'brilliant', 'enjoyed', 'excellent', 'favorite', 'great', 'heart', 'loved', 'perfect', 'wonderful'] 14\n"
     ]
    }
   ],
   "source": [
    "matrix_filtered = matrix.filter_tokens(filter_tokens_a + filter_tokens_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 14), 90403 reviews, 14 tokens\n"
     ]
    }
   ],
   "source": [
    "matrix_filtered_pos = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 14), 17466 reviews, 14 tokens\n"
     ]
    }
   ],
   "source": [
    "matrix_filtered_neg = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make lists of words (Atok Btok) to be used for filtering for both distributions\n",
    "- add review to list e.g. if at least 20% or 5 tokens of Atok are included and less than 10% or 2 tokens of Btok are included.\n",
    "\n",
    "TODO:\n",
    "\n",
    "-  Sum up occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69239\n"
     ]
    }
   ],
   "source": [
    "# in: tokens\n",
    "# out: revId to counts of all tokens\n",
    "# get token indices\n",
    "# iterate nonzero[0] and filter by if token_index searched\n",
    "# check nonzero[1] which is revIndex and create revIndex to number of token found.\n",
    "\n",
    "reviews_token_occurences = matrix_filtered_pos.filter_reviews_by_tokens(filter_tokens_a)\n",
    "print(len(reviews_token_occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if False:\n",
    "    it = iter(reviews_token_occurences.items())\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "    print(filter_tokens_a)\n",
    "    get_text(6589)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[1]))\n",
    "    print(m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "# print_matrix_info = None\n",
    "# get_test_matrix = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
