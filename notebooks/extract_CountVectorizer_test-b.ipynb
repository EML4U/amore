{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 contain only a few reviews and are not included in Doc2Vec embeddings.  \n",
    "  (The years 2000 to 2012 are included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* **opinion_lexicon**: Instance of OpinionLexicon to access negative and positive words. Not used for now.\n",
    "* **year_star_ids**: Collection of IDs sorted by years and stars. Used afterwards to filter by stars and years in method **get_review_ids**.\n",
    "* **reader**: Instance of AmazonReviewsReader to access review data. Used afterwards to create revno_to_text.\n",
    "* **revno_to_text**: Dictionary review-number to full text. Used afterwards to access review-texts in method **get_text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 296.57380703\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files\n",
    "\n",
    "Data required to create **Matrix** instances afterwards.\n",
    "\n",
    "* **doc_term_matrix**: Matrix containing document-IDs and term-IDs. Used afterwards for building Matrix.\n",
    "* **vocabulary**: Dictionary containing tuples of terms and term-IDs. Used afterwards to create inv_vocabulary.\n",
    "* **inv_vocabulary**: Dictionary containing tuples of term-IDs and terms.Used afterwards to create tokens.\n",
    "* **tokens**: Ordered list of terms, index represents term-ID. Used afterwards as token-indices for building Matrix.\n",
    "* **vecid_revno**: Values used afterwards as review-indices for building Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 37.365150742000026\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access methods and Matrix class\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used afterwards in Matrix class.\n",
    "\n",
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could be improved by using matrix instead of dict.\n",
    "        \n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could maybe be improved by using matrix instead of dict.\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1\n",
    "                \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Main matrix containing all data\n",
    "\n",
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 607181), 17466 reviews, 607181 tokens\n",
      "Filtering. Based on 99536 review IDs\n",
      "Created: Matrix (99536, 607181), 99536 reviews, 607181 tokens\n",
      "Filtering. Based on 117002 review IDs\n",
      "Created: Matrix (117002, 607181), 117002 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "ids_neg = get_review_ids(years=[2005], stars=[1,2])\n",
    "ids_pos = get_review_ids(years=[2006], stars=[4,5])\n",
    "m_neg = matrix.filter_keep_reviews(ids_neg)\n",
    "m_pos = matrix.filter_keep_reviews(ids_pos)\n",
    "m_both = matrix.filter_keep_reviews(ids_neg + ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens which occur predominantly in only one dataset: Get ratio for each token.\n",
    "\n",
    "* **get_ratio**\n",
    "  * In: matrix-A, matrix-B\n",
    "  * In: min_token_occurences (if token occures less than minimum in both matrixes)\n",
    "  * Out: tokens sorted by ratio\n",
    "  * Out: tokens sorted by inverted ratio\n",
    "  * Out: tokens not reached minimum\n",
    "  * Out: tokens only in A\n",
    "  * Out: tokens only in B\n",
    "* **filter_tokens_by_ratio**\n",
    "  * In: ratio generated by method get_ratio\n",
    "  * In: min_ratio minimum threshold \n",
    "  * In: max_tokens maximum number of tokens to return\n",
    "  * In: exclude_tokens tokens not to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    # Pairs of tokens and occurences\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter/remove rare:\n",
    "        # minimum is set AND\n",
    "        # token has to be in both matrixes AND\n",
    "        # threshold reached in A AND\n",
    "        # threshod reached in B AND\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare[tok_occ[0]] = str(tok_occ[1]) + \" / \" + str(b_token_occurences[tok_occ[0]])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 152728 / 63005\n",
      "Output sizes: ratio 227 ; rare 49742 ; only a 102759 ; only b 13036\n",
      "\n",
      "ratio_a [('excellent', 9.74), ('awesome', 9.23), ('wonderful', 9.15), ('highly', 8.62), ('fantastic', 8.43), ('amazing', 8.14), ('perfect', 7.52), ('favorite', 7.28), ('season', 6.63), ('enjoyed', 6.14), ('collection', 5.96), ('loved', 5.72), ('beautiful', 5.37), ('heart', 5.27), ('great', 5.13), ('today', 4.67), ('fun', 4.63), ('best', 4.62), ('definitely', 4.62), ('family', 4.61), ('classic', 4.6), ('features', 4.57), ('enjoy', 4.51), ('episodes', 4.49), ('love', 4.39), ('works', 4.34), ('shows', 4.28), ('episode', 4.28), ('performances', 4.28), ('happy', 4.23), ('recommend', 4.06), ('songs', 3.95), ('drama', 3.93), ('gives', 3.91), ('entertaining', 3.77), ('young', 3.73), ('including', 3.73), ('lives', 3.68), ('series', 3.62), ('nice', 3.59), ('father', 3.58), ('years', 3.54), ('days', 3.53), ('son', 3.49), ('early', 3.46), ('job', 3.42), ('set', 3.41), ('finally', 3.33), ('music', 3.32), ('true', 3.32)]\n",
      "\n",
      "ratio_b [('money', 1.52), ('bad', 1.45), ('instead', 1.09), ('minutes', 1.01), ('plot', 0.88), ('reason', 0.78), ('acting', 0.77), ('trying', 0.74), ('thing', 0.72), ('guy', 0.72), ('looks', 0.72), ('star', 0.7), ('effects', 0.69), ('let', 0.68), ('actually', 0.66), ('believe', 0.65), ('read', 0.65), ('sense', 0.65), ('try', 0.65), ('point', 0.64), ('movie', 0.62), ('scene', 0.62), ('left', 0.62), ('rest', 0.62), ('going', 0.61), ('better', 0.6), ('buy', 0.6), ('hard', 0.6), ('tell', 0.6), ('fact', 0.59), ('director', 0.59), ('thought', 0.58), ('stars', 0.58), ('want', 0.57), ('original', 0.57), ('actors', 0.57), ('pretty', 0.57), ('book', 0.57), ('review', 0.57), ('movies', 0.56), ('big', 0.56), ('scenes', 0.55), ('far', 0.55), ('looking', 0.55), ('said', 0.55), ('kind', 0.55), ('ending', 0.55), ('actor', 0.55), ('people', 0.54), ('version', 0.54)]\n",
      "\n",
      "rare [('audience', '3995 / 892'), ('maybe', '3988 / 1343'), ('beginning', '3987 / 714'), ('documentary', '3986 / 447'), ('friend', '3983 / 603'), ('experience', '3982 / 440'), ('easy', '3966 / 294'), ('horror', '3965 / 1364'), ('extras', '3962 / 348'), ('person', '3935 / 789')]\n",
      "\n",
      "only_a [('transfere', 238), ('hostel', 185), ('routh', 162), ('copeland', 150), ('chrissy', 123), ('excelent', 118), ('mcvay', 105), ('woodward', 105), ('kgharris', 96), ('dreamer', 96)]\n",
      "\n",
      "only_b [('quot', 49), ('crapfest', 16), ('nasaly', 15), ('borefest', 15), ('farisreel', 11), ('atuan', 9), ('mancherai', 8), ('bleah', 7), ('guterman', 7), ('cra', 6)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_pos,\n",
    "    m_neg,\n",
    "    min_token_occurences=4000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'awesome', 'wonderful', 'fantastic', 'amazing', 'perfect', 'favorite', 'enjoyed', 'loved', 'beautiful', 'heart', 'great', 'fun', 'best', 'enjoy', 'love', 'happy', 'recommend', 'entertaining']\n"
     ]
    }
   ],
   "source": [
    "# filtering top ratio words (found 16 with only some blacklisted)\n",
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 19,\n",
    "['highly', 'season', 'collection', 'today', 'definitely', 'family', 'classic',\n",
    " 'features', 'episodes', 'works', 'shows', 'episode', 'performances', 'songs',\n",
    " 'drama', 'gives', 'young', 'including'])\n",
    "#['highly',\n",
    "#                                                          'season', 'collection',\n",
    "#                                                          'extras', 'episodes', 'features',\n",
    "#                                                          'classic', 'today', 'episode', 'definitely', 'performances', 'song', 'family', 'works', 'songs', 'shows'])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "# min ratio had to set down to 1.4 to get at least one word\n",
    "# money seems to be a thing\n",
    "#filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.4, 10, ['money'])\n",
    "#print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Create matrix based on 1/2 star data for year x+0.  \n",
    "  Each review should include min n1 neg tokens  \n",
    "  (Each review should include max p1 pos tokens)\n",
    "- Create matrix based on 4/5 star data for year x+1.  \n",
    "  Each review should include min p2 pos tokens  \n",
    "  (Each review should include max n2 neg tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 17), 1584098 reviews, 17 tokens\n"
     ]
    }
   ],
   "source": [
    "matrix_filtered = matrix.filter_tokens(filter_tokens_a + filter_tokens_b)\n",
    "matrix_filtered_neg = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[1,2]))\n",
    "matrix_filtered_pos = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 17), 90403 reviews, 17 tokens\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 17), 17466 reviews, 17 tokens\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make lists of words (Atok Btok) to be used for filtering for both distributions\n",
    "- add review to list e.g. if at least 20% or 5 tokens of Atok are included and less than 10% or 2 tokens of Btok are included.\n",
    "\n",
    "TODO:\n",
    "\n",
    "-  Sum up occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72175\n"
     ]
    }
   ],
   "source": [
    "# in: tokens\n",
    "# out: revId to counts of all tokens\n",
    "# get token indices\n",
    "# iterate nonzero[0] and filter by if token_index searched\n",
    "# check nonzero[1] which is revIndex and create revIndex to number of token found.\n",
    "\n",
    "reviews_token_occurences = matrix_filtered_pos.filter_reviews_by_tokens(filter_tokens_a)\n",
    "print(len(reviews_token_occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if False:\n",
    "    it = iter(reviews_token_occurences.items())\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "    print(filter_tokens_a)\n",
    "    get_text(6589)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[1]))\n",
    "    print(m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "# print_matrix_info = None\n",
    "# get_test_matrix = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
