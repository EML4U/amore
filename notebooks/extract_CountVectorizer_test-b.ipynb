{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 contain only a few reviews and are not included in Doc2Vec embeddings.  \n",
    "  (The years 2000 to 2012 are included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* **opinion_lexicon**: Instance of OpinionLexicon to access negative and positive words. Not used for now.\n",
    "* **year_star_ids**: Collection of IDs sorted by years and stars. Used afterwards to filter by stars and years in method **get_review_ids**.\n",
    "* **reader**: Instance of AmazonReviewsReader to access review data. Used afterwards to create revno_to_text.\n",
    "* **revno_to_text**: Dictionary review-number to full text. Used afterwards to access review-texts in method **get_text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 291.32208580401493\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files\n",
    "\n",
    "Data required to create **Matrix** instances afterwards.\n",
    "\n",
    "* **doc_term_matrix**: Matrix containing document-IDs and term-IDs. Used afterwards for building Matrix.\n",
    "* **vocabulary**: Dictionary containing tuples of terms and term-IDs. Used afterwards to create inv_vocabulary.\n",
    "* **inv_vocabulary**: Dictionary containing tuples of term-IDs and terms.Used afterwards to create tokens.\n",
    "* **tokens**: Ordered list of terms, index represents term-ID. Used afterwards as token-indices for building Matrix.\n",
    "* **vecid_revno**: Values used afterwards as review-indices for building Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 37.48579945595702\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access methods and Matrix class\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used afterwards in Matrix class.\n",
    "\n",
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None # list of tokens\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1  \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences\n",
    "    \n",
    "    def count_token_occurences(self, review_ids, tokens):\n",
    "        \"\"\"\n",
    "        Sums up the occurencies of given tokens in given reviews.\n",
    "        \"\"\"\n",
    "        review_to_tokenoccurences = {}\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "        for review_id in review_ids:\n",
    "            if not review_id in review_ids:\n",
    "                continue\n",
    "            review_to_tokenoccurences[review_id] = 0\n",
    "            for counts in self.get_token_data(review_id=review_id):\n",
    "                review_to_tokenoccurences[review_id] += 1\n",
    "        return review_to_tokenoccurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Main matrix containing all data\n",
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 34459 review IDs\n",
      "Created: Matrix (34459, 607181), 34459 reviews, 607181 tokens\n",
      "Filtering. Based on 189939 review IDs\n",
      "Created: Matrix (189939, 607181), 189939 reviews, 607181 tokens\n",
      "Filtering. Based on 224398 review IDs\n",
      "Created: Matrix (224398, 607181), 224398 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "ids_neg = get_review_ids(years=[2005, 2006], stars=[1,2])\n",
    "ids_pos = get_review_ids(years=[2005, 2006], stars=[4,5])\n",
    "m_neg = matrix.filter_keep_reviews(ids_neg)\n",
    "m_pos = matrix.filter_keep_reviews(ids_pos)\n",
    "m_both = matrix.filter_keep_reviews(ids_neg + ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens which occur predominantly in only one dataset: Get ratio for each token.\n",
    "\n",
    "* **get_ratio**\n",
    "  * In: matrix-A, matrix-B\n",
    "  * In: min_token_occurences (token has to occure at least x times in each of both matrixes)\n",
    "  * Out: tokens sorted by ratio\n",
    "  * Out: tokens sorted by inverted ratio\n",
    "  * Out: tokens not reached minimum\n",
    "  * Out: tokens only in A\n",
    "  * Out: tokens only in B\n",
    "* **filter_tokens_by_ratio**\n",
    "  * In: ratio (generated by method get_ratio)\n",
    "  * In: min_ratio (minimum threshold)\n",
    "  * In: max_tokens (maximum number of tokens to return)\n",
    "  * In: exclude_tokens (tokens not to return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    # Pairs of tokens and occurences\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter/remove rare:\n",
    "        # minimum is set AND\n",
    "        # token has to be in both matrixes AND\n",
    "        # threshold reached in A AND\n",
    "        # threshod reached in B AND\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare[tok_occ[0]] = str(tok_occ[1]) + \" / \" + str(b_token_occurences[tok_occ[0]])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 217041 / 86377\n",
      "Output sizes: ratio 159 ; rare 68141 ; only a 148741 ; only b 18077\n",
      "\n",
      "ratio_a [('excellent', 8.89), ('wonderful', 8.82), ('amazing', 8.47), ('highly', 8.11), ('perfect', 7.38), ('favorite', 7.14), ('season', 6.34), ('collection', 5.64), ('enjoyed', 5.39), ('loved', 5.33), ('heart', 5.1), ('beautiful', 4.94), ('episodes', 4.86), ('great', 4.79), ('features', 4.65), ('classic', 4.62), ('best', 4.61), ('fun', 4.42), ('episode', 4.33), ('definitely', 4.32), ('shows', 4.3), ('enjoy', 4.24), ('love', 4.13), ('family', 4.09), ('performances', 4.05), ('gives', 3.87), ('recommend', 3.82), ('young', 3.52), ('entertaining', 3.46), ('series', 3.41), ('music', 3.39), ('performance', 3.35), ('live', 3.33), ('years', 3.32), ('job', 3.32), ('nice', 3.29), ('john', 3.26), ('set', 3.23), ('true', 3.2), ('finally', 3.2), ('truly', 3.2), ('world', 3.19), ('different', 3.18), ('dvd', 3.16), ('especially', 3.13), ('history', 3.12), ('life', 3.1), ('fans', 3.08), ('cast', 3.06), ('day', 3.02)]\n",
      "\n",
      "ratio_b [('bad', 1.45), ('plot', 0.91), ('acting', 0.79), ('thing', 0.71), ('star', 0.7), ('let', 0.7), ('point', 0.67), ('actually', 0.66), ('believe', 0.65), ('movie', 0.64), ('going', 0.62), ('better', 0.61), ('pretty', 0.61), ('scene', 0.6), ('stars', 0.6), ('actors', 0.6), ('hard', 0.6), ('movies', 0.58), ('want', 0.58), ('thought', 0.58), ('fact', 0.58), ('buy', 0.57), ('original', 0.57), ('scenes', 0.56), ('big', 0.56), ('director', 0.56), ('far', 0.56), ('looking', 0.56), ('said', 0.56), ('kind', 0.56), ('book', 0.55), ('film', 0.54), ('people', 0.54), ('know', 0.54), ('character', 0.54), ('got', 0.54), ('version', 0.54), ('like', 0.53), ('watching', 0.52), ('end', 0.52), ('making', 0.52), ('goes', 0.52), ('think', 0.51), ('characters', 0.51), ('look', 0.51), ('video', 0.51), ('away', 0.51), ('quality', 0.51), ('interesting', 0.5), ('sure', 0.5)]\n",
      "\n",
      "rare [('friends', '9956 / 1288'), ('kids', '9877 / 1462'), ('hope', '9829 / 1440'), ('girl', '9761 / 1837'), ('trying', '9748 / 2767'), ('read', '9724 / 2494'), ('today', '9708 / 889'), ('picture', '9688 / 1644'), ('ending', '9672 / 2161'), ('guy', '9601 / 2773')]\n",
      "\n",
      "only_a [('transfere', 338), ('hatton', 163), ('chrissy', 142), ('1907', 117), ('personable', 112), ('gifford', 111), ('motown', 106), ('mcvay', 106), ('unsung', 105), ('rusesabagina', 95)]\n",
      "\n",
      "only_b [('uncompelling', 16), ('stinko', 11), ('guterman', 8), ('unsuspenseful', 7), ('cters', 7), ('alejate', 7), ('cra', 7), ('beaudine', 7), ('cter', 6), ('stinkbomb', 6)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_pos,\n",
    "    m_neg,\n",
    "    min_token_occurences=10*1000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'wonderful', 'amazing', 'perfect', 'favorite', 'enjoyed', 'loved', 'beautiful', 'great', 'best']\n",
      "['bad']\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 10,\n",
    "['highly', 'season', 'collection', 'heart', 'episodes', 'features', 'classic', 'episode', 'definitely', 'shows', 'family', 'performances', 'gives', 'recommend', 'young', 'series',\n",
    " 'music', 'performance', 'live', 'years', 'job', 'john', 'set', 'true', 'finally', 'truly', 'world', 'different', 'dvd', 'especially', 'history'\n",
    "])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.25, 10, [])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (224398, 11), 224398 reviews, 11 tokens\n",
      "Filtering. Based on 17466 review IDs\n",
      "Created: Matrix (17466, 11), 17466 reviews, 11 tokens\n",
      "Filtering. Based on 90403 review IDs\n",
      "Created: Matrix (90403, 11), 90403 reviews, 11 tokens\n"
     ]
    }
   ],
   "source": [
    "# Specify source data\n",
    "matrix_filtered     = m_both.filter_tokens(filter_tokens_a + filter_tokens_b)\n",
    "matrix_filtered_neg = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[1,2]))\n",
    "matrix_filtered_pos = matrix_filtered.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))\n",
    "del matrix_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "reviews_a = {}\n",
    "for review_item in matrix_filtered_neg.count_token_occurences(get_review_ids(years=[2005], stars=[1,2]), filter_tokens_a).items():\n",
    "    if review_item[1] == 0:\n",
    "        reviews_a[review_item[0]] = review_item[1]\n",
    "print(len(reviews_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68371\n"
     ]
    }
   ],
   "source": [
    "reviews_b = {}\n",
    "for review_item in matrix_filtered_pos.count_token_occurences(get_review_ids(years=[2005], stars=[4,5]), filter_tokens_a).items():\n",
    "    if review_item[1] > 0:\n",
    "        reviews_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "To be able to finally argue that a selection is based on a set of terms, those terms should be exactly defined.  \n",
    "The ratio of word usage is already known here.  \n",
    "Positive words with a high ratio should be used predominantly in positive sets.  \n",
    "Positive words with a ratio higher a little larger than 1 should be omitted, i.e. not included overall.  \n",
    "Positive words with a high ratio should not be used for negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [13953, 31987, 58505, 58531, 74958, 76559, 88261, 88957, 93935]\n",
      "b [10659, 10660, 10663, 10664, 13955, 51663, 54703, 58455, 71825]\n",
      "Tundra-Intensive Blatant \"Jaws\" Ripoff Words can't describe how ludicrous and painful the whole \"Snowbeast\" experience is. Retailing at around $4.00, this is one of the most overpriced DVDs that I have ever seen.  The concept is that a Bigfoot-like monster is haunting a Colorado ski resort immediately prior to the annual \"winter carnival,\" and is shredding the skiers. This movie, made for TV in 1977, is such a blatant \"Jaws\" knockoff that I imagine the only reason the makers of \"Jaws\" didn't sue, was because they were convulsing with laughter. Where to start? Well the music is as good as any other place...it is a wonderful tribute to the \"Jaws\" score; the \"we have to close the beach\" theme from \"Jaws\" transmogrifies into \"we have to close the winter carnival\"; instead of the shark scuttling the boat, the creature rolls a bunch of logs down a hill and knocks a camper out of commission (!); and instead of the lucky shot to the oxygen tank blowing the shark up in a man versus beast finale, there is a ridiculous twist (that made me groan out loud) involving a tree, a ski pole, a hill, and a very silly camera angle from the point of view of the Bigfoot. These aren't all the parallels, but you can have the fun of spotting the rest.  Believe it or not, the screenplay for this mess was written by the same guy who wrote \"Psycho\" for Hitchcock, Joseph Stefano. This is not his best work. The story is ludicrous, and in addition to the standard monster stuff, includes a love triangle subplot, a greed subplot, and much, much more.  All this brings me to the monster himself. This is, simply put, the lamest monster I have seen since Phil Tucker's \"Robot Monster.\" Normally we can only see one paw of the creature reaching around things at us (Scary!), and there is one shot where you can see him grabbing a guy and you can clearly see where the monster glove stops and the human arm begins. Occasionally you also get to see a close up shot of the monster's mask, but the only time you ever get to see the monster as an entire entity is from very far away, when he is indistinguishable from, say, a spider monkey, or even Napoleon Bonaparte. The point here isn't to make fun of spider monkeys or the French; it is to say that this film is a piece of cheese, and rank, runny, smelly cheese at that. A particular effect you will grow to know and loathe is after each killing (since we only get to see the monster paw and the victim in frame at once) the paw turns red in the one not-so-special effect in the movie. (This signals a commercial break.)  In summary, this movie would have been perfect fodder for MST3K, but watching it as is makes for a painful night of entertainment. This movie is for hardened bad movie buffs only. Perhaps the box for the DVD sums it up best when it simply says \"Big Foot - Loch Ness Monster - Abominable Snowman - Snowbeast!\" Right.  Boo.\n"
     ]
    }
   ],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if True:\n",
    "    print('a', list(reviews_a.keys())[0:9])\n",
    "    print('b', list(reviews_b)[0:9])\n",
    "    print(get_text(589196))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print('1', m.get_token_data(revs[1]))\n",
    "    print('6', m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print('0', m.get_token_data(revs[0]))\n",
    "    print('4', m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "# print_matrix_info = None\n",
    "# get_test_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
