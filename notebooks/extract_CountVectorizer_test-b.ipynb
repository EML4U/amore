{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 are not included in Doc2Vec embeddings. (2000 to 2012 included.)\n",
    "- Ideas:\n",
    "    - 100/0 to 0/100 neg/pos\n",
    "    - 50/50 to 40/60 neg/pos\n",
    "    - build on results on that: other distributions, e.g. 45/55\n",
    "    - 50/50 to 40/30/30 neg/posCluster1/posCluster2\n",
    "    - for token-level and document-level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 288.19129145399984\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 36.8698407349998\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print(self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could be improved by using matrix instead of dict.\n",
    "        \n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could maybe be improved by using matrix instead of dict.\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1\n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[1]))\n",
    "    print(m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "# print_matrix_info = None\n",
    "# get_test_matrix = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Matrix (17466, 607181), 17466 reviews, 607181 tokens\n",
      "Filtering. Based on 90403 review IDs\n",
      "Matrix (90403, 607181), 90403 reviews, 607181 tokens\n",
      "Filtering. Based on 16993 review IDs\n",
      "Matrix (16993, 607181), 16993 reviews, 607181 tokens\n",
      "Filtering. Based on 99536 review IDs\n",
      "Matrix (99536, 607181), 99536 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "m_2000_neg = matrix.filter_keep_reviews(get_review_ids(years=[2005], stars=[1,2]))\n",
    "m_2000_pos = matrix.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))\n",
    "m_2001_neg = matrix.filter_keep_reviews(get_review_ids(years=[2006], stars=[1,2]))\n",
    "m_2001_pos = matrix.filter_keep_reviews(get_review_ids(years=[2006], stars=[4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter rare\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            #rare.append(tok_occ[0])\n",
    "            rare[tok_occ[0]] = tok_occ[1] + b_token_occurences[tok_occ[0]]\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 153373 / 63005\n",
      "Output sizes: ratio 228 ; rare 50075 ; only a 103070 ; only b 12702\n",
      "\n",
      "ratio_a [('excellent', 8.86), ('awesome', 8.71), ('wonderful', 8.41), ('amazing', 8.15), ('highly', 7.59), ('perfect', 7.34), ('favorite', 7.01), ('brilliant', 6.49), ('season', 5.89), ('collection', 5.38), ('beautiful', 5.22), ('enjoyed', 5.13), ('heart', 4.96), ('extras', 4.95), ('loved', 4.85), ('great', 4.62), ('episodes', 4.58), ('features', 4.57), ('best', 4.54), ('classic', 4.52), ('today', 4.49), ('episode', 4.38), ('definitely', 4.27), ('performances', 4.26), ('song', 4.24), ('family', 4.22), ('works', 4.22), ('songs', 4.19), ('shows', 4.17), ('fun', 4.08), ('love', 4.06), ('enjoy', 4.05), ('gives', 3.97), ('including', 3.85), ('drama', 3.68), ('young', 3.66), ('lives', 3.64), ('recommend', 3.58), ('father', 3.46), ('days', 3.44), ('early', 3.43), ('years', 3.36), ('job', 3.34), ('finally', 3.33), ('entertaining', 3.32), ('music', 3.3), ('performance', 3.28), ('human', 3.27), ('nice', 3.26), ('world', 3.24)]\n",
      "\n",
      "ratio_b [('bad', 1.45), ('money', 1.43), ('minutes', 1.03), ('plot', 0.9), ('horror', 0.83), ('maybe', 0.82), ('acting', 0.79), ('reason', 0.77), ('thing', 0.74), ('trying', 0.72), ('looks', 0.72), ('guy', 0.7), ('let', 0.69), ('effects', 0.69), ('star', 0.68), ('actually', 0.67), ('read', 0.67), ('believe', 0.66), ('sense', 0.66), ('point', 0.65), ('left', 0.65), ('movie', 0.64), ('going', 0.63), ('rest', 0.63), ('better', 0.62), ('book', 0.62), ('fact', 0.61), ('hard', 0.61), ('want', 0.6), ('actors', 0.6), ('pretty', 0.6), ('scene', 0.59), ('thought', 0.59), ('looking', 0.59), ('review', 0.59), ('tell', 0.59), ('stars', 0.58), ('director', 0.58), ('said', 0.58), ('line', 0.58), ('buy', 0.57), ('big', 0.57), ('far', 0.57), ('kind', 0.57), ('movies', 0.56), ('got', 0.56), ('original', 0.56), ('version', 0.56), ('entire', 0.56), ('simply', 0.55)]\n",
      "\n",
      "rare [('instead', 5790), ('past', 4558), ('mother', 4518), ('reviews', 5075), ('try', 5034), ('fantastic', 4109), ('soon', 4410), ('coming', 4425), ('son', 4374), ('bought', 4725)]\n",
      "\n",
      "only_a [('emmys', 112), ('dreamer', 102), ('transfere', 100), ('withers', 88), ('byrd', 86), ('gifford', 84), ('excelent', 84), ('hatton', 82), ('greys', 80), ('rusesabagina', 75)]\n",
      "\n",
      "only_b [('snoozefest', 12), ('djmon', 12), ('dumberer', 9), ('yawned', 8), ('guterman', 7), ('cra', 6), ('alejate', 6), ('cters', 6), ('timbo', 6), ('stinko', 6)]\n"
     ]
    }
   ],
   "source": [
    "matrix_a = m_2000_pos\n",
    "matrix_b = m_2000_neg\n",
    "min_token_occurences = 4000\n",
    "\n",
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(matrix_a, matrix_b, min_token_occurences=min_token_occurences)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'awesome', 'wonderful', 'amazing', 'perfect', 'favorite', 'brilliant', 'beautiful', 'enjoyed', 'heart', 'loved', 'great', 'best']\n",
      "['bad']\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 2, 13, ['highly', 'season', 'collection', 'extras', 'episodes', 'features', 'classic', 'today'])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.1, 10, ['money'])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make lists of words (Atok Btok) to be used for filtering for both distributions\n",
    "- add review to list e.g. if at least 20% or 5 tokens of Atok are included and less than 10% or 2 tokens of Btok are included.\n",
    "\n",
    "TODO:\n",
    "\n",
    "-  Sum up occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 14\n",
      "<class 'dict'> 14\n",
      "Matrix (1584098, 14), 1584098 reviews, 14 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'beautiful',\n",
       " 'best',\n",
       " 'brilliant',\n",
       " 'enjoyed',\n",
       " 'excellent',\n",
       " 'favorite',\n",
       " 'great',\n",
       " 'heart',\n",
       " 'loved',\n",
       " 'perfect',\n",
       " 'wonderful']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = matrix.filter_tokens(filter_tokens_a + filter_tokens_b)\n",
    "tmp.token_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 90403 review IDs\n",
      "Matrix (90403, 14), 90403 reviews, 14 tokens\n"
     ]
    }
   ],
   "source": [
    "tmp2 = tmp.filter_keep_reviews(get_review_ids(years=[2005], stars=[4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # in: tokens\n",
    "# out: revId to counts of all tokens\n",
    "# get token indices\n",
    "# iterate nonzero[0] and filter by if token_index searched\n",
    "# check nonzero[1] which is revIndex and create revIndex to number of token found.\n",
    "\n",
    "reviews_token_occurences = tmp2.filter_reviews_by_tokens(filter_tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6315, 1)\n",
      "(6589, 2)\n"
     ]
    }
   ],
   "source": [
    "it = iter(reviews_token_occurences.items())\n",
    "print(next(it))\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'awesome', 'wonderful', 'amazing', 'perfect', 'favorite', 'brilliant', 'beautiful', 'enjoyed', 'heart', 'loved', 'great', 'best']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cafe Society This DVD shows Bobby Short at his best.  He is really the last of the great \"cafe society\" entertainers.  The songs selected are terrific and the fact that his audience is shown around the piano makes the whole event quite wonderful.  You have the feeling that you are actually in attendance.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filter_tokens_a)\n",
    "get_text(6589)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = m_2000_pos.filter_reviews_by_tokens(filter_tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix (8, 607181), 8 reviews, 607181 tokens\n",
      "Pretty pointless fictionalization The murders in Juarez are real. This movie is a badly acted fantasy of revenge and holy intercession.  If there is a good movie about Juarez, I don't know what it is, but it is not this one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'movie': 2,\n",
       " 'juarez': 2,\n",
       " 'real': 1,\n",
       " 'murders': 1,\n",
       " 'pretty': 1,\n",
       " 'pointless': 1,\n",
       " 'fictionalization': 1,\n",
       " 'badly': 1,\n",
       " 'acted': 1,\n",
       " 'fantasy': 1,\n",
       " 'revenge': 1,\n",
       " 'holy': 1,\n",
       " 'intercession': 1,\n",
       " 'good': 1,\n",
       " 'know': 1}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = get_test_matrix()\n",
    "revs = list(m.reviewids_to_reviewindices.keys())\n",
    "print (get_text(revs[1]))\n",
    "m.get_token_counts(revs[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: debug get_token_counts() to generate overview for test of filter_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: tokens\n",
    "# out: revId to counts of all tokens\n",
    "# get token indices\n",
    "# iterate nonzero[0] and filter by if token_index searched\n",
    "# check nonzero[1] which is revIndex and create revIndex to number of token found.\n",
    "m.doc_term_matrix.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
