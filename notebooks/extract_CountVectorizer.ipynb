{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction: CountVectorizer\n",
    "\n",
    "- Reads `deduplicated.pickle.bz2`, data format: `{year {star [(number, year, star)]}}`\n",
    "- Uses IDs to read review texts, starting from year 2007 and filtered by [1,2] and [4,5] star ratings\n",
    "- Applies CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.printer import Printer\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "printer = Printer()\n",
    "\n",
    "# Count items in Year-Star-lists\n",
    "def count_ysl(ysl):\n",
    "    c = 0\n",
    "    for year in ysl.keys():\n",
    "        for star in ysl[year].keys():\n",
    "            c += len(ysl[year][star])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deduplicated Year/star/review-IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: /home/eml4u/EML4U/notebooks/amore/data/benchmark/deduplicated.pickle.bz2\n",
      "size: 1727821\n",
      "first item: [16505, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "filepath = file_storage.get_filepath('deduplicated')\n",
    "print('File path:', filepath)\n",
    "file_duplicates = filepath\n",
    "with bz2.BZ2File(file_duplicates, 'r') as file:\n",
    "    dup_ids = pickle.loads(file.read())\n",
    "\n",
    "# Print overview\n",
    "print_year_star_sum = False\n",
    "count = 0\n",
    "first = None\n",
    "for year in dup_ids:\n",
    "    for star in dup_ids[year]:\n",
    "        size = len(dup_ids[year][star])\n",
    "        if print_year_star_sum:\n",
    "            print(year, star, size)\n",
    "        count += size\n",
    "        if first is None:\n",
    "            first = dup_ids[year][star][0]\n",
    "            \n",
    "print('size: ' + str(count)) # size: 1727821\n",
    "print('first item:', first)  # first item: [16505, 2007, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>597</td>\n",
       "      <td>2512</td>\n",
       "      <td>3015</td>\n",
       "      <td>3597</td>\n",
       "      <td>3689</td>\n",
       "      <td>6643</td>\n",
       "      <td>10413</td>\n",
       "      <td>9943</td>\n",
       "      <td>11125</td>\n",
       "      <td>12661</td>\n",
       "      <td>14150</td>\n",
       "      <td>15822</td>\n",
       "      <td>19132</td>\n",
       "      <td>21570</td>\n",
       "      <td>134897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>437</td>\n",
       "      <td>2162</td>\n",
       "      <td>2541</td>\n",
       "      <td>3048</td>\n",
       "      <td>3364</td>\n",
       "      <td>4880</td>\n",
       "      <td>7053</td>\n",
       "      <td>7050</td>\n",
       "      <td>8067</td>\n",
       "      <td>8417</td>\n",
       "      <td>8846</td>\n",
       "      <td>9536</td>\n",
       "      <td>11363</td>\n",
       "      <td>12041</td>\n",
       "      <td>88835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>880</td>\n",
       "      <td>3932</td>\n",
       "      <td>4562</td>\n",
       "      <td>5064</td>\n",
       "      <td>5860</td>\n",
       "      <td>8592</td>\n",
       "      <td>11420</td>\n",
       "      <td>11322</td>\n",
       "      <td>13932</td>\n",
       "      <td>13944</td>\n",
       "      <td>14835</td>\n",
       "      <td>14925</td>\n",
       "      <td>16796</td>\n",
       "      <td>17593</td>\n",
       "      <td>143723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>146</td>\n",
       "      <td>2166</td>\n",
       "      <td>9832</td>\n",
       "      <td>11216</td>\n",
       "      <td>12257</td>\n",
       "      <td>13466</td>\n",
       "      <td>19364</td>\n",
       "      <td>25958</td>\n",
       "      <td>27917</td>\n",
       "      <td>37664</td>\n",
       "      <td>36838</td>\n",
       "      <td>37089</td>\n",
       "      <td>36408</td>\n",
       "      <td>40392</td>\n",
       "      <td>40528</td>\n",
       "      <td>351245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>561</td>\n",
       "      <td>7266</td>\n",
       "      <td>25204</td>\n",
       "      <td>26294</td>\n",
       "      <td>29576</td>\n",
       "      <td>32416</td>\n",
       "      <td>46222</td>\n",
       "      <td>64445</td>\n",
       "      <td>71619</td>\n",
       "      <td>108952</td>\n",
       "      <td>104455</td>\n",
       "      <td>112998</td>\n",
       "      <td>113957</td>\n",
       "      <td>130571</td>\n",
       "      <td>134571</td>\n",
       "      <td>1009121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>21</td>\n",
       "      <td>828</td>\n",
       "      <td>11346</td>\n",
       "      <td>43642</td>\n",
       "      <td>47628</td>\n",
       "      <td>53542</td>\n",
       "      <td>58795</td>\n",
       "      <td>85701</td>\n",
       "      <td>119289</td>\n",
       "      <td>127851</td>\n",
       "      <td>179740</td>\n",
       "      <td>176315</td>\n",
       "      <td>187918</td>\n",
       "      <td>190648</td>\n",
       "      <td>218254</td>\n",
       "      <td>226303</td>\n",
       "      <td>1727821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1997 1998   1999   2000   2001   2002   2003   2004    2005    2006  \\\n",
       "1      2   26    597   2512   3015   3597   3689   6643   10413    9943   \n",
       "2    NaN   30    437   2162   2541   3048   3364   4880    7053    7050   \n",
       "3      1   65    880   3932   4562   5064   5860   8592   11420   11322   \n",
       "4      4  146   2166   9832  11216  12257  13466  19364   25958   27917   \n",
       "5     14  561   7266  25204  26294  29576  32416  46222   64445   71619   \n",
       "Sum   21  828  11346  43642  47628  53542  58795  85701  119289  127851   \n",
       "\n",
       "       2007    2008    2009    2010    2011    2012        Sum  \n",
       "1     11125   12661   14150   15822   19132   21570   134897.0  \n",
       "2      8067    8417    8846    9536   11363   12041    88835.0  \n",
       "3     13932   13944   14835   14925   16796   17593   143723.0  \n",
       "4     37664   36838   37089   36408   40392   40528   351245.0  \n",
       "5    108952  104455  112998  113957  130571  134571  1009121.0  \n",
       "Sum  179740  176315  187918  190648  218254  226303  1727821.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews in dup_ids: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Print duplicate IDs as table\n",
    "if True:\n",
    "    printer.ipython_display(printer.get_dataframe_with_sums(dup_ids))\n",
    "if False:\n",
    "    print(printer.get_dataframe_markdown(printer.get_dataframe_with_sums(dup_ids), float_as_integer=True, tablefmt=\"pipe\"))\n",
    "print('Reviews in dup_ids:', count_ysl(dup_ids))\n",
    "# Reviews in ys_lists: 1,727,821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and filter texts\n",
    "\n",
    "- Texts **starting from year 2007** (the years 1997 to 2006 are not included in available doc2vec embeddings\n",
    "- Only **1 & 2 star** and **4 & 5 star** reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and collect review numbers/IDs\n",
    "review_numbers = set()\n",
    "for year in dup_ids.keys():\n",
    "    if(year < 2006):\n",
    "        continue\n",
    "    for star in dup_ids[year].keys():\n",
    "        if(star == 3):\n",
    "            continue\n",
    "        for tup in dup_ids[year][star]:\n",
    "            review_numbers.add(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postitive and negative reviews: 1203682\n",
      "Example review ID: 2097152\n"
     ]
    }
   ],
   "source": [
    "print('Postitive and negative reviews:', len(review_numbers)) # Postitive and negative reviews: 1203682\n",
    "print('Example review ID:', next(iter(review_numbers))) # Example review ID: 2097152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  256.4706284236163\n"
     ]
    }
   ],
   "source": [
    "# Collect review texts\n",
    "start_time = timeit.default_timer()\n",
    "revno_to_text = {}\n",
    "\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, max_docs=-1)\n",
    "for item in reader:\n",
    "    if item[AmazonReviewsReader.KEY_NUMBER] in review_numbers:\n",
    "        revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "\n",
    "print('Runtime:', timeit.default_timer() - start_time)  # Runtime: 257.3623419497162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of review texts: 1203682\n",
      "Example: (3, \"This movie needed to be made. The scenes in this film can be very disquieting due to their graphic re-enactment of real events, but this story needs to be told. I will say the violence was injected into the movie with as much taste as manageable when dealing with rape scenes, etc. Inspired by true events, women are being murdered in Juarez after they leave the factory where they work. A fearful community is suddenly given some hope when one of the young victims not only lives, but experiences 'stigmata' after seeing the Virgin Mary.  I was shocked to learn that murders in Juarez are still happening and many are unsolved. I believe this director brought a very important story to the surface. Though it's never pleasant to think about young women being murdered, this movie depicts a harsh reality of the high cost of exploited-cheap labor.  Chrissy K. McVay - Author\")\n"
     ]
    }
   ],
   "source": [
    "print('Size of review texts:', len(revno_to_text)) # Size of review texts: 1203682\n",
    "print('Example:', next(iter(revno_to_text.items()))) # Example: (3, \"This movie needed to [...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_all 412\n"
     ]
    }
   ],
   "source": [
    "tmp_stopwords_file = InterimStorage('stopwords')\n",
    "\n",
    "if not tmp_stopwords_file.isfile():\n",
    "    from gensim.parsing.preprocessing import STOPWORDS as stopwords_gensim\n",
    "    print('stopwords_gensim', len(stopwords_gensim))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    #import nltk\n",
    "    #nltk.download('stopwords')\n",
    "    stopwords_nltk = set(stopwords.words('english'))\n",
    "    print('stopwords_nltk', len(stopwords_nltk))\n",
    "\n",
    "    from sklearn.feature_extraction import _stop_words\n",
    "    stopwords_sklearn = _stop_words.ENGLISH_STOP_WORDS\n",
    "    print('stopwords_sklearn', len(stopwords_sklearn))\n",
    "\n",
    "    import spacy\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    stopwords_spacy = spacy.load(\"en_core_web_sm\").Defaults.stop_words\n",
    "    print('stopwords_spacy', len(stopwords_spacy))\n",
    "\n",
    "    stopwords_all = stopwords_nltk.union(stopwords_sklearn).union(stopwords_spacy).union(stopwords_gensim)\n",
    "    print(tmp_stopwords_file.write(stopwords_all).get_filepath())\n",
    "    \n",
    "    # stopwords_gensim 337\n",
    "    # stopwords_nltk 179\n",
    "    # stopwords_sklearn 318\n",
    "    # stopwords_spacy 326\n",
    "    # stopwords_all 412\n",
    "\n",
    "else:\n",
    "    stopwords_all = tmp_stopwords_file.read()\n",
    "\n",
    "print('stopwords_all', len(stopwords_all))\n",
    "# stopwords_all 412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all collected stopwords\n",
    "if False:\n",
    "    print(stopwords_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Alternatives: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 | packaged by conda-forge | (default, Sep 24 2020, 16:55:52) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "# \"changed in Python 3.6. The built-in dict class now keeps its items ordered as well.\"\n",
    "# https://realpython.com/python-ordereddict/\n",
    "print(sys.version)  # 3.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath vectorizer ID to review no: /tmp/InterimStorage/CountVectorizer-VecidRevno.pickle.bz2\n",
      "Filepath document-term matrix:       /tmp/InterimStorage/CountVectorizer-DocTermMatrix.pickle.bz2\n",
      "Filepath vocabulary:                 /tmp/InterimStorage/CountVectorizer-Vocabulary.pickle.bz2\n",
      "Filepath vectorizer:                 /tmp/InterimStorage/CountVectorizer-object.pickle.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 918065\n",
      "Number of stop words: 412\n",
      "Runtime:  3446.189210616052\n",
      "vectorizer ID to review no: 1203682 <class 'dict'>\n",
      "document-term matrix:       (1203682, 918065) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "vocabulary:                 10 <class 'dict'>\n",
      "vectorizer:                 <class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#\n",
    "# Installation:    \n",
    "#  import nltk\n",
    "#  nltk.download('punkt')\n",
    "\n",
    "tmp_vecid_revno_file      = InterimStorage('CountVectorizer-VecidRevno')\n",
    "tmp_docterm_matrix_file   = InterimStorage('CountVectorizer-DocTermMatrix')  # formerly 'countvec-object'\n",
    "tmp_vocabulary_file       = InterimStorage('CountVectorizer-Vocabulary')     # formerly 'countvec-vectorizer'\n",
    "tmp_count_vectorizer_file = InterimStorage('CountVectorizer-Object')\n",
    "max_features = None # 1000\n",
    "\n",
    "#if not tmp_docterm_matrix_file.isfile():\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # The documents in doc_term_matrix will be numbered starting from 0\n",
    "    vecid_revno = {}\n",
    "    corpus = []\n",
    "    for i, item in enumerate(revno_to_text.items()):\n",
    "        vecid_revno[i] = item[0]\n",
    "        corpus.append(item[1])\n",
    "        if(False and i>10):\n",
    "            break\n",
    "    \n",
    "    remove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)\n",
    "    \n",
    "    def filter_tokens(tokens):\n",
    "        return [item for item in tokens if len(item)>=3]\n",
    "\n",
    "    def normalize(text):\n",
    "        return filter_tokens(word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "    stop_words = stopwords_all  # alternative: \"english\"\n",
    "    vectorizer = CountVectorizer(tokenizer=normalize, stop_words=stop_words, max_features=max_features)\n",
    "    doc_term_matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    print('Filepath vectorizer ID to review no:', tmp_vecid_revno_file.write(vecid_revno).get_filepath())\n",
    "    print('Filepath document-term matrix:      ', tmp_docterm_matrix_file.write(doc_term_matrix).get_filepath())\n",
    "    print('Filepath vocabulary:                ', tmp_vocabulary_file.write(vectorizer.vocabulary_).get_filepath())\n",
    "    print('Filepath vectorizer:                ', tmp_count_vectorizer_file.write(vectorizer).get_filepath())\n",
    "    \n",
    "    #print('Feature names:', vectorizer.get_feature_names())\n",
    "    print('Number of features:', len(vectorizer.get_feature_names()))\n",
    "    print('Number of stop words:', len(vectorizer.get_stop_words()))\n",
    "    \n",
    "    print('Runtime: ', timeit.default_timer() - start_time)\n",
    "#else:\n",
    "#    vecid_revno = tmp_vecid_revno_file.read()\n",
    "#    doc_term_matrix = tmp_docterm_matrix_file.read()\n",
    "#    vocabulary = tmp_vocabulary_file.read()\n",
    "#    vectorizer = tmp_count_vectorizer_file.read()\n",
    "#    print('vocabulary:                ', len(vocabulary), type(vocabulary))\n",
    "    \n",
    "print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "print('document-term matrix:      ', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "print('vectorizer:                ', type(vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Filepath vectorizer ID to review no: /tmp/InterimStorage/CountVectorizer-VecidRevno.pickle.bz2\n",
    "Filepath document-term matrix:       /tmp/InterimStorage/CountVectorizer-DocTermMatrix.pickle.bz2\n",
    "Filepath vocabulary:                 /tmp/InterimStorage/CountVectorizer-Vocabulary.pickle.bz2\n",
    "Filepath vectorizer:                 /tmp/InterimStorage/CountVectorizer-object.pickle.bz2\n",
    "\n",
    "IOPub data rate exceeded.\n",
    "\n",
    "Current values:\n",
    "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "NotebookApp.rate_limit_window=3.0 (secs)\n",
    "\n",
    "Number of features: 918065\n",
    "Number of stop words: 412\n",
    "Runtime:  3446.189210616052\n",
    "vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "document-term matrix:       (1203682, 918065) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "vocabulary:                 10 <class 'dict'>\n",
    "vectorizer:                 <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Run with previous implementation, comments added:\n",
    "https://github.com/EML4U/amore/blob/27919abc6f7f6c7fc8cfc39a2dde780dc9a44d32/notebooks/_filtering-CountVectorizer.ipynb\n",
    "\n",
    "Filepath document-term matrix: /tmp/InterimStorage/countvec-object.pickle.bz2\n",
    "Filepath vocabulary: /tmp/InterimStorage/countvec-vectorizer.pickle.bz2\n",
    "Feature names: ['70', '80', 'abil', 'abl', 'abov', 'absolut', 'accept', 'act', 'action', 'actor', 'actress', 'actual', 'ad', 'adapt', 'add', 'addit', 'admit', 'adult', 'advanc', 'adventur', 'age', 'ago', 'agre', 'air', 'album', 'alien', 'allow', 'alon', 'alreadi', 'alway', 'amaz', 'amazon', 'america', 'american', 'angel', 'ani', 'anim', 'annoy', 'anoth', 'answer', 'anyon', 'anyth', 'apart', 'appar', 'appeal', 'appear', 'appreci', 'approach', 'area', 'arent', 'arm', 'arriv', 'art', 'artist', 'ask', 'aspect', 'attack', 'attempt', 'attent', 'attract', 'audienc', 'audio', 'author', 'avail', 'averag', 'avoid', 'aw', 'award', 'away', 'awesom', 'babi', 'background', 'bad', 'balanc', 'band', 'base', 'basic', 'battl', 'beat', 'beauti', 'becam', 'becaus', 'becom', 'befor', 'begin', 'beginn', 'believ', 'best', 'better', 'big', 'bit', 'black', 'blood', 'blu', 'blue', 'bluray', 'bob', 'bodi', 'bond', 'bonu', 'book', 'bore', 'bought', 'box', 'boy', 'break', 'brilliant', 'bring', 'british', 'brother', 'brought', 'budget', 'build', 'burn', 'busi', 'buy', 'came', 'camera', 'captur', 'car', 'care', 'career', 'carri', 'cartoon', 'case', 'cast', 'catch', 'caught', 'caus', 'center', 'centuri', 'certain', 'certainli', 'challeng', 'chanc', 'chang', 'channel', 'charact', 'charm', 'chase', 'cheap', 'check', 'child', 'children', 'choic', 'choos', 'christian', 'christma', 'cinema', 'cinematographi', 'citi', 'class', 'classic', 'clean', 'clear', 'clearli', 'clever', 'close', 'collect', 'colleg', 'color', 'combin', 'come', 'comedi', 'comic', 'comment', 'commentari', 'commun', 'compani', 'compar', 'compel', 'complet', 'complex', 'comput', 'concept', 'concern', 'concert', 'condit', 'confus', 'connect', 'consid', 'contain', 'content', 'continu', 'control', 'convinc', 'cool', 'cop', 'copi', 'costum', 'count', 'countri', 'coupl', 'cours', 'cover', 'crazi', 'creat', 'creativ', 'credit', 'crew', 'cri', 'crime', 'critic', 'cultur', 'current', 'cut', 'cute', 'danc', 'danger', 'dark', 'date', 'daughter', 'david', 'day', 'dead', 'deal', 'death', 'decad', 'decent', 'decid', 'deep', 'definit', 'delight', 'deliv', 'depict', 'depth', 'describ', 'deserv', 'design', 'despit', 'develop', 'dialogu', 'didnt', 'die', 'differ', 'difficult', 'digit', 'direct', 'director', 'disappoint', 'disc', 'discov', 'discuss', 'disk', 'disney', 'doctor', 'documentari', 'doe', 'doesnt', 'dog', 'dont', 'doubt', 'drama', 'dramat', 'dream', 'drive', 'drug', 'dure', 'dvd', 'earli', 'earlier', 'earth', 'easi', 'easili', 'eat', 'edg', 'edit', 'educ', 'effect', 'effort', 'element', 'els', 'emot', 'end', 'engag', 'english', 'enjoy', 'entertain', 'entir', 'epic', 'episod', 'equal', 'era', 'escap', 'especi', 'event', 'eventu', 'everi', 'everyon', 'everyth', 'evil', 'exactli', 'exampl', 'excel', 'excit', 'execut', 'exercis', 'exist', 'expect', 'experi', 'explain', 'explor', 'express', 'extra', 'extrem', 'eye', 'face', 'fact', 'fail', 'fairli', 'faith', 'fall', 'famili', 'familiar', 'famou', 'fan', 'fantasi', 'fantast', 'far', 'fascin', 'fast', 'father', 'favorit', 'fear', 'featur', 'feel', 'felt', 'femal', 'fiction', 'fight', 'figur', 'film', 'filmmak', 'final', 'fine', 'finish', 'fit', 'flaw', 'fli', 'flick', 'focu', 'focus', 'folk', 'follow', 'food', 'footag', 'forc', 'forget', 'form', 'format', 'forward', 'frank', 'free', 'french', 'friend', 'fun', 'funni', 'futur', 'game', 'gave', 'gay', 'gem', 'gener', 'genr', 'georg', 'german', 'ghost', 'gift', 'girl', 'given', 'glad', 'god', 'goe', 'gone', 'good', 'gore', 'got', 'govern', 'graphic', 'great', 'greatest', 'green', 'group', 'grow', 'guess', 'guitar', 'gun', 'guy', 'ha', 'half', 'hand', 'happen', 'happi', 'hard', 'harri', 'hate', 'havent', 'head', 'hear', 'heard', 'heart', 'hell', 'help', 'hero', 'hi', 'high', 'highli', 'hilari', 'histor', 'histori', 'hit', 'hold', 'hollywood', 'home', 'hook', 'hope', 'horribl', 'horror', 'hot', 'hour', 'hous', 'howev', 'huge', 'human', 'humor', 'husband', 'idea', 'ill', 'imag', 'imagin', 'immedi', 'import', 'impress', 'improv', 'includ', 'incred', 'inde', 'individu', 'inform', 'insid', 'insight', 'inspir', 'instead', 'instruct', 'intellig', 'intens', 'interview', 'intrigu', 'introduc', 'involv', 'island', 'isnt', 'issu', 'item', 'ive', 'jack', 'jame', 'jane', 'japanes', 'job', 'joe', 'john', 'johnni', 'joke', 'jone', 'journey', 'joy', 'jump', 'justic', 'kept', 'key', 'kick', 'kid', 'kill', 'killer', 'kind', 'king', 'knew', 'know', 'known', 'lack', 'ladi', 'land', 'languag', 'larg', 'late', 'later', 'laugh', 'law', 'lead', 'learn', 'leav', 'lee', 'left', 'lesson', 'let', 'level', 'librari', 'lie', 'life', 'light', 'like', 'limit', 'line', 'list', 'listen', 'liter', 'littl', 'live', 'local', 'locat', 'long', 'longer', 'look', 'lose', 'lost', 'lot', 'love', 'lover', 'low', 'magic', 'main', 'major', 'man', 'manag', 'mani', 'manner', 'mari', 'mark', 'marri', 'martin', 'master', 'masterpiec', 'match', 'materi', 'matter', 'mayb', 'mean', 'meet', 'member', 'memor', 'memori', 'men', 'mention', 'messag', 'michael', 'middl', 'mind', 'minor', 'minut', 'miss', 'mix', 'modern', 'mom', 'moment', 'money', 'monster', 'month', 'moral', 'mostli', 'mother', 'motiv', 'movement', 'movi', 'murder', 'music', 'mysteri', 'narrat', 'nation', 'natur', 'near', 'nearli', 'need', 'neg', 'new', 'nice', 'night', 'normal', 'note', 'noth', 'notic', 'novel', 'number', 'obviou', 'obvious', 'odd', 'offer', 'offic', 'okay', 'old', 'older', 'onc', 'onli', 'open', 'opera', 'opinion', 'order', 'origin', 'oscar', 'otherwis', 'outsid', 'outstand', 'overal', 'pace', 'pack', 'packag', 'pain', 'parent', 'parti', 'particular', 'particularli', 'pass', 'passion', 'past', 'paul', 'pay', 'peopl', 'perfect', 'perfectli', 'perform', 'perhap', 'period', 'person', 'peter', 'physic', 'pick', 'pictur', 'piec', 'place', 'plan', 'planet', 'play', 'player', 'pleas', 'plenti', 'plot', 'plu', 'point', 'polic', 'polit', 'poor', 'pop', 'popular', 'portray', 'posit', 'possibl', 'power', 'practic', 'predict', 'prefer', 'present', 'pretti', 'previou', 'price', 'print', 'prison', 'probabl', 'problem', 'process', 'produc', 'product', 'program', 'project', 'promis', 'prove', 'provid', 'public', 'pull', 'purchas', 'pure', 'qualiti', 'queen', 'question', 'quick', 'quickli', 'quit', 'race', 'rais', 'rare', 'rate', 'ray', 'reach', 'read', 'real', 'realist', 'realiti', 'realiz', 'realli', 'reason', 'receiv', 'recent', 'recommend', 'record', 'red', 'refer', 'regard', 'relat', 'relationship', 'releas', 'remain', 'remak', 'remark', 'rememb', 'remind', 'rent', 'repeat', 'replac', 'requir', 'respect', 'respons', 'rest', 'restor', 'result', 'return', 'reveal', 'review', 'rich', 'richard', 'ride', 'ridicul', 'right', 'road', 'robert', 'rock', 'role', 'roll', 'romanc', 'romant', 'room', 'routin', 'run', 'sad', 'said', 'sam', 'satisfi', 'save', 'saw', 'scari', 'scene', 'sceneri', 'school', 'scienc', 'scifi', 'score', 'scott', 'screen', 'script', 'search', 'season', 'second', 'secret', 'section', 'seen', 'segment', 'select', 'sell', 'seller', 'sens', 'sequel', 'sequenc', 'seri', 'seriou', 'serv', 'servic', 'set', 'sever', 'sex', 'sexual', 'shame', 'shape', 'share', 'ship', 'shock', 'shoot', 'short', 'shot', 'shown', 'sign', 'silli', 'similar', 'simpl', 'simpli', 'sinc', 'sing', 'singer', 'singl', 'sister', 'sit', 'situat', 'skill', 'skip', 'slow', 'small', 'smart', 'smith', 'social', 'societi', 'soldier', 'solid', 'someon', 'someth', 'sometim', 'somewhat', 'son', 'song', 'soon', 'sorri', 'sort', 'soul', 'sound', 'soundtrack', 'space', 'speak', 'special', 'spend', 'spent', 'spirit', 'spot', 'stage', 'stand', 'standard', 'star', 'start', 'state', 'stay', 'step', 'steve', 'stone', 'stop', 'store', 'stori', 'storylin', 'straight', 'strang', 'street', 'strength', 'stretch', 'strong', 'struggl', 'student', 'studi', 'studio', 'stuff', 'stun', 'stupid', 'style', 'subject', 'subtitl', 'success', 'suffer', 'suggest', 'super', 'superb', 'superman', 'support', 'suppos', 'sure', 'surpris', 'surround', 'surviv', 'suspens', 'sweet', 'taken', 'tale', 'talent', 'talk', 'tape', 'teach', 'teacher', 'team', 'tear', 'techniqu', 'teen', 'teenag', 'televis', 'tell', 'term', 'terribl', 'terrif', 'thank', 'theater', 'theme', 'themselv', 'theyr', 'thi', 'thing', 'think', 'thought', 'thrill', 'thriller', 'throw', 'time', 'tire', 'titl', 'today', 'togeth', 'told', 'tom', 'tone', 'took', 'total', 'touch', 'tough', 'tour', 'town', 'track', 'tradit', 'trailer', 'train', 'transfer', 'travel', 'treat', 'tri', 'trip', 'troubl', 'true', 'truli', 'truth', 'turn', 'twice', 'twist', 'type', 'typic', 'ultim', 'understand', 'unfortun', 'uniqu', 'unit', 'univers', 'unlik', 'use', 'usual', 'valu', 'vampir', 'variou', 'veri', 'version', 'vh', 'video', 'view', 'viewer', 'villain', 'violenc', 'visit', 'visual', 'voic', 'volum', 'wa', 'wait', 'walk', 'want', 'war', 'warn', 'wasnt', 'wast', 'watch', 'water', 'way', 'weak', 'wear', 'week', 'weight', 'went', 'west', 'western', 'whatev', 'whi', 'white', 'wife', 'wild', 'william', 'win', 'wish', 'wit', 'woman', 'women', 'wonder', 'wont', 'word', 'work', 'workout', 'world', 'wors', 'worst', 'worth', 'wouldnt', 'wow', 'write', 'writer', 'written', 'wrong', 'ye', 'year', 'yoga', 'york', 'youll', 'young', 'younger', 'youv', 'zombi']\n",
    "Number of features: 1000\n",
    "Number of stop words: 412\n",
    "Runtime:  3347.527569539845\n",
    "document-term matrix: (1203682, 1000) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
