{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering: CountVectorizer\n",
    "\n",
    "Reads `deduplicated.pickle.bz2`.\n",
    "\n",
    "Data format: `{year {star [(number, year, star)] } }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from amore.printer import Printer\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple usage afterwards\n",
    "file_storage = FileStorage()\n",
    "printer = Printer()\n",
    "\n",
    "\n",
    "KEY_NUMBER = 0\n",
    "#KEY_YEAR   = 1\n",
    "#KEY_STAR   = 2\n",
    "\n",
    "def count_ysl(ysl):\n",
    "    c = 0\n",
    "    for year in ysl.keys():\n",
    "        for star in ysl[year].keys():\n",
    "            c += len(ysl[year][star])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read deduplicated Year/star/review-IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1727821\n",
      "first item: [16505, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "file_duplicates = file_storage.get_filepath('deduplicated')\n",
    "with bz2.BZ2File(file_duplicates, 'r') as file:\n",
    "    dup_ids = pickle.loads(file.read())\n",
    "\n",
    "# Print overview\n",
    "print_year_star_sum = False\n",
    "count = 0\n",
    "first = None\n",
    "for year in dup_ids:\n",
    "    for star in dup_ids[year]:\n",
    "        size = len(dup_ids[year][star])\n",
    "        if print_year_star_sum:\n",
    "            print(year, star, size)\n",
    "        count += size\n",
    "        if first is None:\n",
    "            first = dup_ids[year][star][0]\n",
    "print('size: ' + str(count)) # size: 1727821\n",
    "print('first item:', first)  # first item: [16505, 2007, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>597</td>\n",
       "      <td>2512</td>\n",
       "      <td>3015</td>\n",
       "      <td>3597</td>\n",
       "      <td>3689</td>\n",
       "      <td>6643</td>\n",
       "      <td>10413</td>\n",
       "      <td>9943</td>\n",
       "      <td>11125</td>\n",
       "      <td>12661</td>\n",
       "      <td>14150</td>\n",
       "      <td>15822</td>\n",
       "      <td>19132</td>\n",
       "      <td>21570</td>\n",
       "      <td>134897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>437</td>\n",
       "      <td>2162</td>\n",
       "      <td>2541</td>\n",
       "      <td>3048</td>\n",
       "      <td>3364</td>\n",
       "      <td>4880</td>\n",
       "      <td>7053</td>\n",
       "      <td>7050</td>\n",
       "      <td>8067</td>\n",
       "      <td>8417</td>\n",
       "      <td>8846</td>\n",
       "      <td>9536</td>\n",
       "      <td>11363</td>\n",
       "      <td>12041</td>\n",
       "      <td>88835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>880</td>\n",
       "      <td>3932</td>\n",
       "      <td>4562</td>\n",
       "      <td>5064</td>\n",
       "      <td>5860</td>\n",
       "      <td>8592</td>\n",
       "      <td>11420</td>\n",
       "      <td>11322</td>\n",
       "      <td>13932</td>\n",
       "      <td>13944</td>\n",
       "      <td>14835</td>\n",
       "      <td>14925</td>\n",
       "      <td>16796</td>\n",
       "      <td>17593</td>\n",
       "      <td>143723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>146</td>\n",
       "      <td>2166</td>\n",
       "      <td>9832</td>\n",
       "      <td>11216</td>\n",
       "      <td>12257</td>\n",
       "      <td>13466</td>\n",
       "      <td>19364</td>\n",
       "      <td>25958</td>\n",
       "      <td>27917</td>\n",
       "      <td>37664</td>\n",
       "      <td>36838</td>\n",
       "      <td>37089</td>\n",
       "      <td>36408</td>\n",
       "      <td>40392</td>\n",
       "      <td>40528</td>\n",
       "      <td>351245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>561</td>\n",
       "      <td>7266</td>\n",
       "      <td>25204</td>\n",
       "      <td>26294</td>\n",
       "      <td>29576</td>\n",
       "      <td>32416</td>\n",
       "      <td>46222</td>\n",
       "      <td>64445</td>\n",
       "      <td>71619</td>\n",
       "      <td>108952</td>\n",
       "      <td>104455</td>\n",
       "      <td>112998</td>\n",
       "      <td>113957</td>\n",
       "      <td>130571</td>\n",
       "      <td>134571</td>\n",
       "      <td>1009121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>21</td>\n",
       "      <td>828</td>\n",
       "      <td>11346</td>\n",
       "      <td>43642</td>\n",
       "      <td>47628</td>\n",
       "      <td>53542</td>\n",
       "      <td>58795</td>\n",
       "      <td>85701</td>\n",
       "      <td>119289</td>\n",
       "      <td>127851</td>\n",
       "      <td>179740</td>\n",
       "      <td>176315</td>\n",
       "      <td>187918</td>\n",
       "      <td>190648</td>\n",
       "      <td>218254</td>\n",
       "      <td>226303</td>\n",
       "      <td>1727821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1997 1998   1999   2000   2001   2002   2003   2004    2005    2006  \\\n",
       "1      2   26    597   2512   3015   3597   3689   6643   10413    9943   \n",
       "2    NaN   30    437   2162   2541   3048   3364   4880    7053    7050   \n",
       "3      1   65    880   3932   4562   5064   5860   8592   11420   11322   \n",
       "4      4  146   2166   9832  11216  12257  13466  19364   25958   27917   \n",
       "5     14  561   7266  25204  26294  29576  32416  46222   64445   71619   \n",
       "Sum   21  828  11346  43642  47628  53542  58795  85701  119289  127851   \n",
       "\n",
       "       2007    2008    2009    2010    2011    2012        Sum  \n",
       "1     11125   12661   14150   15822   19132   21570   134897.0  \n",
       "2      8067    8417    8846    9536   11363   12041    88835.0  \n",
       "3     13932   13944   14835   14925   16796   17593   143723.0  \n",
       "4     37664   36838   37089   36408   40392   40528   351245.0  \n",
       "5    108952  104455  112998  113957  130571  134571  1009121.0  \n",
       "Sum  179740  176315  187918  190648  218254  226303  1727821.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews in dup_ids: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Print duplicate IDs as table\n",
    "if True:\n",
    "    printer.ipython_display(printer.get_dataframe_with_sums(dup_ids))\n",
    "if False:\n",
    "    print(printer.get_dataframe_markdown(printer.get_dataframe_with_sums(dup_ids), float_as_integer=True, tablefmt=\"pipe\"))\n",
    "print('Reviews in dup_ids:', count_ysl(dup_ids))\n",
    "# Reviews in ys_lists: 1,727,821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_numbers = set()\n",
    "for year in dup_ids.keys():\n",
    "    if(year < 2006):\n",
    "        continue\n",
    "    for star in dup_ids[year].keys():\n",
    "        if(star == 3):\n",
    "            continue\n",
    "        for tup in dup_ids[year][star]:\n",
    "            review_numbers.add(tup[KEY_NUMBER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(review_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revno_to_text = {}\n",
    "\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, max_docs=-1)\n",
    "for item in reader:\n",
    "    if item[AmazonReviewsReader.KEY_NUMBER] in review_numbers:\n",
    "        revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print(len(revno_to_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from gensim.parsing.preprocessing import STOPWORDS as stopwords_gensim\n",
    "    print('stopwords_gensim', len(stopwords_gensim))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    #import nltk\n",
    "    #nltk.download('stopwords')\n",
    "    stopwords_nltk = set(stopwords.words('english'))\n",
    "    print('stopwords_nltk', len(stopwords_nltk))\n",
    "\n",
    "    from sklearn.feature_extraction import _stop_words\n",
    "    stopwords_sklearn = _stop_words.ENGLISH_STOP_WORDS\n",
    "    print('stopwords_sklearn', len(stopwords_sklearn))\n",
    "\n",
    "    import spacy\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    stopwords_spacy = spacy.load(\"en_core_web_sm\").Defaults.stop_words\n",
    "    print('stopwords_spacy', len(stopwords_spacy))\n",
    "\n",
    "    stopwords_all = stopwords_nltk.union(stopwords_sklearn).union(stopwords_spacy).union(stopwords_gensim)\n",
    "    # stopwords_gensim 337\n",
    "    # stopwords_nltk 179\n",
    "    # stopwords_sklearn 318\n",
    "    # stopwords_spacy 326\n",
    "    # stopwords_all 412\n",
    "\n",
    "    print(InterimStorage('stopwords').write(stopwords_all).get_filepath())\n",
    "else:\n",
    "    stopwords_all = InterimStorage('stopwords').read()\n",
    "print('stopwords_all', len(stopwords_all))\n",
    "# stopwords_all 412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'other', 'all', 'did', 'whereupon', 'whole', 'its', 'con', 'wasn', 'four', 'wherein', 'o', 'be', \"weren't\", 'because', \"you're\", 'again', 'nine', 'am', 'twenty', 'de', 'however', 'doesn', 'could', 'whose', 'un', 'may', 'toward', 'hundred', 'would', 'out', '‘ve', 'if', 'some', 'anywhere', 'perhaps', '‘m', \"didn't\", 'becoming', 'five', 'system', 'on', 'me', 'mine', 'least', 'meanwhile', \"mustn't\", 'two', 'mostly', '’d', 'fifteen', 'having', 'per', 'further', 'whatever', 'top', 'shan', 'himself', 'several', 'thereafter', 'part', 'about', 'cannot', 'whoever', '’s', 'seem', 'too', 'rather', 'latterly', 'sometime', 'where', 'off', 'anyway', 'really', 'last', 'many', 'when', 'whither', 'unless', 'hereby', 'though', 'become', 'will', 'own', 'front', \"should've\", 'above', 'along', 'another', 'an', 'what', 'cant', \"haven't\", 'm', 'for', 'doing', \"wouldn't\", 'using', 'hereafter', 'eg', 'mightn', 'beforehand', 'anyone', 'none', 'are', 'why', 'empty', 'which', '’ll', 'everywhere', 'co', 'same', 'cry', 'seems', 'afterwards', 'that', 'via', 'something', 'quite', 'find', '’re', 'upon', 'everything', 'around', 've', 'under', 'the', 'km', 'very', \"you'd\", 'take', 'my', 'we', 'otherwise', 'get', 'former', 'once', 'kg', 'then', 'should', 'various', 'name', 'became', 'ours', 'everyone', 'yourself', \"isn't\", 'yours', 'first', 'either', 'bill', 'it', 'haven', 'these', 'call', 'until', 'couldn', 'whereas', 'while', 'hence', 'those', 'hadn', \"n't\", 'whereby', 'put', 'seemed', 'found', 'thru', 'amount', 'whether', 'without', 'won', 'six', '‘ll', 'besides', 'there', 'within', '’m', 'ourselves', 'were', 'throughout', 'except', 'hasnt', \"doesn't\", 'beside', 'down', 'than', 'sixty', 'beyond', '‘s', 'also', 'in', 'formerly', 'fill', 'among', 'nowhere', \"won't\", 'sometimes', 'ie', 'being', 'make', 'else', 'anything', \"needn't\", 'her', 'therefore', 'seeming', 'computer', \"couldn't\", 'herein', 'enough', 'therein', 'but', 'd', 'becomes', 'ma', \"you'll\", 'twelve', 'three', 'shouldn', 'he', 'wherever', 'our', 'eleven', 'have', 'already', \"hasn't\", 'show', 'yet', 'wouldn', 'few', 'herself', 'every', 'whenever', 'isn', 'give', \"don't\", 'even', 'amongst', 'always', \"'s\", \"hadn't\", 'before', 'weren', 'now', 'do', 'nevertheless', 'much', 'most', 'might', 'how', 'needn', 'anyhow', 'your', 'any', 'itself', 'others', \"aren't\", 'whence', 'hereupon', 'interest', 'one', 'thus', 'side', 'this', 'used', 'next', 'since', 'you', 'n’t', 'serious', 'his', 'behind', 'must', 'somewhere', 'at', 'fifty', 'go', 'and', 'mustn', 'she', 'ever', 'nobody', 'still', 'neither', 'no', 'moreover', \"it's\", 'nor', 'during', 'across', \"'m\", \"wasn't\", 'bottom', '‘d', 'more', 'back', \"she's\", 'thick', 'theirs', '’ve', \"shouldn't\", 'a', 'eight', 'couldnt', 'yourselves', 'noone', 'thin', \"'d\", 'hers', 'of', \"'ll\", 'thereby', 'is', 'due', 'over', 'has', \"you've\", 'whereafter', 'keep', 'so', 'to', 'with', 'through', 'amoungst', 'done', 'together', 'never', 'well', 's', 'him', 'someone', 'don', 'third', 'onto', 'mill', 'thereupon', 'move', 'somehow', 'n‘t', 't', 'such', 'i', 'into', 'each', 'ca', 'sincere', 'often', 'fire', 'made', 'them', \"shan't\", 're', 'below', 'here', 'between', 'only', 'ten', \"'ve\", 'from', 'myself', 'almost', 'by', 'as', 'describe', 'alone', 'against', \"mightn't\", 'elsewhere', 'been', 'aren', 'inc', 'had', 'latter', 'full', 'please', 'forty', 'thence', 'ltd', 'does', 'etc', 'after', 'say', 'can', 'detail', 'was', 'towards', 'll', 'whom', 'themselves', 'or', \"that'll\", 'both', 'although', 'up', \"'re\", 'who', 'nothing', 'namely', 'not', '‘re', 'ain', 'us', 'less', 'they', 'see', 'their', 'didn', 'just', 'indeed', 'hasn', 'y', 'regarding'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Alternatives: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "index_to_num = {}\n",
    "for i, item in enumerate(revno_to_text.items()):\n",
    "    corpus.append(item[1])\n",
    "    index_to_num[i] = item[0]\n",
    "\n",
    "if False:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    # Installation:    \n",
    "    #import nltk\n",
    "    #nltk.download('punkt')\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk import word_tokenize\n",
    "    import string\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    \n",
    "    def stem_tokens(tokens):\n",
    "        return [stemmer.stem(item) for item in tokens if len(item)>=3]\n",
    "\n",
    "    def normalize(text):\n",
    "        return stem_tokens(word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "    #stop_words = \"english\"\n",
    "    stop_words = stopwords_all\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    corpus = list(revno_to_text.values())\n",
    "    vectorizer = CountVectorizer(tokenizer=normalize, stop_words=stop_words, max_features=1000)\n",
    "    countvec = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    print(InterimStorage('countvec-object').write(countvec).get_filepath())\n",
    "    print(InterimStorage('countvec-vectorizer').write(vectorizer.vocabulary_).get_filepath())\n",
    "    \n",
    "    print(len(vectorizer.get_feature_names()))\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(len(vectorizer.get_stop_words()))\n",
    "    \n",
    "else:\n",
    "    countvec = InterimStorage('countvec-object').read()\n",
    "    \n",
    "print(countvec.shape, type(countvec))\n",
    "# (1,203,682, 1000) <class 'scipy.sparse.csr.csr_matrix'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
