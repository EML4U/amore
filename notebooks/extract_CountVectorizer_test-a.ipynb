{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 are not included in Doc2Vec embeddings. (2000 to 2012 included.)\n",
    "- Ideas:\n",
    "    - 100/0 to 0/100 neg/pos\n",
    "    - 50/50 to 40/60 neg/pos\n",
    "    - build on results on that: other distributions, e.g. 45/55\n",
    "    - 50/50 to 40/30/30 neg/posCluster1/posCluster2\n",
    "    - for token-level and document-level evaluation\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 324.32746749557555\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 38.51574709266424\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    ids += year_star_ids[year][star]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    # Note (TODO): Maybe a simplified version could become to replace the indices dicts by lists,\n",
    "    # as indices probably simply increment starting at 0.\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id: dict\n",
    "    docindex_to_reviewid = None\n",
    "    reviewid_to_docindex = None\n",
    "    \n",
    "    # matrix-term-index to token: dict\n",
    "    tokenindex_to_token = None\n",
    "    token_to_tokenindex = None\n",
    "        \n",
    "    def __init__(self, doc_term_matrix, docindex_to_reviewid, tokenindex_to_token):\n",
    "        print('Document-term matrix:       ', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "        print('Document-index to review-id:', len(docindex_to_reviewid), type(docindex_to_reviewid))\n",
    "        print('Term-index to token:        ', len(tokenindex_to_token), type(tokenindex_to_token))\n",
    "        self.doc_term_matrix      = doc_term_matrix\n",
    "        self.docindex_to_reviewid = docindex_to_reviewid\n",
    "        self.tokenindex_to_token  = tokenindex_to_token\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', docs ' + str(len(self.docindex_to_reviewid)) + ', tokens ' + str(len(self.tokenindex_to_token))\n",
    "    \n",
    "    def get_reviewid_to_docindex(self, review_id):\n",
    "        if not self.reviewid_to_docindex:\n",
    "            self.reviewid_to_docindex = {v: k for k, v in self.docindex_to_reviewid.items()}\n",
    "        if review_id in self.reviewid_to_docindex.keys():\n",
    "            return self.reviewid_to_docindex[review_id]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_token_to_tokenindex(self, token):\n",
    "        if not self.token_to_tokenindex:\n",
    "            self.token_to_tokenindex = {v: k for k, v in self.tokenindex_to_token.items()}\n",
    "        if token in self.token_to_tokenindex:\n",
    "            return self.token_to_tokenindex[token]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_token_indices(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].indices\n",
    "        \n",
    "    def get_token_data(self, review_id=None, doc_index=None):\n",
    "        \"\"\"\n",
    "        Returns 1-dimensional numpy.ndarray.\n",
    "        \"\"\"\n",
    "        if not doc_index:\n",
    "            doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        return self.doc_term_matrix[doc_index].data\n",
    "    \n",
    "    def get_token_counts(self, review_id):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count).\n",
    "        \"\"\"\n",
    "        token_counts = {}\n",
    "        doc_index = self.get_reviewid_to_docindex(review_id)\n",
    "        token_data = self.get_token_data(doc_index=doc_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(doc_index=doc_index)):\n",
    "            token = self.get_token(token_index)\n",
    "            if token:\n",
    "                token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def get_token(self, token_index):\n",
    "        if token_index in self.tokenindex_to_token:\n",
    "            return self.tokenindex_to_token[token_index]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_overall_token_counts(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their sums of (multiple) occurences in all documents.\n",
    "        \"\"\"\n",
    "        token_sums = self.doc_term_matrix.sum(0)\n",
    "        token_counts = {}\n",
    "        for token_index in range(0, token_sums.shape[1]):\n",
    "            token_counts[self.get_token(token_index)] = token_sums.item(token_index)\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Note (TODO): Expensive. Could be improved by using matrix instead of dict.\n",
    "        \n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.get_token(item[0])] = item[1]\n",
    "        return token_occurences\n",
    "        \n",
    "    def filter_min_count(self, min_count):  \n",
    "        \"\"\"\n",
    "        Filters matrix based on token minimum counts (overall word usage)\n",
    "        \"\"\"\n",
    "        # Sum up token occurences in docs\n",
    "        token_sums = self.doc_term_matrix.sum(0)\n",
    "        print('Filtering. Based on', token_sums.shape[1], 'summed up tokens')\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        token_indices_extract = []\n",
    "        new_inv_vocabulary = {}\n",
    "        new_token_index = 0\n",
    "        for token_index in range(0, token_sums.shape[1]):\n",
    "            if token_sums.item(token_index) >= min_count:\n",
    "                token_indices_extract.append(token_index)\n",
    "                new_inv_vocabulary[new_token_index] = self.get_token(token_index)\n",
    "                new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,token_indices_extract]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)\n",
    "    \n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Collect available token-indices\n",
    "        tokenindex_to_token = {}\n",
    "        for token in tokens:\n",
    "            token_index = self.get_token_to_tokenindex(token)\n",
    "            if token_index:\n",
    "                tokenindex_to_token[token_index] = token\n",
    "        \n",
    "        # Create new inverse vocabulary\n",
    "        new_inv_vocabulary = {}\n",
    "        for new_token_index, token_index in enumerate(tokenindex_to_token.keys()):\n",
    "            new_inv_vocabulary[new_token_index] = tokenindex_to_token[token_index]\n",
    "            new_token_index += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,list(tokenindex_to_token.keys())]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.docindex_to_reviewid, new_inv_vocabulary)\n",
    "    \n",
    "    def filter_reviews(self, years=None, stars=None):\n",
    "        \"\"\"\n",
    "        Filters matrix by years and stars of reviews.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get review-ids\n",
    "        review_ids = []\n",
    "        for review_tup in get_review_ids(years, stars):\n",
    "            review_ids.append(review_tup[0])\n",
    "        review_ids = sorted(review_ids)\n",
    "        \n",
    "        # Collect matrix-doc-indices from review-ids\n",
    "        doc_indices_extract = []\n",
    "        new_docindex_to_reviewid = {}\n",
    "        new_docindex = 0\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.get_reviewid_to_docindex(review_id))\n",
    "            new_docindex_to_reviewid[new_docindex] = review_id\n",
    "            new_docindex += 1\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_docindex_to_reviewid, self.tokenindex_to_token)\n",
    "\n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_remove.append(self.get_reviewid_to_docindex(review_id))\n",
    "\n",
    "        docindex_to_reviewid_keep = dict(self.docindex_to_reviewid)\n",
    "        for doc_index in doc_indices_remove:\n",
    "            docindex_to_reviewid_keep.pop(doc_index)\n",
    "        \n",
    "        new_docindex_to_reviewid = {}\n",
    "        i = 0\n",
    "        for item in sorted(docindex_to_reviewid_keep.items()):\n",
    "            if i < 10:\n",
    "                print(item[0], item[1])\n",
    "            new_docindex_to_reviewid[i] = item[1]\n",
    "            i += 1\n",
    "            \n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), doc_indices_remove, []), new_docindex_to_reviewid, self.tokenindex_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit tokens\n",
    "\n",
    "- all: 607,181\n",
    "- min 2: 299,517\n",
    "- min 10: 111,678\n",
    "- min 100: 38,080\n",
    "- min 1k: 10,842\n",
    "- min 10k: 1,842\n",
    "- min 100k: 124\n",
    "- min 1m: 2\n",
    "\n",
    "Top 124 occurences (counted max 1 times per doc):  \n",
    "'movie', 'great', 'like', 'good', 'film', 'dvd', 'time', 'love', 'best', 'story', 'watch', 'way', 'movies', 'people', 'seen', 'better', 'think', 'know', 'little', 'life', 'watching', 'new', 'series', 'years', 'characters', 'old', 'end', 'want', 'excellent', 'character', 'set', 'bad', 'acting', 'real', 'work', 'films', 'man', 'video', 'makes', 'music', 'fun', 'worth', 'fan', 'long', 'scenes', 'times', 'got', 'recommend', 'going', 'buy', 'world', 'lot', 'look', 'enjoy', 'funny', 'plot', 'thought', 'right', 'thing', 'come', 'family', 'cast', 'things', 'loved', 'wonderful', 'feel', 'watched', 'action', 'actually', 'original', 'classic', 'quality', 'day', 'shows', 'big', 'saw', 'actors', 'season', 'bit', 'scene', 'year', 'favorite', 'stars', 'young', 'interesting', 'true', 'different', 'special', 'far', 'highly', 'pretty', 'version', 'especially', 'away', 'enjoyed', 'looking', 'sure', 'hard', 'director', 'amazing', 'job', 'fact', 'performance', 'beautiful', 'amazon', 'perfect', 'gets', 'star', 'second', 'fans', 'high', 'live', 'played', 'comedy', 'collection', 'sound', 'play', 'role', 'book', 'episodes', 'kids', 'episode', 'quot', 'horror'\n",
    "\n",
    "Top 124 counts:  \n",
    "'movie', 'film', 'great', 'like', 'dvd', 'good', 'time', 'story', 'love', 'best', 'watch', 'series', 'quot', 'movies', 'people', 'way', 'life', 'think', 'better', 'little', 'seen', 'new', 'season', 'know', 'characters', 'films', 'video', 'years', 'set', 'character', 'old', 'watching', 'man', 'music', 'bad', 'end', 'work', 'scenes', 'fun', 'real', 'excellent', 'want', 'world', 'family', 'acting', 'action', 'original', 'makes', 'fan', 'funny', 'long', 'going', 'lot', 'version', 'got', 'times', 'look', 'worth', 'plot', 'buy', 'scene', 'right', 'quality', 'things', 'thing', 'classic', 'day', 'cast', 'thought', 'feel', 'recommend', 'shows', 'enjoy', 'actually', 'book', 'wonderful', 'come', 'big', 'young', 'amazon', 'loved', 'special', 'episodes', 'bit', 'year', 'watched', 'saw', 'performance', 'actors', 'stars', 'different', 'interesting', 'episode', 'true', 'pretty', 'director', 'favorite', 'far', 'star', 'comedy', 'amazing', 'live', 'away', 'beautiful', 'especially', 'fact', 'job', 'looking', 'hard', 'horror', 'fans', 'played', 'sound', 'highly', 'sure', 'collection', 'gets', 'enjoyed', 'perfect', 'role', 'second', 'high', 'play', 'kids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         607181 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 607181 summed up tokens\n",
      "Document-term matrix:        (1584098, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1584098 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# all reviews starting from 2000 without 3 stars\n",
    "m = Matrix(doc_term_matrix, vecid_revno, inv_vocabulary)\n",
    "print()\n",
    "\n",
    "if False:\n",
    "    m2 = m.filter_min_count(2)\n",
    "    print()\n",
    "\n",
    "m100 = m.filter_min_count(100)\n",
    "\n",
    "# Test, should be \"Term-index to token: 10842\"\n",
    "if False:\n",
    "    m1k = m.filter_min_count(1000)\n",
    "    m1k = m2.filter_min_count(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit matrix to subset of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 1572849 review IDs\n",
      "Document-term matrix:        (1572849, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1572849 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# matrix to use afterwards\n",
    "matrix = m100.filter_reviews(years=[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012], stars=[1,2,4,5])\n",
    "\n",
    "# Only keep matrix in memory\n",
    "m = None\n",
    "m2 = None\n",
    "m100 = None\n",
    "m1k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit matrix tokens to pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1572849, 3084) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1572849 <class 'dict'>\n",
      "Term-index to token:         3084 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# original list size: 4,783\n",
    "m_neg = matrix.filter_tokens(opinion_lexicon.get_negative_set())\n",
    "#print('some negetive words:', list(m_neg.tokenindex_to_token.values())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (1572849, 1500) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 1572849 <class 'dict'>\n",
      "Term-index to token:         1500 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# original list size: 2,006\n",
    "m_pos = matrix.filter_tokens(opinion_lexicon.get_positive_set())\n",
    "#print('some positive words:', list(m_pos.tokenindex_to_token.values())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example review\n",
    "\n",
    "(e.g. review ID 6590 contains pos and neg words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 146616 review IDs\n",
      "Document-term matrix:        (146616, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 146616 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n",
      "\n",
      "Number of review IDs: 108952\n",
      "Review ID: 6590\n"
     ]
    }
   ],
   "source": [
    "m_2007_pos = matrix.filter_reviews(years=[2007], stars=[4,5])\n",
    "print()\n",
    "\n",
    "review_ids = get_review_ids([2007], [5])\n",
    "print(\"Number of review IDs:\", len(review_ids))\n",
    "KEY_ID = 0\n",
    "review_id = review_ids[11][KEY_ID]\n",
    "print(\"Review ID:\", review_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review text:\n",
      "A Priceless Treasure Bobby Short was a favorite performer of mine since I moved to New York in 1955. A friend had his LP with \"At the Moving Picture Ball\" on it and I insisted she play it whenever I dropped by for coffee, which I did every morning when I was between jobs. I always intended to splurge and go see him at the Carlyle but I was too poor and then too populist for a night club and then too stingy. And there were always the great, great records. And suddenly, after only half a century, he was gone. Thank god for this DVD of a wonderful performance at the club. The ebullience, the superb artistry and the glow of his personal niceness make it a marvelous experience to treasure over and over. And he does \"Moving Picture Ball\" and other personal favorites, \"On the Amazon\" and \"Why Shouldn't I?\"  Bobby fans won't need prompting but this great treat should also be a key discovery for anyone interested in popular song styling of the civilized pre-wail-and-whine era, in Manhattan high life, in the triumph of individual striving and dedication, in... Oh hell, in anything that's fun. -- Paul Rawlings, Bayport, N.Y..\n",
      "\n",
      "{'great': 3, 'bobby': 2, 'club': 2, 'moving': 2, 'personal': 2, 'treasure': 2, 'picture': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'dvd': 1, 'era': 1, 'performance': 1, 'song': 1, 'morning': 1, 'amazon': 1, 'pre': 1, 'need': 1, 'wonderful': 1, 'experience': 1, 'marvelous': 1, 'night': 1, 'gone': 1, 'half': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'thank': 1, 'hell': 1, 'short': 1, 'fun': 1, 'play': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'god': 1, 'interested': 1, 'superb': 1, 'york': 1, 'century': 1, 'key': 1, 'poor': 1, 'priceless': 1, 'intended': 1, 'popular': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'wail': 1, 'insisted': 1, 'discovery': 1, 'civilized': 1, 'paul': 1, 'striving': 1, 'styling': 1, 'triumph': 1, 'dedication': 1, 'jobs': 1, 'performer': 1, 'glow': 1, 'dropped': 1, 'records': 1, 'prompting': 1, 'manhattan': 1, 'whine': 1, '1955': 1, 'stingy': 1, 'splurge': 1, 'carlyle': 1, 'populist': 1, 'rawlings': 1}\n",
      "\n",
      "Based on 2007 4 and 5 star reviews\n",
      "{'great': 3, 'bobby': 2, 'club': 2, 'moving': 2, 'personal': 2, 'treasure': 2, 'picture': 2, 'ball': 2, 'suddenly': 1, 'high': 1, 'dvd': 1, 'era': 1, 'performance': 1, 'song': 1, 'morning': 1, 'amazon': 1, 'pre': 1, 'need': 1, 'wonderful': 1, 'experience': 1, 'marvelous': 1, 'night': 1, 'gone': 1, 'half': 1, 'friend': 1, 'life': 1, 'favorite': 1, 'thank': 1, 'hell': 1, 'short': 1, 'fun': 1, 'play': 1, 'new': 1, 'fans': 1, 'favorites': 1, 'treat': 1, 'god': 1, 'interested': 1, 'superb': 1, 'york': 1, 'century': 1, 'key': 1, 'poor': 1, 'priceless': 1, 'intended': 1, 'popular': 1, 'individual': 1, 'moved': 1, 'artistry': 1, 'coffee': 1, 'wail': 1, 'insisted': 1, 'discovery': 1, 'civilized': 1, 'paul': 1, 'striving': 1, 'styling': 1, 'triumph': 1, 'dedication': 1, 'jobs': 1, 'performer': 1, 'glow': 1, 'dropped': 1, 'records': 1, 'prompting': 1, 'manhattan': 1, 'whine': 1, '1955': 1, 'stingy': 1, 'splurge': 1, 'carlyle': 1, 'populist': 1, 'rawlings': 1}\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Review text:')\n",
    "print(get_text(review_id))\n",
    "\n",
    "print()\n",
    "print(matrix.get_token_counts(review_id))\n",
    "\n",
    "print()\n",
    "print('Based on 2007 4 and 5 star reviews')\n",
    "print(m_2007_pos.get_token_counts(review_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare years\n",
    "\n",
    "- Assume 50/50 to 40/60 (neg/pos)  \n",
    "  It makes sense to decrease number of negatives, as there are less available.  \n",
    "  Later similar sets may also be generated, e.g. 50/50 to 45/55 (neg/pos)\n",
    "- Goal: Get and explain drift based on benchmark data.  \n",
    "  Should be based on tokens (words) and also documents.  \n",
    "  Drift in docs: Both, Bow and also BERT (focus on semantics, not words), could have advantages\n",
    "- Explaination on docs: Set of docs (prototypes) would be returned.  \n",
    "  -> Each of the 60% positive docs should be valid. And none of the 40% negative docs.  \n",
    "  -> Goal of algo will be to have exact docs/tokens. Design benchmark data to enable this.\n",
    "\n",
    "Same negative words should be be included in neg reviews of both years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 17466 review IDs\n",
      "Document-term matrix:        (17466, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 17466 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 90403 review IDs\n",
      "Document-term matrix:        (90403, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 90403 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 16993 review IDs\n",
      "Document-term matrix:        (16993, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 16993 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n",
      "\n",
      "Filtering. Based on 99536 review IDs\n",
      "Document-term matrix:        (99536, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 99536 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "m_2000_neg = matrix.filter_reviews(years=[2005], stars=[1,2])\n",
    "print()\n",
    "m_2000_pos = matrix.filter_reviews(years=[2005], stars=[4,5])\n",
    "print()\n",
    "m_2001_neg = matrix.filter_reviews(years=[2006], stars=[1,2])\n",
    "print()\n",
    "m_2001_pos = matrix.filter_reviews(years=[2006], stars=[4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check word/token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_overview(matrix, matrix_name, occurences=True, top=20):\n",
    "    print(matrix_name, matrix.doc_term_matrix.shape, 'top', top)\n",
    "    if occurences:\n",
    "        print('Occurences: ', list(matrix.get_overall_token_occurences().items())[:top])\n",
    "    else:\n",
    "        print('Counts:     ', list(matrix.get_overall_token_counts().items())[:top])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_2000_neg (17466, 38080) top 20\n",
      "Occurences:  [('movie', 10355), ('like', 7235), ('film', 6588), ('good', 5585), ('time', 5139), ('bad', 4852), ('dvd', 4048), ('story', 3616), ('people', 3600), ('better', 3530), ('way', 3432), ('think', 3165), ('movies', 3160), ('know', 3130), ('watch', 3129), ('great', 3110), ('acting', 2910), ('plot', 2838), ('seen', 2750), ('money', 2640)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_token_overview(m_2000_neg, 'm_2000_neg')\n",
    "\n",
    "if False:\n",
    "    print_token_overview(m_2000_neg, 'm_2000_neg', occurences=False)\n",
    "    print_token_overview(m_2000_pos, 'm_2000_pos')\n",
    "    print_token_overview(m_2001_neg, 'm_2001_neg', top=100)\n",
    "    print_token_overview(m_2001_pos, 'm_2001_pos', top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    only_b = {}\n",
    "    rare = []\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter rare\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare.append(tok_occ[0])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = ratio_a / ratio_b\n",
    "            \n",
    "    print('Output sizes:', 'ratio', len(ratio), '; only a', len(only_a), '; only b', len(b_token_occurences), '; rare', len(rare))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), only_a, b_token_occurences, rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 37702 / 32975\n",
      "Output sizes: ratio 1220 ; only a 4768 ; only b 41 ; rare 31714\n",
      "ratio [('outstanding', 31.15784810326214), ('superb', 28.1636127606463), ('beautifully', 24.484116207461973), ('delightful', 24.36269222381365), ('captures', 23.117242561926883), ('hooked', 23.00236420177007), ('gem', 22.980094385404445), ('wonderfully', 21.55692629818359), ('recommended', 20.57704733726798), ('magnificent', 20.384223648612807), ('finest', 19.892475339853533), ('touching', 19.43338107615865), ('terrific', 18.846184253081464), ('excellent', 18.845633666873763), ('awesome', 18.47691866651018), ('wonderful', 17.883745440248138), ('fascinating', 17.332217842778466), ('amazing', 17.318079487449495), ('favorites', 17.028657227271992), ('unique', 16.325075592806748), ('fantastic', 16.286965112032416), ('highly', 16.129475889873216), ('fabulous', 15.701277524200076), ('bonus', 15.672150364529157), ('perfect', 15.601335873347645), ('favorite', 14.903661344225771), ('packed', 14.826991199995282), ('stunning', 14.7696881054687), ('notch', 14.755399085937833), ('remarkable', 14.464609344133928), ('ages', 14.42161582939897), ('includes', 13.878732534772002), ('brilliant', 13.767287429756824), ('journey', 13.729493319793571), ('incredible', 13.656709605239065), ('powerful', 13.373931807636048), ('treat', 13.368174343295026), ('memories', 13.023647551854014), ('helps', 12.81096779595611), ('funniest', 12.772676397659394), ('greatest', 12.67172939983739), ('rare', 12.595696018718103), ('perfectly', 12.59408937499056), ('warm', 12.543365352668712), ('season', 12.519721297917902), ('joy', 12.451099962509067), ('strength', 12.30113703130949), ('sharp', 12.146570473714922), ('hilarious', 12.084271712938628), ('masterpiece', 11.959478729124571)]\n",
      "only_a [('emmys', 112), ('dreamer', 102), ('transfere', 100), ('withers', 88), ('byrd', 86), ('excelent', 84), ('gifford', 84), ('hatton', 82), ('greys', 80), ('rusesabagina', 75)]\n",
      "only_b [('snoozefest', 12), ('hudgens', 5), ('birthed', 4), ('powerpoint', 3), ('tortilla', 2), ('kenshiro', 2), ('subsidies', 2), ('laos', 2), ('graysmith', 2), ('bettie', 2)]\n",
      "rare ['vision', 'convincing', 'nicely', 'legend', 'grace', 'opens', 'lets', 'results', 'engaging', 'theatre']\n"
     ]
    }
   ],
   "source": [
    "ratio, only_a, only_b, rare = get_ratio(m_2000_pos, m_2000_neg, min_token_occurences=1000)\n",
    "print('ratio', list(ratio.items())[:50])\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print('only_b', list(only_b.items())[:10])\n",
    "print('rare', rare[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 32975 / 37702\n",
      "Output sizes: ratio 1220 ; only a 41 ; only b 4768 ; rare 31714\n",
      "ratio [('waste', 4.402619033110612), ('worst', 2.037940921604039), ('horrible', 1.800039257403425), ('awful', 1.7893180720319632), ('terrible', 1.5519201230298887), ('boring', 1.4451619134949723), ('stupid', 1.2091838354750541), ('worse', 1.1099394992263976), ('poor', 0.8083152489848967), ('rent', 0.7820021209234612)]\n",
      "only_a [('snoozefest', 12), ('hudgens', 5), ('birthed', 4), ('powerpoint', 3), ('tortilla', 2), ('kenshiro', 2), ('subsidies', 2), ('laos', 2), ('graysmith', 2), ('bettie', 2)]\n",
      "only_b [('emmys', 112), ('dreamer', 102), ('transfere', 100), ('withers', 88), ('byrd', 86), ('excelent', 84), ('gifford', 84), ('hatton', 82), ('greys', 80), ('rusesabagina', 75)]\n",
      "rare ['crap', 'disappointing', 'lame', 'cheap', 'poorly', 'ridiculous', 'garbage', 'avoid', 'disappointment', 'wasted']\n"
     ]
    }
   ],
   "source": [
    "ratio, only_a, only_b, rare = get_ratio(m_2000_neg, m_2000_pos, min_token_occurences=1000)\n",
    "print('ratio', list(ratio.items())[:10])\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print('only_b', list(only_b.items())[:10])\n",
    "print('rare', rare[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use positive words with high ratio to create drift\n",
    "- Do not use words with low ratio (best: near 1.0) to create drift. Try to create equal document sets.\n",
    "- Remove rare words from matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 2006 | ratio 1220\n",
      "pos UNION ratio 3082\n",
      "pos - ratio 1862\n",
      "ratio - pos 1076\n",
      "['section', 'complete', 'disney', 'rate', 'weeks', 'added', 'christmas', 'sense', '2004', 'hearing', 'form', 'charles', 'edge', 'seeing', 'fiction', 'mean', '100', 'baby', 'gun', 'bring']\n"
     ]
    }
   ],
   "source": [
    "print('pos', len(opinion_lexicon.get_positive_set()), '| ratio', len(ratio.keys()))\n",
    "\n",
    "tmp = opinion_lexicon.get_positive_set().union(ratio.keys())\n",
    "print('pos UNION ratio', len(tmp))\n",
    "\n",
    "tmp = opinion_lexicon.get_positive_set() - ratio.keys()\n",
    "print('pos - ratio', len(tmp))\n",
    "\n",
    "tmp = ratio.keys() - opinion_lexicon.get_positive_set()\n",
    "print('ratio - pos', len(tmp))\n",
    "\n",
    "print(list(tmp)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1215\n"
     ]
    }
   ],
   "source": [
    "tokens_ratio_high = []\n",
    "tokens_ratio_low = []\n",
    "ratio_threshold = 1.5\n",
    "for item in ratio.items():\n",
    "    if item[1] >= ratio_threshold:\n",
    "        tokens_ratio_high.append(item[0])\n",
    "    else:\n",
    "        tokens_ratio_low.append(item[0])\n",
    "        \n",
    "print(len(tokens_ratio_high))\n",
    "print(len(tokens_ratio_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (90403, 5) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 90403 <class 'dict'>\n",
      "Term-index to token:         5 <class 'dict'>\n",
      "(90403, 5)\n",
      "4318\n",
      "(90403, 5)\n",
      "4318\n"
     ]
    }
   ],
   "source": [
    "tmp = m_2000_pos.filter_tokens(tokens_ratio_high).doc_term_matrix\n",
    "print(tmp.shape)\n",
    "print(tmp.count_nonzero())\n",
    "tmp.eliminate_zeros()\n",
    "tmp.prune()\n",
    "print(tmp.shape)\n",
    "print(tmp.count_nonzero())\n",
    "\n",
    "# does not work this way, clean matrix based on document filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_ids 90403\n",
      "ids 232 4\n",
      "data 1\n",
      "ids 233 5\n",
      "data 2\n",
      "\n",
      "232 5625\n",
      "234 5997\n",
      "Document-term matrix:        (2, 38080) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 2 <class 'dict'>\n",
      "Term-index to token:         38080 <class 'dict'>\n",
      "ids 232 None\n",
      "data [[ 2  2  4  1  1  1  2  1  1  1  3  1  2  1  2  1  1  1  1  1  1  1  1  2\n",
      "   1  1  1  1  1  1  2  3  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  3\n",
      "   1  1  1  1  1  1  1  1 13  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  2  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1 10  1  1  1  1\n",
      "   1  2  1  1  1  1  1  1  2  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  2  3  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1\n",
      "   1  2  1  1  1  1  1  1  3  1  1  1  1  1  1  1  7  1  1  2  1  3  1  1\n",
      "   1  2  1  2  1  1  1  1]]\n",
      "ids 233 None\n",
      "data [[ 2  2  4  1  1  1  2  1  1  1  3  1  2  1  2  1  1  1  1  1  1  1  1  2\n",
      "   1  1  1  1  1  1  2  3  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  3\n",
      "   1  1  1  1  1  1  1  1 13  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  2  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1 10  1  1  1  1\n",
      "   1  2  1  1  1  1  1  1  2  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  2  3  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1\n",
      "   1  2  1  1  1  1  1  1  3  1  1  1  1  1  1  1  7  1  1  2  1  3  1  1\n",
      "   1  2  1  2  1  1  1  1]]\n",
      "shape (2, 38080)\n"
     ]
    }
   ],
   "source": [
    "tmp = m_2000_pos#.filter_tokens(tokens_ratio_high) # TODO: probably buggy\n",
    "print()\n",
    "\n",
    "review_ids = list(tmp.docindex_to_reviewid.values())\n",
    "print('review_ids', len(review_ids))\n",
    "\n",
    "key = 4\n",
    "review_id = review_ids[key]\n",
    "doc_index = tmp.get_reviewid_to_docindex(review_id)\n",
    "print('ids', review_id, doc_index)\n",
    "print('data', tmp.doc_term_matrix.data[doc_index])\n",
    "review_id_2 = review_ids[key+1]\n",
    "doc_index = tmp.get_reviewid_to_docindex(review_id_2)\n",
    "print('ids', review_id_2, doc_index)\n",
    "print('data', tmp.doc_term_matrix.data[doc_index])\n",
    "print()\n",
    "\n",
    "review_ids.pop(review_id)\n",
    "review_ids.pop(review_id_2)\n",
    "tmp = tmp.filter_remove_reviews(review_ids)\n",
    "doc_index = tmp.get_reviewid_to_docindex(review_id)\n",
    "print('ids', review_id, doc_index)\n",
    "print('data', tmp.doc_term_matrix.data[doc_index])\n",
    "doc_index = tmp.get_reviewid_to_docindex(review_id_2)\n",
    "print('ids', review_id_2, doc_index)\n",
    "print('data', tmp.doc_term_matrix.data[doc_index])\n",
    "print('shape', tmp.doc_term_matrix.shape)\n",
    "#print('data', tmp.doc_term_matrix.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5625, 1: 5997}\n"
     ]
    }
   ],
   "source": [
    "print(tmp.docindex_to_reviewid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix:        (90403, 5) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Document-index to review-id: 90403 <class 'dict'>\n",
      "Term-index to token:         5 <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "tmp = m_2000_pos.filter_tokens(tokens_ratio_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625\n"
     ]
    }
   ],
   "source": [
    "print(tmp.docindex_to_reviewid[232])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
