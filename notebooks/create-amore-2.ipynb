{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 contain only a few reviews and are not included in Doc2Vec embeddings.  \n",
    "  (The years 2000 to 2012 are included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* **opinion_lexicon**: Instance of OpinionLexicon to access negative and positive words. Not used for now.\n",
    "* **year_star_ids**: Collection of IDs sorted by years and stars. Used afterwards to filter by stars and years in method **get_review_ids**.\n",
    "* **reader**: Instance of AmazonReviewsReader to access review data. Used afterwards to create revno_to_text.\n",
    "* **revno_to_text**: Dictionary review-number to full text. Used afterwards to access review-texts in method **get_text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple usage afterwards\n",
    "file_storage = FileStorage()\n",
    "\n",
    "# Already used before, not required in this version.\n",
    "if False:\n",
    "    from amore.opinion_lexicon import OpinionLexicon\n",
    "    opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "    print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "    print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "    # negative words: 4783\n",
    "    # positive words: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 2129442\n",
      "Runtime: 253.98197159497067\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "\n",
    "# Configuration: Only texts of reviews to be included afterwards have be be read.\n",
    "min_year = 2006\n",
    "max_year = 2008\n",
    "\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_year=max_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files\n",
    "\n",
    "Data required to create **Matrix** instances afterwards.\n",
    "\n",
    "* **doc_term_matrix**: Matrix containing document-IDs and term-IDs. Used afterwards for building Matrix.\n",
    "* **vocabulary**: Dictionary containing tuples of terms and term-IDs. Used afterwards to create inv_vocabulary.\n",
    "* **inv_vocabulary**: Dictionary containing tuples of term-IDs and terms.Used afterwards to create tokens.\n",
    "* **tokens**: Ordered list of terms, index represents term-ID. Used afterwards as token-indices for building Matrix.\n",
    "* **vecid_revno**: Values used afterwards as review-indices for building Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 37.495712163392454\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access methods and Matrix class\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used afterwards in Matrix class.\n",
    "\n",
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None # list of tokens\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1  \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences\n",
    "    \n",
    "    def count_token_occurences(self, review_ids, tokens):\n",
    "        \"\"\"\n",
    "        Sums up the occurencies of given tokens in given reviews.\n",
    "        \"\"\"\n",
    "        review_to_tokenoccurences = {}\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "        for review_id in review_ids:\n",
    "            if not review_id in review_ids:\n",
    "                continue\n",
    "            review_to_tokenoccurences[review_id] = 0\n",
    "            for counts in self.get_token_data(review_id=review_id):\n",
    "                review_to_tokenoccurences[review_id] += 1\n",
    "        return review_to_tokenoccurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "#### Get tokens which occur predominantly in only one dataset: Get ratio for each token.\n",
    "\n",
    "* **get_ratio**\n",
    "  * In: matrix-A, matrix-B\n",
    "  * In: min_token_occurences (token has to occure at least x times in each of both matrixes)\n",
    "  * Out: tokens sorted by ratio\n",
    "  * Out: tokens sorted by inverted ratio\n",
    "  * Out: tokens not reached minimum\n",
    "  * Out: tokens only in A\n",
    "  * Out: tokens only in B\n",
    "* **filter_tokens_by_ratio**\n",
    "  * In: ratio (generated by method get_ratio)\n",
    "  * In: min_ratio (minimum threshold)\n",
    "  * In: max_tokens (maximum number of tokens to return)\n",
    "  * In: exclude_tokens (tokens not to return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    # Pairs of tokens and occurences\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter/remove rare:\n",
    "        # minimum is set AND\n",
    "        # token has to be in both matrixes AND\n",
    "        # threshold reached in A AND\n",
    "        # threshod reached in B AND\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare[tok_occ[0]] = str(tok_occ[1]) + \" / \" + str(b_token_occurences[tok_occ[0]])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data / create base matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Main matrix containing all data\n",
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews to use\n",
    "ids_neg_a = get_review_ids(years=[2006,2007], stars=[1,2])\n",
    "ids_neg_b = get_review_ids(years=[2008], stars=[1,2])\n",
    "ids_neg = ids_neg_a + ids_neg_b\n",
    "\n",
    "ids_pos_a = []\n",
    "ids_pos_b = get_review_ids(years=[2008], stars=[4,5])\n",
    "ids_pos = ids_pos_a + ids_pos_b\n",
    "\n",
    "ids_all = ids_neg + ids_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 57263 review IDs\n",
      "Created: Matrix (57263, 607181), 57263 reviews, 607181 tokens\n",
      "\n",
      "Filtering. Based on 141293 review IDs\n",
      "Created: Matrix (141293, 607181), 141293 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Sub-matrixes\n",
    "m_neg = matrix.filter_keep_reviews(ids_neg)\n",
    "print()\n",
    "m_pos = matrix.filter_keep_reviews(ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens to use\n",
    "\n",
    "Note: During exploration of the data it turned out that there are more positive words that have a large ratio and therefore can be used to determine (unique) drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 156420 / 102852\n",
      "Output sizes: ratio 60 ; rare 69315 ; only a 87045 ; only b 33477\n",
      "\n",
      "only_a [('magers', 108), ('webmaster', 106), ('nareau', 105), ('guideslines', 104), ('jbl', 68), ('druxman', 61), ('salutations', 56), ('stane', 54), ('obadiah', 53), ('dessay', 52)]\n",
      "\n",
      "ratio_a [('highly', 6.68), ('excellent', 6.51), ('wonderful', 6.1), ('loved', 4.13), ('season', 4.05), ('great', 3.79), ('fun', 3.59), ('classic', 3.48), ('recommend', 3.28), ('enjoy', 3.06), ('love', 3.0), ('best', 2.84), ('family', 2.72), ('series', 2.55), ('music', 2.16), ('years', 2.11), ('dvd', 2.03), ('set', 2.03), ('life', 1.85), ('world', 1.85), ('worth', 1.8), ('times', 1.8), ('watched', 1.79), ('fan', 1.78), ('new', 1.75), ('old', 1.67), ('video', 1.65), ('good', 1.59), ('makes', 1.59), ('watch', 1.53), ('work', 1.48), ('lot', 1.48), ('man', 1.44), ('long', 1.4), ('story', 1.39), ('little', 1.39), ('time', 1.34), ('seen', 1.33), ('funny', 1.33), ('watching', 1.3), ('real', 1.29), ('films', 1.25), ('way', 1.24), ('look', 1.19), ('like', 1.17), ('movies', 1.17), ('got', 1.17), ('buy', 1.17), ('think', 1.15), ('characters', 1.13)]\n",
      "\n",
      "ratio_b [('bad', 2.63), ('acting', 1.27), ('better', 0.96), ('end', 0.95), ('character', 0.95), ('people', 0.93), ('know', 0.93), ('movie', 0.92), ('film', 0.92), ('want', 0.92), ('characters', 0.88), ('think', 0.87), ('like', 0.85), ('movies', 0.85), ('got', 0.85), ('buy', 0.85), ('look', 0.84), ('way', 0.81), ('films', 0.8), ('real', 0.78), ('watching', 0.77), ('time', 0.75), ('seen', 0.75), ('funny', 0.75), ('story', 0.72), ('little', 0.72), ('long', 0.71), ('man', 0.69), ('work', 0.68), ('lot', 0.68), ('watch', 0.65), ('good', 0.63), ('makes', 0.63), ('video', 0.61), ('old', 0.6), ('new', 0.57), ('fan', 0.56), ('worth', 0.56), ('times', 0.56), ('watched', 0.56), ('life', 0.54), ('world', 0.54), ('dvd', 0.49), ('set', 0.49), ('years', 0.47), ('music', 0.46), ('series', 0.39), ('family', 0.37), ('best', 0.35), ('love', 0.33)]\n",
      "\n",
      "only_b [('paolini', 36), ('freedomland', 22), ('retarted', 19), ('galbatorix', 19), ('malus', 19), ('refunds', 18), ('contagium', 16), ('hemophages', 15), ('fallacious', 15), ('snorefest', 14)]\n",
      "\n",
      "rare [('shows', '9995 / 2621'), ('going', '9941 / 6846'), ('scenes', '9818 / 6445'), ('feel', '9797 / 4100'), ('favorite', '9778 / 1341'), ('come', '9641 / 4739'), ('day', '9612 / 3342'), ('enjoyed', '9610 / 1406'), ('right', '9601 / 4709'), ('amazon', '9545 / 3451')]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_pos,\n",
    "    m_neg,\n",
    "    min_token_occurences=10*1000)\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'wonderful', 'loved', 'great', 'fun', 'recommend', 'enjoy', 'love', 'best', 'good']\n",
      "['bad', 'acting']\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 1.25, 10,\n",
    "['highly', 'season', 'collection', 'episodes', 'features', 'classic', 'episode', 'definitely', 'shows', 'family', 'performances', 'gives', 'young', 'series',\n",
    " 'music', 'performance', 'live', 'years', 'job', 'john', 'set', 'true', 'finally', 'truly', 'world', 'different', 'dvd', 'especially', 'history',\n",
    " 'life', 'cast', 'worth', 'fan', 'times', 'new', 'old', 'makes', 'watch',\n",
    " 'work', 'man', 'video', 'long', 'story', 'lot', 'little', 'come', 'seen', 'watched'\n",
    "])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.25, 10, [])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 10), 1584098 reviews, 10 tokens\n",
      "Filtering. Based on 36185 review IDs\n",
      "Created: Matrix (36185, 10), 36185 reviews, 10 tokens\n",
      "Filtering. Based on 21078 review IDs\n",
      "Created: Matrix (21078, 10), 21078 reviews, 10 tokens\n",
      "Filtering. Based on 141293 review IDs\n",
      "Created: Matrix (141293, 10), 141293 reviews, 10 tokens\n"
     ]
    }
   ],
   "source": [
    "# Remove everything but filtered tokens\n",
    "matrix_filtered       = matrix.filter_tokens(filter_tokens_a)\n",
    "matrix_filtered_neg_a = matrix_filtered.filter_keep_reviews(ids_neg_a)\n",
    "matrix_filtered_neg_b = matrix_filtered.filter_keep_reviews(ids_neg_b)\n",
    "matrix_filtered_pos_b = matrix_filtered.filter_keep_reviews(ids_pos_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27668\n"
     ]
    }
   ],
   "source": [
    "# Filter: Only texts with minimal specified positive words\n",
    "reviews_neg_a = {}\n",
    "for review_item in matrix_filtered_neg_a.count_token_occurences(ids_neg_a, filter_tokens_a).items():\n",
    "    if review_item[1] <= 1:\n",
    "        reviews_neg_a[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16446\n"
     ]
    }
   ],
   "source": [
    "# Filter: Only texts with minimal specified positive words\n",
    "reviews_neg_b = {}\n",
    "for review_item in matrix_filtered_neg_b.count_token_occurences(ids_neg_b, filter_tokens_a).items():\n",
    "    if review_item[1] <= 1:\n",
    "        reviews_neg_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2888\n"
     ]
    }
   ],
   "source": [
    "# Filter: Only texts with at least 5 specified positive words\n",
    "reviews_pos_b = {}\n",
    "for review_item in matrix_filtered_pos_b.count_token_occurences(ids_pos_b, filter_tokens_a).items():\n",
    "    if review_item[1] >= 5:\n",
    "        reviews_pos_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_pos_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sort by number of occurences of specified positive words\n",
    "reviews_neg_a_valuesorted = {k: v for k, v in sorted(reviews_neg_a.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "end_id = 1010\n",
    "print(list(reviews_neg_a_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_neg_a_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sort by number of occurences of specified positive words\n",
    "reviews_neg_b_valuesorted = {k: v for k, v in sorted(reviews_neg_b.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "end_id = 1010\n",
    "print(list(reviews_neg_b_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_neg_b_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[9, 9, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "# Sort by number of occurences of specified positive words\n",
    "reviews_pos_b_valuesorted = {k: v for k, v in sorted(reviews_pos_b.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "end_id = 1005\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 9000 1000 10000\n"
     ]
    }
   ],
   "source": [
    "review_ids_neg_a = list(reviews_neg_a_valuesorted.keys())[:10000]\n",
    "review_ids_neg_b = list(reviews_neg_b_valuesorted.keys())[:9000]\n",
    "review_ids_pos_b = list(reviews_pos_b_valuesorted.keys())[:1000]\n",
    "review_ids_b = review_ids_neg_b + review_ids_pos_b\n",
    "review_ids_neg_a.sort()\n",
    "review_ids_neg_b.sort()\n",
    "review_ids_pos_b.sort()\n",
    "review_ids_b.sort()\n",
    "print(len(review_ids_neg_a), len(review_ids_neg_b), len(review_ids_pos_b), len(review_ids_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 10000 review IDs\n",
      "Created: Matrix (10000, 607181), 10000 reviews, 607181 tokens\n",
      "Filtering. Based on 10000 review IDs\n",
      "Created: Matrix (10000, 607181), 10000 reviews, 607181 tokens\n",
      "Input sizes: 44124 / 48459\n",
      "Output sizes: ratio 174 ; rare 27863 ; only a 16087 ; only b 20422\n",
      "\n",
      "only_a [('pornographic', 11), ('paula', 10), ('jms', 10), ('romania', 9), ('wack', 9), ('ellroy', 9), ('stained', 8), ('idiocracy', 8), ('unentertaining', 8), ('scathing', 8)]\n",
      "\n",
      "ratio_a [('worst', 1.54), ('waste', 1.47), ('awful', 1.47), ('stupid', 1.43), ('terrible', 1.4), ('money', 1.35), ('poor', 1.32), ('horrible', 1.32), ('thing', 1.31), ('worse', 1.3), ('bad', 1.27), ('boring', 1.25), ('horror', 1.25), ('good', 1.24), ('idea', 1.24), ('let', 1.22), ('director', 1.22), ('buy', 1.21), ('instead', 1.21), ('film', 1.19), ('care', 1.19), ('movie', 1.18), ('people', 1.18), ('acting', 1.18), ('half', 1.18), ('movies', 1.17), ('want', 1.17), ('plot', 1.16), ('saw', 1.16), ('script', 1.16), ('review', 1.16), ('point', 1.15), ('minutes', 1.14), ('completely', 1.14), ('year', 1.14), ('original', 1.13), ('scene', 1.13), ('effects', 1.13), ('simply', 1.13), ('problem', 1.13), ('making', 1.12), ('sound', 1.12), ('like', 1.11), ('know', 1.11), ('got', 1.11), ('sense', 1.11), ('dvd', 1.1), ('actors', 1.1), ('said', 1.1), ('wrong', 1.1)]\n",
      "\n",
      "ratio_b [('wonderful', 2.94), ('excellent', 2.56), ('loved', 2.13), ('fun', 1.92), ('enjoy', 1.61), ('recommend', 1.54), ('product', 1.45), ('amazon', 1.32), ('job', 1.27), ('best', 1.25), ('young', 1.25), ('night', 1.25), ('love', 1.23), ('bit', 1.22), ('getting', 1.16), ('family', 1.16), ('especially', 1.16), ('series', 1.15), ('shows', 1.15), ('true', 1.15), ('lot', 1.14), ('fan', 1.14), ('course', 1.14), ('set', 1.12), ('play', 1.12), ('times', 1.11), ('cast', 1.11), ('makes', 1.1), ('world', 1.1), ('day', 1.1), ('ending', 1.1), ('short', 1.1), ('production', 1.1), ('music', 1.09), ('action', 1.09), ('seeing', 1.09), ('gets', 1.06), ('main', 1.05), ('fans', 1.05), ('life', 1.04), ('long', 1.04), ('interesting', 1.04), ('disappointed', 1.04), ('feel', 1.04), ('special', 1.04), ('great', 1.03), ('work', 1.03), ('high', 1.03), ('second', 1.03), ('use', 1.03)]\n",
      "\n",
      "only_b [('cloverfield', 45), ('toning', 18), ('yuma', 15), ('mulder', 15), ('dragonlance', 15), ('earning', 14), ('bardem', 14), ('mcavoy', 13), ('ving', 13), ('tudors', 13)]\n",
      "\n",
      "rare [('unfortunately', '491 / 483'), ('rest', '489 / 479'), ('understand', '476 / 492'), ('supposed', '474 / 431'), ('dialogue', '468 / 413'), ('felt', '459 / 466'), ('given', '458 / 463'), ('release', '457 / 439'), ('went', '453 / 445'), ('mind', '447 / 454')]\n"
     ]
    }
   ],
   "source": [
    "# A vs B\n",
    "check_ratio_a, check_ratio_b, check_rare, check_only_a, check_only_b = get_ratio(\n",
    "    matrix.filter_keep_reviews(review_ids_neg_a),\n",
    "    matrix.filter_keep_reviews(review_ids_b),\n",
    "    min_token_occurences=500)\n",
    "print()\n",
    "print('only_a', list(check_only_a.items())[:10])\n",
    "print()\n",
    "print('ratio_a', list(check_ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(check_ratio_b.items())[:50])\n",
    "print()\n",
    "print('only_b', list(check_only_b.items())[:10])\n",
    "print()\n",
    "print('rare', list(check_rare.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 19000 review IDs\n",
      "Created: Matrix (19000, 607181), 19000 reviews, 607181 tokens\n",
      "Filtering. Based on 1000 review IDs\n",
      "Created: Matrix (1000, 607181), 1000 reviews, 607181 tokens\n",
      "Input sizes: 59005 / 25386\n",
      "Output sizes: ratio 343 ; rare 19502 ; only a 39160 ; only b 5541\n",
      "\n",
      "only_a [('redeeming', 262), ('pretentious', 155), ('amateurish', 142), ('uninteresting', 141), ('unfunny', 133), ('idiotic', 127), ('stinks', 118), ('bin', 106), ('rubbish', 103), ('ugh', 94)]\n",
      "\n",
      "ratio_a [('waste', 80.12), ('garbage', 45.9), ('wasted', 40.5), ('worst', 29.65), ('awful', 25.89), ('lame', 24.64), ('horrible', 22.7), ('boring', 20.31), ('disappointment', 20.31), ('disappointing', 18.04), ('poorly', 17.61), ('rent', 17.49), ('terrible', 16.08), ('stupid', 13.84), ('worse', 13.7), ('money', 13.14), ('sorry', 12.5), ('ridiculous', 12.44), ('predictable', 12.2), ('disappointed', 12.01), ('cheap', 11.94), ('mess', 11.89), ('poor', 11.88), ('movie', 11.15), ('avoid', 10.87), ('bad', 10.85), ('supposed', 9.75), ('horror', 8.37), ('save', 8.15), ('scary', 8.12), ('plot', 7.66), ('annoying', 7.57), ('weak', 7.33), ('decent', 7.15), ('guess', 7.05), ('script', 6.97), ('low', 6.95), ('expected', 6.89), ('film', 6.56), ('dialogue', 6.54), ('minutes', 6.44), ('bought', 6.44), ('sad', 6.44), ('half', 6.42), ('unfortunately', 6.4), ('buy', 6.16), ('attempt', 6.15), ('seriously', 6.03), ('acting', 5.97), ('budget', 5.97)]\n",
      "\n",
      "ratio_b [('fun', 2.0), ('enjoy', 1.56), ('recommend', 1.2), ('best', 0.95), ('love', 0.91), ('classic', 0.78), ('job', 0.69), ('great', 0.68), ('performance', 0.68), ('episodes', 0.62), ('collection', 0.62), ('season', 0.61), ('episode', 0.61), ('especially', 0.59), ('fans', 0.58), ('shows', 0.57), ('nice', 0.57), ('family', 0.55), ('plays', 0.54), ('truly', 0.53), ('friends', 0.53), ('true', 0.52), ('course', 0.52), ('role', 0.52), ('played', 0.52), ('live', 0.52), ('playing', 0.52), ('john', 0.52), ('music', 0.5), ('style', 0.5), ('small', 0.5), ('bit', 0.49), ('later', 0.49), ('humor', 0.49), ('young', 0.48), ('overall', 0.48), ('able', 0.48), ('takes', 0.47), ('world', 0.46), ('cast', 0.46), ('entertaining', 0.46), ('fine', 0.46), ('new', 0.45), ('makes', 0.45), ('different', 0.45), ('year', 0.45), ('help', 0.45), ('remember', 0.45), ('past', 0.45), ('set', 0.44)]\n",
      "\n",
      "only_b [('heartily', 9), ('samoa', 8), ('saddle', 8), ('elinor', 7), ('medley', 7), ('replayability', 6), ('sharpshooter', 6), ('idina', 6), ('jbl', 6), ('oakley', 5)]\n",
      "\n",
      "rare [('final', '499 / 107'), ('words', '497 / 73'), ('dvds', '497 / 100'), ('child', '496 / 78'), ('basically', '496 / 46'), ('add', '495 / 84'), ('stay', '494 / 51'), ('silly', '493 / 40'), ('documentary', '493 / 49'), ('dull', '491 / 21')]\n"
     ]
    }
   ],
   "source": [
    "# Neg vs. Pos\n",
    "check_ratio_a, check_ratio_b, check_rare, check_only_a, check_only_b = get_ratio(\n",
    "    matrix.filter_keep_reviews(review_ids_neg_a + review_ids_neg_b),\n",
    "    matrix.filter_keep_reviews(review_ids_pos_b),\n",
    "    min_token_occurences=500)\n",
    "print()\n",
    "print('only_a', list(check_only_a.items())[:10])\n",
    "print()\n",
    "print('ratio_a', list(check_ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(check_ratio_b.items())[:50])\n",
    "print()\n",
    "print('only_b', list(check_only_b.items())[:10])\n",
    "print()\n",
    "print('rare', list(check_rare.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ids(file_path, ids_list):\n",
    "    print('Writing ' + str(len(ids_list)) + ' items to ' + file_path)\n",
    "    with open(file_path, 'w') as fp:\n",
    "        fp.write(\"\\n\".join(str(item) for item in ids_list))\n",
    "\n",
    "if False:\n",
    "    # Note: Formerly amore-2-a-neg.txt\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-a.txt', review_ids_neg_a)\n",
    "    # Note: Deleted as included in amore-2-b.txt\n",
    "    # write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-b-neg.txt', review_ids_neg_b)\n",
    "    # Note: Formerly amore-2-b-pos.txt and wrongly named amore-2-a-pos.txt\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-drift.txt', review_ids_pos_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-b.txt', review_ids_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-tokens.txt', filter_tokens_a)\n",
    "\n",
    "# Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-neg.txt\n",
    "# Writing 9000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b-neg.txt\n",
    "# Writing 1000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-pos.txt\n",
    "# Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b.txt\n",
    "# Writing 10 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if False:\n",
    "    print('a', list(reviews_neg_b_valuesorted.keys())[0:9])\n",
    "    #print('b', list(reviews_neg_b_valuesorted)[0:9])\n",
    "    print(get_text(1172))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print('1', m.get_token_data(revs[1]))\n",
    "    print('6', m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print('0', m.get_token_data(revs[0]))\n",
    "    print('4', m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "print_matrix_info = None\n",
    "get_test_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
