{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMORE datasets\n",
    "\n",
    "- Note: Years 1997 to 1999 contain only a few reviews and are not included in Doc2Vec embeddings.  \n",
    "  (The years 2000 to 2012 are included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project root\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from access.file_storage import FileStorage\n",
    "from access.interim_storage import InterimStorage\n",
    "from amore.amazon_reviews_reader import AmazonReviewsReader\n",
    "from amore.opinion_lexicon import OpinionLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* **opinion_lexicon**: Instance of OpinionLexicon to access negative and positive words. Not used for now.\n",
    "* **year_star_ids**: Collection of IDs sorted by years and stars. Used afterwards to filter by stars and years in method **get_review_ids**.\n",
    "* **reader**: Instance of AmazonReviewsReader to access review data. Used afterwards to create revno_to_text.\n",
    "* **revno_to_text**: Dictionary review-number to full text. Used afterwards to access review-texts in method **get_text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2006\n"
     ]
    }
   ],
   "source": [
    "# For multiple usage afterwards\n",
    "\n",
    "file_storage = FileStorage()\n",
    "\n",
    "opinion_lexicon = OpinionLexicon(file_storage.get_filepath('opinion-words'))\n",
    "print('negative words:', len(opinion_lexicon.get_negative_set()))\n",
    "print('positive words:', len(opinion_lexicon.get_positive_set()))\n",
    "# negative words: 4783\n",
    "# positive words: 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "Example stars:   [1, 2, 3, 4, 5]\n",
      "Example entry:   [4368, 2007, 1]\n",
      "Reviews: 1727821\n"
     ]
    }
   ],
   "source": [
    "# Read deduplicated review Ids\n",
    "with bz2.BZ2File(file_storage.get_filepath('deduplicated'), 'r') as file:\n",
    "    year_star_ids = pickle.loads(file.read())\n",
    "print('Available years:', sorted(year_star_ids.keys()))\n",
    "print('Example stars:  ', sorted(year_star_ids[2007].keys()))\n",
    "print('Example entry:  ', year_star_ids[2007][1][0])\n",
    "count = 0\n",
    "for year in year_star_ids:\n",
    "    for star in year_star_ids[year]:\n",
    "        count += len(year_star_ids[year][star])\n",
    "print('Reviews:', count)\n",
    "\n",
    "# Available years: Available years: [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
    "# Example stars:   [1, 2, 3, 4, 5]\n",
    "# Example entry:   [4368, 2007, 1]\n",
    "# Reviews: 1727821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: 7827594\n",
      "Runtime: 287.5022290580673\n"
     ]
    }
   ],
   "source": [
    "# Read review texts\n",
    "min_year = 2000\n",
    "max_docs = -1\n",
    "start_time = timeit.default_timer()\n",
    "reader = AmazonReviewsReader(file_storage.get_filepath('amazon_gz_file'), AmazonReviewsReader.MODE_TYPED, min_year=min_year, max_docs=max_docs)\n",
    "revno_to_text = {}\n",
    "def get_texts(item):\n",
    "    return (item[AmazonReviewsReader.KEY_SUMMARY] + \" \" + item[AmazonReviewsReader.KEY_TEXT]).replace('<br />', ' ')\n",
    "for item in reader:\n",
    "    revno_to_text[item[AmazonReviewsReader.KEY_NUMBER]] = get_texts(item)\n",
    "print('Texts:', len(revno_to_text))\n",
    "print('Runtime:', timeit.default_timer() - start_time)\n",
    "\n",
    "# start year: 2007\n",
    "# Texts: 4662381\n",
    "# Runtime: 265.2943881880492\n",
    "\n",
    "# start year: 2000\n",
    "# Texts: 7827594\n",
    "# Runtime: 312.4320105519146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read document-term matrix files\n",
    "\n",
    "Data required to create **Matrix** instances afterwards.\n",
    "\n",
    "* **doc_term_matrix**: Matrix containing document-IDs and term-IDs. Used afterwards for building Matrix.\n",
    "* **vocabulary**: Dictionary containing tuples of terms and term-IDs. Used afterwards to create inv_vocabulary.\n",
    "* **inv_vocabulary**: Dictionary containing tuples of term-IDs and terms.Used afterwards to create tokens.\n",
    "* **tokens**: Ordered list of terms, index represents term-ID. Used afterwards as token-indices for building Matrix.\n",
    "* **vecid_revno**: Values used afterwards as review-indices for building Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Runtime: 36.32529074000195\n"
     ]
    }
   ],
   "source": [
    "# Read document-term matrix\n",
    "start_time = timeit.default_timer()\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-DocTermMatrix'), 'r') as file:\n",
    "    doc_term_matrix = pickle.loads(file.read())\n",
    "    print('document-term matrix:', doc_term_matrix.shape, type(doc_term_matrix))\n",
    "    print('Runtime:', timeit.default_timer() - start_time)\n",
    "    \n",
    "#print(doc_term_matrix)\n",
    "#       (0, 299799)  3\n",
    "#        :       :\n",
    "# (1203681, 367201)  1\n",
    "\n",
    "# start year: 2007\n",
    "# document-term matrix: (1203682, 486546) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime:  26.56719038821757\n",
    "\n",
    "# start year: 2000\n",
    "# document-term matrix: (1584098, 607181) <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# Runtime: 54.29142002761364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 607181 <class 'dict'>\n",
      "example: ('movie', 371301)\n",
      "inv_vocabulary: 607181 <class 'dict'>\n",
      "example: (371301, 'movie')\n"
     ]
    }
   ],
   "source": [
    "# Read vocabulary of document-term matrix\n",
    "# Invert it\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-Vocabulary'), 'r') as file:\n",
    "    vocabulary = pickle.loads(file.read())\n",
    "    print('vocabulary:', len(vocabulary), type(vocabulary))\n",
    "    print('example:', next(iter(vocabulary.items())))\n",
    "\n",
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "print('inv_vocabulary:', len(inv_vocabulary), type(inv_vocabulary))\n",
    "print('example:', next(iter(inv_vocabulary.items())))\n",
    "\n",
    "tokens = list(dict(sorted(inv_vocabulary.items())).values())\n",
    "\n",
    "# start year: 2007\n",
    "# vocabulary: 486546 <class 'dict'>\n",
    "# example: ('movie', 299799)\n",
    "# inv_vocabulary: 486546 <class 'dict'>\n",
    "# example: (299799, 'movie')\n",
    "\n",
    "# start year: 2000\n",
    "# vocabulary: 607181 <class 'dict'>\n",
    "# example: ('movie', 371301)\n",
    "# inv_vocabulary: 607181 <class 'dict'>\n",
    "# example: (371301, 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer ID to review no: 1584098 <class 'dict'>\n",
      "example: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read count-vector-ID to review-ID mapping\n",
    "with bz2.BZ2File(file_storage.get_filepath('AMORE-CountVec-VecidRevno'), 'r') as file:\n",
    "    vecid_revno = pickle.loads(file.read())\n",
    "    print('vectorizer ID to review no:', len(vecid_revno), type(vecid_revno))\n",
    "    print('example:', next(iter(vecid_revno.items())))\n",
    "\n",
    "# start year: 2007\n",
    "# vectorizer ID to review no: 1203682 <class 'dict'>\n",
    "# example: (0, 3)\n",
    "\n",
    "# start year: 2000\n",
    "# vectorizer ID to review no: 1584098 <class 'dict'>\n",
    "# example: (0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access methods and Matrix class\n",
    "\n",
    "- Docs:\n",
    "    - [docs.scipy.org scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "    - [docs.scipy.org scipy.sparse.spmatrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html)\n",
    "    - [docs.scipy.org sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(review_id):\n",
    "    return revno_to_text[review_id]\n",
    "\n",
    "def get_review_ids(years, stars):\n",
    "    ids = []\n",
    "    for year in year_star_ids:\n",
    "        if year in years:\n",
    "            for star in year_star_ids[year]:\n",
    "                if star in stars:\n",
    "                    for tup in year_star_ids[year][star]:\n",
    "                        ids.append(tup[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used afterwards in Matrix class.\n",
    "\n",
    "def delete_from_csr(mat, row_indices=[], col_indices=[]):\n",
    "    \"\"\"\n",
    "    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.\n",
    "    WARNING: Indices of altered axes are reset in the returned matrix\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    https://stackoverflow.com/a/45486349\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    if row_indices:\n",
    "        rows = list(row_indices)\n",
    "    if col_indices:\n",
    "        cols = list(col_indices)\n",
    "\n",
    "    if len(rows) > 0 and len(cols) > 0:\n",
    "        row_mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        row_mask[rows] = False\n",
    "        col_mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        col_mask[cols] = False\n",
    "        return mat[row_mask][:,col_mask]\n",
    "    elif len(rows) > 0:\n",
    "        mask = np.ones(mat.shape[0], dtype=bool)\n",
    "        mask[rows] = False\n",
    "        return mat[mask]\n",
    "    elif len(cols) > 0:\n",
    "        mask = np.ones(mat.shape[1], dtype=bool)\n",
    "        mask[cols] = False\n",
    "        return mat[:,mask]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"\n",
    "    Sparse matrix data access and filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # document-term matrix, scipy.sparse.csr.csr_matrix\n",
    "    doc_term_matrix = None\n",
    "    \n",
    "    # matrix-document-index to review-id\n",
    "    review_indices = None\n",
    "    reviewids_to_reviewindices = {}\n",
    "    \n",
    "    # matrix-term-index to token\n",
    "    token_indices = None # list of tokens\n",
    "    token_to_tokenindices = {}\n",
    "        \n",
    "    def __init__(self, doc_term_matrix: csr_matrix, review_indices: list, token_indices: list):\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        \n",
    "        self.review_indices = review_indices\n",
    "        self.reviewids_to_reviewindices = {}\n",
    "        for i, item in enumerate(self.review_indices):\n",
    "            self.reviewids_to_reviewindices[item] = i\n",
    "        \n",
    "        self.token_indices = token_indices\n",
    "        for i, item in enumerate(self.token_indices):\n",
    "            self.token_to_tokenindices[item] = i\n",
    "        \n",
    "        print('Created:', self)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Matrix ' + str(self.doc_term_matrix.shape) + ', ' + str(len(self.review_indices)) + ' reviews, ' + str(len(self.token_indices)) + ' tokens'\n",
    "\n",
    "    def get_token_data(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix data for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].data\n",
    "    \n",
    "    def get_token_indices(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Gets sparse matrix indices of tokens for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        return self.doc_term_matrix[review_index].indices\n",
    "\n",
    "    def get_token_counts(self, review_id=None, review_index=None):\n",
    "        \"\"\"\n",
    "        Returns dict (token, count) for review index or ID.\n",
    "        \"\"\"\n",
    "        if review_index is None:\n",
    "            review_index = self.reviewids_to_reviewindices[review_id]\n",
    "        token_counts = {}\n",
    "        token_data = self.get_token_data(review_index=review_index)\n",
    "        for i, token_index in enumerate(self.get_token_indices(review_index=review_index)):\n",
    "            token = self.token_indices[token_index]\n",
    "            token_counts[token] = token_data[i]\n",
    "        return dict(sorted(token_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    def filter_remove_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Removes given review IDs.\n",
    "        \"\"\"\n",
    "        review_indices_remove = []\n",
    "        for review_id in review_ids:\n",
    "            review_indices_remove.append(self.reviewids_to_reviewindices[review_id])\n",
    "        review_indices_new = self.review_indices.copy()\n",
    "        for review_index_remove in sorted(review_indices_remove, reverse=True):\n",
    "            review_indices_new.pop(review_index_remove)\n",
    "        return Matrix(delete_from_csr(csr_matrix(self.doc_term_matrix), review_indices_remove, []), review_indices_new, self.token_indices.copy())\n",
    "    \n",
    "    def filter_keep_reviews(self, review_ids):\n",
    "        \"\"\"\n",
    "        Keeps only given review IDs.\n",
    "        \"\"\"\n",
    "        doc_indices_extract = []\n",
    "        new_review_indices = []\n",
    "        for review_id in review_ids:\n",
    "            doc_indices_extract.append(self.reviewids_to_reviewindices[review_id])\n",
    "            new_review_indices.append(review_id)\n",
    "\n",
    "        # Filter matrix\n",
    "        print('Filtering. Based on', len(review_ids), 'review IDs')\n",
    "        new_doc_term_matrix = self.doc_term_matrix[doc_indices_extract,:]\n",
    "        \n",
    "        return Matrix(new_doc_term_matrix, new_review_indices, self.token_indices)\n",
    "\n",
    "    def get_overall_token_occurences(self):\n",
    "        \"\"\"\n",
    "        Returns tokens and their occurences (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        # Count non-zero values of token-indices\n",
    "        tokenindex_occurences = {}\n",
    "        for tokenindex in self.doc_term_matrix.nonzero()[1]:\n",
    "            if tokenindex in tokenindex_occurences:\n",
    "                tokenindex_occurences[tokenindex] += 1\n",
    "            else:\n",
    "                tokenindex_occurences[tokenindex] = 1\n",
    "                \n",
    "        # Sort by values/counts\n",
    "        tokenindex_occurences = dict(sorted(tokenindex_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Token-indices to tokens\n",
    "        token_occurences = {}\n",
    "        for item in tokenindex_occurences.items():\n",
    "            token_occurences[self.token_indices[item[0]]] = item[1]\n",
    "        return token_occurences\n",
    "\n",
    "    def filter_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Filters matrix by a given set of tokens (e.g. positive words).\n",
    "        \"\"\"\n",
    "        # Collect available token-indices (required to filter matrix)\n",
    "        tokenindices = []\n",
    "        new_token_indices = {}\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_tokenindices:\n",
    "                tokenindices.append(self.token_to_tokenindices[token])\n",
    "                new_token_indices[self.token_to_tokenindices[token]] = token\n",
    "        new_token_indices = dict(sorted(new_token_indices.items(), reverse=False))\n",
    "        new_token_indices = list(new_token_indices.values())\n",
    "        \n",
    "        # Filter matrix\n",
    "        new_doc_term_matrix = self.doc_term_matrix[:,tokenindices]\n",
    "                \n",
    "        return Matrix(new_doc_term_matrix, self.review_indices, new_token_indices)\n",
    "\n",
    "    def filter_reviews_by_tokens(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Returns review IDs and occurences of given tokens (counted max 1 time) in all documents.\n",
    "        \"\"\"\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "            \n",
    "        reviews_to_occurences = {}\n",
    "        nonzero = self.doc_term_matrix.nonzero()\n",
    "        i = -1  \n",
    "        for tokenindex in nonzero[1]:\n",
    "            i += 1\n",
    "            if tokenindex in token_indices:\n",
    "                review_index = nonzero[0][i]\n",
    "                if review_index in reviews_to_occurences:\n",
    "                    reviews_to_occurences[review_index] += 1\n",
    "                else:\n",
    "                    reviews_to_occurences[review_index] = 1\n",
    "        \n",
    "        reviewids_to_occurences = {}\n",
    "        for item in reviews_to_occurences.items():\n",
    "            reviewids_to_occurences[self.review_indices[item[0]]] = item[1]\n",
    "            \n",
    "        return reviewids_to_occurences\n",
    "    \n",
    "    def count_token_occurences(self, review_ids, tokens):\n",
    "        \"\"\"\n",
    "        Sums up the occurencies of given tokens in given reviews.\n",
    "        \"\"\"\n",
    "        review_to_tokenoccurences = {}\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            token_indices.append(self.token_to_tokenindices[token])\n",
    "        for review_id in review_ids:\n",
    "            if not review_id in review_ids:\n",
    "                continue\n",
    "            review_to_tokenoccurences[review_id] = 0\n",
    "            for counts in self.get_token_data(review_id=review_id):\n",
    "                review_to_tokenoccurences[review_id] += 1\n",
    "        return review_to_tokenoccurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "#### Get tokens which occur predominantly in only one dataset: Get ratio for each token.\n",
    "\n",
    "* **get_ratio**\n",
    "  * In: matrix-A, matrix-B\n",
    "  * In: min_token_occurences (token has to occure at least x times in each of both matrixes)\n",
    "  * Out: tokens sorted by ratio\n",
    "  * Out: tokens sorted by inverted ratio\n",
    "  * Out: tokens not reached minimum\n",
    "  * Out: tokens only in A\n",
    "  * Out: tokens only in B\n",
    "* **filter_tokens_by_ratio**\n",
    "  * In: ratio (generated by method get_ratio)\n",
    "  * In: min_ratio (minimum threshold)\n",
    "  * In: max_tokens (maximum number of tokens to return)\n",
    "  * In: exclude_tokens (tokens not to return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(matrix_a, matrix_b, min_token_occurences = -1):\n",
    "    ratio = {}\n",
    "    only_a = {}\n",
    "    rare = {}\n",
    "    # Pairs of tokens and occurences\n",
    "    a_token_occurences = matrix_a.get_overall_token_occurences()\n",
    "    b_token_occurences = matrix_b.get_overall_token_occurences()\n",
    "    print('Input sizes:', len(a_token_occurences), '/', len(b_token_occurences))\n",
    "    for tok_occ in a_token_occurences.items():\n",
    "\n",
    "        # Filter/remove rare:\n",
    "        # minimum is set AND\n",
    "        # token has to be in both matrixes AND\n",
    "        # threshold reached in A AND\n",
    "        # threshod reached in B AND\n",
    "        if min_token_occurences != -1 and \\\n",
    "           tok_occ[0] in b_token_occurences and \\\n",
    "           min_token_occurences > tok_occ[1] and \\\n",
    "           min_token_occurences > b_token_occurences[tok_occ[0]]:\n",
    "            rare[tok_occ[0]] = str(tok_occ[1]) + \" / \" + str(b_token_occurences[tok_occ[0]])\n",
    "            b_token_occurences.pop(tok_occ[0])\n",
    "            continue\n",
    "            \n",
    "        # Only A\n",
    "        if not tok_occ[0] in b_token_occurences:\n",
    "            only_a[tok_occ[0]] = tok_occ[1]\n",
    "            continue\n",
    "        \n",
    "        # Add ratio, remove from B\n",
    "        ratio_a = tok_occ[1] / len(a_token_occurences)\n",
    "        ratio_b = b_token_occurences.pop(tok_occ[0]) / len(b_token_occurences)\n",
    "        ratio[tok_occ[0]] = round(ratio_a / ratio_b, 2)\n",
    "\n",
    "    ratio_inverted = {}\n",
    "    for item in ratio.items():\n",
    "        ratio_inverted[item[0]] = round(1 / item[1], 2)\n",
    "\n",
    "    print('Output sizes:', 'ratio', len(ratio), '; rare', len(rare), '; only a', len(only_a), '; only b', len(b_token_occurences))\n",
    "    return dict(sorted(ratio.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(ratio_inverted.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    rare, \\\n",
    "    dict(sorted(only_a.items(), key=lambda item: item[1], reverse=True)), \\\n",
    "    dict(sorted(b_token_occurences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def filter_tokens_by_ratio(ratio_results, min_ratio, max_tokens, exclude_tokens):\n",
    "    tokens = []\n",
    "    for item in ratio_results.items():\n",
    "        if item[0] in exclude_tokens:\n",
    "            continue\n",
    "        if item[1] >= min_ratio:\n",
    "            tokens.append(item[0])\n",
    "        if len(tokens) >= max_tokens:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data / create base matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 607181), 1584098 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Main matrix containing all data\n",
    "matrix = Matrix(doc_term_matrix, vecid_revno.values(), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews to use\n",
    "ids_neg_a = get_review_ids(years=[2006,2007], stars=[1,2])\n",
    "ids_neg_b = get_review_ids(years=[2008], stars=[1,2])\n",
    "ids_neg = ids_neg_a + ids_neg_b\n",
    "\n",
    "ids_pos_a = []\n",
    "ids_pos_b = get_review_ids(years=[2008], stars=[4,5])\n",
    "ids_pos = ids_pos_a + ids_pos_b\n",
    "\n",
    "ids_all = ids_neg + ids_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering. Based on 57263 review IDs\n",
      "Created: Matrix (57263, 607181), 57263 reviews, 607181 tokens\n",
      "\n",
      "Filtering. Based on 141293 review IDs\n",
      "Created: Matrix (141293, 607181), 141293 reviews, 607181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Sub-matrixes\n",
    "m_neg = matrix.filter_keep_reviews(ids_neg)\n",
    "print()\n",
    "m_pos = matrix.filter_keep_reviews(ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tokens to use\n",
    "\n",
    "Note: During exploration of the data it turned out that there are more positive words that have a large ratio and therefore can be used to determine (unique) drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: 156420 / 102852\n",
      "Output sizes: ratio 60 ; rare 69315 ; only a 87045 ; only b 33477\n",
      "\n",
      "ratio_a [('highly', 6.68), ('excellent', 6.51), ('wonderful', 6.1), ('loved', 4.13), ('season', 4.05), ('great', 3.79), ('fun', 3.59), ('classic', 3.48), ('recommend', 3.28), ('enjoy', 3.06), ('love', 3.0), ('best', 2.84), ('family', 2.72), ('series', 2.55), ('music', 2.16), ('years', 2.11), ('dvd', 2.03), ('set', 2.03), ('life', 1.85), ('world', 1.85), ('worth', 1.8), ('times', 1.8), ('watched', 1.79), ('fan', 1.78), ('new', 1.75), ('old', 1.67), ('video', 1.65), ('good', 1.59), ('makes', 1.59), ('watch', 1.53), ('work', 1.48), ('lot', 1.48), ('man', 1.44), ('long', 1.4), ('story', 1.39), ('little', 1.39), ('time', 1.34), ('seen', 1.33), ('funny', 1.33), ('watching', 1.3), ('real', 1.29), ('films', 1.25), ('way', 1.24), ('look', 1.19), ('like', 1.17), ('movies', 1.17), ('got', 1.17), ('buy', 1.17), ('think', 1.15), ('characters', 1.13)]\n",
      "\n",
      "ratio_b [('bad', 2.63), ('acting', 1.27), ('better', 0.96), ('end', 0.95), ('character', 0.95), ('people', 0.93), ('know', 0.93), ('movie', 0.92), ('film', 0.92), ('want', 0.92), ('characters', 0.88), ('think', 0.87), ('like', 0.85), ('movies', 0.85), ('got', 0.85), ('buy', 0.85), ('look', 0.84), ('way', 0.81), ('films', 0.8), ('real', 0.78), ('watching', 0.77), ('time', 0.75), ('seen', 0.75), ('funny', 0.75), ('story', 0.72), ('little', 0.72), ('long', 0.71), ('man', 0.69), ('work', 0.68), ('lot', 0.68), ('watch', 0.65), ('good', 0.63), ('makes', 0.63), ('video', 0.61), ('old', 0.6), ('new', 0.57), ('fan', 0.56), ('worth', 0.56), ('times', 0.56), ('watched', 0.56), ('life', 0.54), ('world', 0.54), ('dvd', 0.49), ('set', 0.49), ('years', 0.47), ('music', 0.46), ('series', 0.39), ('family', 0.37), ('best', 0.35), ('love', 0.33)]\n",
      "\n",
      "rare [('shows', '9995 / 2621'), ('going', '9941 / 6846'), ('scenes', '9818 / 6445'), ('feel', '9797 / 4100'), ('favorite', '9778 / 1341'), ('come', '9641 / 4739'), ('day', '9612 / 3342'), ('enjoyed', '9610 / 1406'), ('right', '9601 / 4709'), ('amazon', '9545 / 3451')]\n",
      "\n",
      "only_a [('magers', 108), ('webmaster', 106), ('nareau', 105), ('guideslines', 104), ('jbl', 68), ('druxman', 61), ('salutations', 56), ('stane', 54), ('obadiah', 53), ('dessay', 52)]\n",
      "\n",
      "only_b [('paolini', 36), ('freedomland', 22), ('retarted', 19), ('galbatorix', 19), ('malus', 19), ('refunds', 18), ('contagium', 16), ('hemophages', 15), ('fallacious', 15), ('snorefest', 14)]\n"
     ]
    }
   ],
   "source": [
    "ratio_a, ratio_b, rare, only_a, only_b = get_ratio(\n",
    "    m_pos,\n",
    "    m_neg,\n",
    "    min_token_occurences=10*1000)\n",
    "print()\n",
    "print('ratio_a', list(ratio_a.items())[:50])\n",
    "print()\n",
    "print('ratio_b', list(ratio_b.items())[:50])\n",
    "print()\n",
    "print('rare', list(rare.items())[:10])\n",
    "print()\n",
    "print('only_a', list(only_a.items())[:10])\n",
    "print()\n",
    "print('only_b', list(only_b.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'wonderful', 'loved', 'great', 'fun', 'recommend', 'enjoy', 'love', 'best', 'good']\n",
      "['bad', 'acting']\n"
     ]
    }
   ],
   "source": [
    "filter_tokens_a = filter_tokens_by_ratio(ratio_a, 1.25, 10,\n",
    "['highly', 'season', 'collection', 'episodes', 'features', 'classic', 'episode', 'definitely', 'shows', 'family', 'performances', 'gives', 'young', 'series',\n",
    " 'music', 'performance', 'live', 'years', 'job', 'john', 'set', 'true', 'finally', 'truly', 'world', 'different', 'dvd', 'especially', 'history',\n",
    " 'life', 'cast', 'worth', 'fan', 'times', 'new', 'old', 'makes', 'watch',\n",
    " 'work', 'man', 'video', 'long', 'story', 'lot', 'little', 'come', 'seen', 'watched'\n",
    "])\n",
    "print(filter_tokens_a)\n",
    "\n",
    "filter_tokens_b = filter_tokens_by_ratio(ratio_b, 1.25, 10, [])\n",
    "print(filter_tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new matrix only consisting of filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: Matrix (1584098, 10), 1584098 reviews, 10 tokens\n",
      "Filtering. Based on 36185 review IDs\n",
      "Created: Matrix (36185, 10), 36185 reviews, 10 tokens\n",
      "Filtering. Based on 21078 review IDs\n",
      "Created: Matrix (21078, 10), 21078 reviews, 10 tokens\n",
      "Filtering. Based on 141293 review IDs\n",
      "Created: Matrix (141293, 10), 141293 reviews, 10 tokens\n"
     ]
    }
   ],
   "source": [
    "# Remove everything but filtered tokens\n",
    "matrix_filtered       = matrix.filter_tokens(filter_tokens_a)\n",
    "matrix_filtered_neg_a = matrix_filtered.filter_keep_reviews(ids_neg_a)\n",
    "matrix_filtered_neg_b = matrix_filtered.filter_keep_reviews(ids_neg_b)\n",
    "matrix_filtered_pos_b = matrix_filtered.filter_keep_reviews(ids_pos_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27668\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_a = {}\n",
    "for review_item in matrix_filtered_neg_a.count_token_occurences(ids_neg_a, filter_tokens_a).items():\n",
    "    if review_item[1] <= 1:\n",
    "        reviews_neg_a[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16446\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_b = {}\n",
    "for review_item in matrix_filtered_neg_b.count_token_occurences(ids_neg_b, filter_tokens_a).items():\n",
    "    if review_item[1] <= 1:\n",
    "        reviews_neg_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_neg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2888\n"
     ]
    }
   ],
   "source": [
    "reviews_pos_b = {}\n",
    "for review_item in matrix_filtered_pos_b.count_token_occurences(ids_pos_b, filter_tokens_a).items():\n",
    "    if review_item[1] >= 5:\n",
    "        reviews_pos_b[review_item[0]] = review_item[1]\n",
    "print(len(reviews_pos_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_a_valuesorted = {k: v for k, v in sorted(reviews_neg_a.items(), key=lambda item: item[1], reverse=True)}\n",
    "end_id = 1010\n",
    "print(list(reviews_neg_a_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_neg_a_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "reviews_neg_b_valuesorted = {k: v for k, v in sorted(reviews_neg_b.items(), key=lambda item: item[1], reverse=True)}\n",
    "end_id = 1010\n",
    "print(list(reviews_neg_b_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_neg_b_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[9, 9, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "reviews_pos_b_valuesorted = {k: v for k, v in sorted(reviews_pos_b.items(), key=lambda item: item[1], reverse=True)}\n",
    "end_id = 1005\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])\n",
    "end_id = 10\n",
    "print(list(reviews_pos_b_valuesorted.values())[end_id-10:end_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 9000 1000 10000\n"
     ]
    }
   ],
   "source": [
    "review_ids_neg_a = list(reviews_neg_a_valuesorted.keys())[:10000]\n",
    "review_ids_neg_b = list(reviews_neg_b_valuesorted.keys())[:9000]\n",
    "review_ids_pos_b = list(reviews_pos_b_valuesorted.keys())[:1000]\n",
    "review_ids_b = review_ids_neg_b + review_ids_pos_b\n",
    "review_ids_neg_a.sort()\n",
    "review_ids_neg_b.sort()\n",
    "review_ids_pos_b.sort()\n",
    "review_ids_b.sort()\n",
    "print(len(review_ids_neg_a), len(review_ids_neg_b), len(review_ids_pos_b), len(review_ids_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-neg.txt\n",
      "Writing 9000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b-neg.txt\n",
      "Writing 1000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-pos.txt\n",
      "Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b.txt\n",
      "Writing 10 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-tokens.txt\n"
     ]
    }
   ],
   "source": [
    "def write_ids(file_path, ids_list):\n",
    "    print('Writing ' + str(len(ids_list)) + ' items to ' + file_path)\n",
    "    with open(file_path, 'w') as fp:\n",
    "        fp.write(\"\\n\".join(str(item) for item in ids_list))\n",
    "\n",
    "if True:\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-a-neg.txt', review_ids_neg_a)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-b-neg.txt', review_ids_neg_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-a-pos.txt', review_ids_pos_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-b.txt', review_ids_b)\n",
    "    write_ids(file_storage.get_storage_directory() + '/benchmark-ids/amore-2-tokens.txt', filter_tokens_a)\n",
    "\n",
    "# Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-neg.txt\n",
    "# Writing 9000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b-neg.txt\n",
    "# Writing 1000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-a-pos.txt\n",
    "# Writing 10000 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-b.txt\n",
    "# Writing 10 items to /home/eml4u/EML4U/notebooks/amore/data/benchmark-ids/amore-2-tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [16503, 22152, 27990, 29834, 30635, 33205, 34459, 44418, 44818]\n",
      "Great getting started video I decided I wanted to learn the autoharp. I really liked this tape better than some because it gave you the basic strums you could practice and easily put together. You must practice. The instrument is simple but takes practice to learn but after a few afternoons you will have fun strumming and picking your way thru familiar songs.\n"
     ]
    }
   ],
   "source": [
    "# Check dict review-id to number of occurences\n",
    "if True:\n",
    "    print('a', list(reviews_neg_b_valuesorted.keys())[0:9])\n",
    "    #print('b', list(reviews_neg_b_valuesorted)[0:9])\n",
    "    print(get_text(1172))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for matrix class\n",
    "\n",
    "def print_matrix_info(m, t=8):\n",
    "    print('Review IDs ', list(m.reviewids_to_reviewindices.keys()))\n",
    "    print('Token IDs', t,  list(m.token_to_tokenindices.keys())[:t])\n",
    "\n",
    "def get_test_matrix(print_info=False):\n",
    "    r = list(vecid_revno.values())[:8]\n",
    "    t = list(tokens)\n",
    "    c = doc_term_matrix[:8]\n",
    "    if print_info:\n",
    "        print('r    ', r)\n",
    "        print('t[:8]', t[:8])\n",
    "    m = Matrix(c, r, t)\n",
    "    return m\n",
    "\n",
    "if False:\n",
    "    # Test matrix generation\n",
    "    m = get_test_matrix(True)\n",
    "    print_matrix_info(m)\n",
    "    m = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_remove_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    rem = revs.copy()\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    print()\n",
    "    rem.pop(7)\n",
    "    rem.pop(5)\n",
    "    rem.pop(0)\n",
    "    print('Revs to remove', rem)\n",
    "    m = m.filter_remove_reviews(rem)\n",
    "    print_matrix_info(m)\n",
    "    print(m.get_token_data(revs[0]))\n",
    "    print(m.get_token_data(revs[7]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    rem = None\n",
    "\n",
    "if False:\n",
    "    # Test filter_keep_reviews()\n",
    "    m = get_test_matrix()\n",
    "    revs = list(m.reviewids_to_reviewindices.keys())\n",
    "    print_matrix_info(m)\n",
    "    print('1', m.get_token_data(revs[1]))\n",
    "    print('6', m.get_token_data(revs[6]))\n",
    "    print()\n",
    "    revs.pop(7)\n",
    "    revs.pop(5)\n",
    "    revs.pop(0)\n",
    "    print('Revs to keep', revs)\n",
    "    m = m.filter_keep_reviews(revs)\n",
    "    print_matrix_info(m)\n",
    "    print('0', m.get_token_data(revs[0]))\n",
    "    print('4', m.get_token_data(revs[4]))\n",
    "    m = None\n",
    "    revs = None\n",
    "    \n",
    "print_matrix_info = None\n",
    "get_test_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
